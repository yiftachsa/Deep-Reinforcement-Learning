{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOOLUVuEOnII",
        "outputId": "c76022f5-eb0f-45aa-97f5-03f614ee97c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 92 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 122 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4.1\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "!pip install tensorboardX\n",
        "# !pip install tensorflow-gpu \n",
        "\n",
        "# !pip install tf_slim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZv3ylAvRc_t"
      },
      "outputs": [],
      "source": [
        "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "# !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# !pip install -U colabgymrender\n",
        "\n",
        "# from colabgymrender.recorder import Recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WArmaXplNQUH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "# !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# !apt-get update > /dev/null 2>&1\n",
        "# !apt-get install cmake > /dev/null 2>&1\n",
        "# !pip install --upgrade setuptools 2>&1\n",
        "# !pip install ez_setup > /dev/null 2>&1\n",
        "# !pip install gym[atari] > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN1kCRH_PBv-"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import datetime\n",
        "# import tf_slim as slim\n",
        "from tensorboardX import SummaryWriter\n",
        "import random\n",
        "import time\n",
        "from gym.wrappers import Monitor\n",
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBO3_k5hNjsL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import glob\n",
        "# import io\n",
        "# import base64\n",
        "# from IPython.display import HTML\n",
        "# from pyvirtualdisplay import Display\n",
        "# from IPython import display as ipythondisplay\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()\n",
        "\n",
        "# def show_video():\n",
        "#   mp4list = glob.glob('video/*.mp4')\n",
        "#   if len(mp4list) > 0:\n",
        "#     mp4 = mp4list[0]\n",
        "#     video = io.open(mp4, 'r+b').read()\n",
        "#     encoded = base64.b64encode(video)\n",
        "#     ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "#                 loop controls style=\"height: 400px;\">\n",
        "#                 <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "#              </video>'''.format(encoded.decode('ascii'))))\n",
        "#   else: \n",
        "#     print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "# def wrap_env(env):\n",
        "#   env = Monitor(env, './video', force=True)\n",
        "#   return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3rAMsYYqdp2"
      },
      "source": [
        "#Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh3vKQZGqlIM"
      },
      "source": [
        "##Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAIEjzgmYOKH"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "import gym\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Activation, concatenate   # , Reshape\n",
        "from tensorflow.keras.models import Model, model_from_json\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow import GradientTape, math, one_hot, square, summary, config\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49JqRgsBbPvn"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghMjwStXX4nx"
      },
      "outputs": [],
      "source": [
        "# class LearningRateLoggingCallback(tf.keras.callbacks.Callback):\n",
        "#     def on_epoch_end(self, epoch):\n",
        "#         lr = self.model.optimizer.lr\n",
        "#         tf.summary.scalar('learning rate', data=lr, step=epoch)\n",
        "\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "    \"\"\"\n",
        "    Neural network class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_layers, init_learning_rate,\n",
        "                 decay_steps, decay_rate, loss=\"mse\",weights=None):\n",
        "        \"\"\"\n",
        "        Initialize the Feed Forward Neural Network and the optimizer\n",
        "\n",
        "        :param input_size: the length of the input\n",
        "        :param output_size: the length of the output\n",
        "        :param hidden_layers: list of the size of the hidden layers\n",
        "        :param init_learning_rate: the initial learning rate\n",
        "        :param decay_steps: Decay steps of the learning rate\n",
        "        :param decay_rate: Decay rate of the learning rate\n",
        "        :param weights: if given, weights of the model\n",
        "        :param loss: either \"mse\" or \"categorical_crossentropy\". detrmines the loss function and the output's layer activation (linear or softmax)\n",
        "        \"\"\"\n",
        "        self.state_size = input_size\n",
        "        self.input_size = input_size\n",
        "        self.action_size = output_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = init_learning_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.decay_rate = decay_rate\n",
        "        self.hidden_layers_sizes = hidden_layers\n",
        "        self.loss = loss\n",
        "        self.step = 0\n",
        "        # self.layers = []\n",
        "\n",
        "        # lr_schedule = ExponentialDecay(\n",
        "        #     initial_learning_rate=init_learning_rate,\n",
        "        #     decay_steps=decay_steps,\n",
        "        #     decay_rate=decay_rate,\n",
        "        #     staircase=True)\n",
        "        self.learning_rate_dynamic = self.learning_rate * self.decay_rate ** (self.step / self.decay_steps)\n",
        "\n",
        "        self.optimizer = Adam(learning_rate=self.get_lr)\n",
        "        lr_metric = self.get_lr_metric(self.optimizer)\n",
        "        self.model = self.create_model(input_size, output_size, layers=list(hidden_layers), lr_metric=lr_metric, loss=loss)\n",
        "        if weights is not None:\n",
        "            self.copy_weights(weights)\n",
        "\n",
        "    # source: https://stackoverflow.com/questions/47490834/how-can-i-print-the-learning-rate-at-each-epoch-with-adam-optimizer-in-keras\n",
        "    def get_lr_metric(self, optimizer):\n",
        "        def lr(y_true, y_pred):\n",
        "            return optimizer._decayed_lr(\"float32\")\n",
        "\n",
        "        return lr\n",
        "\n",
        "    def create_model(self, input_size, output_size, lr_metric, loss, layers=[]):\n",
        "        \"\"\"\n",
        "        Creates and returns Dense NN based on the given params\n",
        "\n",
        "        :param input_size: the length of the input\n",
        "        :param output_size: the length of the output\n",
        "        :param layers: list of the size of the hidden layers\n",
        "        :param lr_metric: Learning rate metric of the optimizer\n",
        "        :param loss: either \"mse\" or \"categorical_crossentropy\". detrmines the loss function and the output's layer activation (linear or softmax)\n",
        "        :return: dense NN\n",
        "        \"\"\"\n",
        "        state_input = Input(shape=(input_size,), dtype='float64', name=\"states input\")\n",
        "\n",
        "        x = Dense(input_size, activation='relu')(state_input)  # , kernel_initializer='RandomNormal'\n",
        "        for i in range(0, len(layers)):\n",
        "            x = Dense(layers[i], activation='relu')(x)\n",
        "            # x = BatchNormalization()(x)\n",
        "            # x = Dropout(0.2)(x)\n",
        "\n",
        "        \n",
        "        if loss == \"mse\":\n",
        "          x = Dense(output_size, activation='linear')(x)\n",
        "        elif loss == \"categorical_crossentropy\":\n",
        "          x = Dense(output_size, activation='softmax')(x)\n",
        "\n",
        "        model = Model(state_input, x)\n",
        "\n",
        "        model.compile(loss=loss, optimizer=self.optimizer, metrics=['acc', lr_metric])\n",
        "\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "    def predict(self, x, np_array=False):\n",
        "        \"\"\"\n",
        "         use self.model to predict on the given x\n",
        "\n",
        "        :param x: input for the model\n",
        "        :return: prediction of the model on x\n",
        "        \"\"\"\n",
        "        if np_array:\n",
        "          return self.model(np.atleast_2d(x.astype('float64')))\n",
        "        else:\n",
        "          return self.model(x)\n",
        "    \n",
        "    def convert_to_progressive(self, sources_models):\n",
        "      input = Input(shape=(self.input_size,), dtype='float64', name=\"input\")\n",
        "\n",
        "      frozen_intermediate_layers_models = []\n",
        "      sources_models_layers_otputs = []\n",
        "      for source_model in sources_models:\n",
        "\n",
        "        # #remove last layer\n",
        "        # layer_input = source_model.input\n",
        "        # layer_output = source_model.layers[-1].output\n",
        "        # sliced_source_model = Model(inputs=layer_input, outputs=layer_output)\n",
        "        # sliced_source_model.summary()\n",
        "        current_frozen_intermediate_layers_models = {}\n",
        "        current_sources_models_layers_otputs = {}\n",
        "        #replacing the input layer to be the joint input\n",
        "        for layer_index, layer in enumerate(source_model.layers[1:]):\n",
        "          layer_output = layer.output\n",
        "          intermediate_layer_model = Model(inputs=source_model.input, outputs=layer_output)\n",
        "          print(\"\\n\\nintermediate_layer_model.summary()\")\n",
        "          intermediate_layer_model.summary()\n",
        "          intermediate_layer_model.layers.pop(0)\n",
        "          # output of layer = intermediate_layer_model(input, training=False)\n",
        "          new_output = intermediate_layer_model(input, training=False)\n",
        "          frozen_intermediate_layer_model = Model(inputs=input, outputs=new_output)\n",
        "          for layer in frozen_intermediate_layer_model.layers:\n",
        "            layer.trainable = False\n",
        "          current_frozen_intermediate_layers_models[layer_index] = frozen_intermediate_layer_model\n",
        "          current_sources_models_layers_otputs[layer_index] = new_output\n",
        "        frozen_intermediate_layers_models.append(current_frozen_intermediate_layers_models)\n",
        "        sources_models_layers_otputs.append(current_sources_models_layers_otputs)\n",
        "\n",
        "      #constructing the progressive network \n",
        "      x = Dense(self.input_size, activation='relu')(input) \n",
        "\n",
        "      # [print(frozen_source_model.layers) for frozen_source_model in frozen_source_models]\n",
        "      \n",
        "      for layer_index, layer_size in enumerate(self.hidden_layers_sizes):\n",
        "        level_layers=[x]\n",
        "        for source_model_layers_otputs in sources_models_layers_otputs:\n",
        "          level_layers.append(source_model_layers_otputs[layer_index])\n",
        "        print(\"level_layers\", level_layers)\n",
        "        x = Dense(layer_size, activation='relu')(concatenate(level_layers))\n",
        "\n",
        "      last_level_layers=[x]\n",
        "      for source_model_layers_otputs in sources_models_layers_otputs:\n",
        "        print(\"source_model_layers_otputs\", source_model_layers_otputs)\n",
        "        print(\"len(source_model_layers_otputs)\", len(source_model_layers_otputs))\n",
        "        last_level_layers.append(source_model_layers_otputs[len(source_model_layers_otputs)-2])\n",
        "      if self.loss == \"mse\":\n",
        "        x = Dense(self.output_size, activation='linear')(concatenate(last_level_layers))\n",
        "      elif self.loss == \"categorical_crossentropy\":\n",
        "        x = Dense(self.output_size, activation='softmax')(concatenate(last_level_layers))\n",
        "\n",
        "      model = Model(input, x)\n",
        "      self.learning_rate_dynamic = self.learning_rate * self.decay_rate ** (self.step / self.decay_steps)\n",
        "      self.model.optimizer = Adam(learning_rate=self.get_lr)\n",
        "      lr_metric = self.get_lr_metric(self.optimizer)\n",
        "      model.compile(loss=self.loss, optimizer=self.optimizer, metrics=['acc', lr_metric])\n",
        "      model.summary()\n",
        "      self.model=model\n",
        "    # def convert_to_progressive(self, sources_models):\n",
        "    #   #model3 = Model(input=model2.input, output=[o])\n",
        "\n",
        "    #   input = Input(shape=(self.input_size,), dtype='float64', name=\"input\")\n",
        "\n",
        "    #   frozen_source_models = []\n",
        "    #   for source_model in sources_models:\n",
        "    #     # #remove last layer\n",
        "    #     # layer_input = source_model.input\n",
        "    #     # layer_output = source_model.layers[-1].output\n",
        "    #     # sliced_source_model = Model(inputs=layer_input, outputs=layer_output)\n",
        "    #     # sliced_source_model.summary()\n",
        "    #     #replacing the input layer to be the joint input\n",
        "    #     source_model.layers.pop(0)\n",
        "    #     new_output = source_model(input, training=False)\n",
        "    #     frozen_source_model = Model(inputs=input, outputs=new_output)\n",
        "    #     frozen_source_model.summary()\n",
        "    #     for layer in frozen_source_model.layers:\n",
        "    #       layer.trainable = False\n",
        "    #     frozen_source_models.append(frozen_source_model)\n",
        "\n",
        "    #   x = Dense(self.input_size, activation='relu')(input)  # , kernel_initializer='RandomNormal'\n",
        "\n",
        "    #   [print(frozen_source_model.layers) for frozen_source_model in frozen_source_models]\n",
        "      \n",
        "    #   for layer_index, layer_size in enumerate(self.hidden_layers_sizes):\n",
        "    #     level_layers=[x]\n",
        "    #     for source_model in frozen_source_models:\n",
        "    #       level_layers.append(source_model.layers[layer_index])\n",
        "    #     print(\"level_layers\", level_layers)\n",
        "    #     x = Dense(layer_size, activation='relu')(concatenate(level_layers))\n",
        "\n",
        "    #   last_level_layers=[x]\n",
        "    #   for source_model in frozen_source_models:\n",
        "    #     last_level_layers.append(source_model.layers[-1])\n",
        "    #   if self.loss == \"mse\":\n",
        "    #     x = Dense(self.output_size, activation='linear')(concatenate(last_level_layers))\n",
        "    #   elif self.loss == \"categorical_crossentropy\":\n",
        "    #     x = Dense(self.output_size, activation='softmax')(concatenate(last_level_layers))\n",
        "\n",
        "    #   model = Model(input, x)\n",
        "    #   self.learning_rate_dynamic = self.learning_rate * self.decay_rate ** (self.step / self.decay_steps)\n",
        "    #   self.model.optimizer = Adam(learning_rate=self.get_lr)\n",
        "    #   lr_metric = self.get_lr_metric(self.optimizer)\n",
        "    #   model.compile(loss=self.loss, optimizer=self.optimizer, metrics=['acc', lr_metric])\n",
        "    #   model.summary()\n",
        "    #   self.model=model\n",
        "\n",
        "    def convert_to_transfer_model(self):\n",
        "      #trasfer weights\n",
        "      layer_input = self.model.input\n",
        "      layer_output = self.model.layers[-2].output\n",
        "      intermediate_layer_model = Model(inputs=layer_input, outputs=layer_output)\n",
        "\n",
        "      input = Input(shape=(self.input_size,), dtype='float64', name=\"states input\")\n",
        "      x = intermediate_layer_model(input, training=False)\n",
        "      \n",
        "      #add a new output layer\n",
        "      if self.loss == \"mse\":\n",
        "          x = Dense(self.output_size, activation='linear')(x)\n",
        "      elif self.loss == \"categorical_crossentropy\":\n",
        "          x = Dense(self.output_size, activation='softmax')(x)\n",
        "      \n",
        "      # lr_schedule = ExponentialDecay(\n",
        "      #       initial_learning_rate=self.learning_rate,\n",
        "      #       decay_steps=self.decay_steps,\n",
        "      #       decay_rate=self.decay_rate,\n",
        "      #       staircase=True)\n",
        "      self.learning_rate_dynamic = self.learning_rate * self.decay_rate ** (self.step / self.decay_steps)\n",
        "\n",
        "\n",
        "      self.model.optimizer = Adam(learning_rate=self.get_lr)\n",
        "      lr_metric = self.get_lr_metric(self.optimizer)\n",
        "\n",
        "      self.transfer_model = Model(input, x)\n",
        "      self.transfer_model.compile(loss=self.loss, optimizer=self.optimizer, metrics=['acc', lr_metric])\n",
        "      for layer in self.transfer_model.layers[:-1]:\n",
        "        layer.trainable = False\n",
        "      self.transfer_model.summary()\n",
        "      self.model = self.transfer_model\n",
        "    \n",
        "    def get_lr(self):\n",
        "      return self.learning_rate_dynamic\n",
        "\n",
        "    def update_lr(self):\n",
        "      self.step = self.step + 1\n",
        "      self.learning_rate_dynamic = self.learning_rate * self.decay_rate ** (self.step / self.decay_steps)\n",
        "      \n",
        "\n",
        "\n",
        "    def predict_logits(self, x, np_array=False):\n",
        "      layer_input = self.model.input\n",
        "      layer_output = self.model.layers[-2].output\n",
        "      intermediate_layer_model = Model(inputs=layer_input, outputs=layer_output)\n",
        "      if np_array:\n",
        "        return intermediate_layer_model(np.atleast_2d(x.astype('float64')))\n",
        "      else:\n",
        "        return intermediate_layer_model(x)\n",
        "\n",
        "    def copy_weights(self, other_weights):\n",
        "        \"\"\"\n",
        "        copies the given weights to self.model\n",
        "\n",
        "        :param other_weights: weights of a different model with the same structure\n",
        "        \"\"\"\n",
        "        # owm_weights = self.model.trainable_variables\n",
        "        # for v1, v2 in zip(owm_weights, other_weights):\n",
        "        #     v1.assign(v2.numpy())\n",
        "        self.model.set_weights(other_weights)\n",
        "\n",
        "    def save(self, time_stamp, env_name, prefix=\"policy\"):\n",
        "        base_path =f\"./trained_internal_models/{env_name}\"\n",
        "        if not os.path.isdir(base_path):\n",
        "            os.makedirs(base_path)\n",
        "\n",
        "        # serialize model to JSON\n",
        "        model_json = self.model.to_json()\n",
        "        with open(base_path + f\"/{prefix}_{time_stamp}.json\", \"w\") as json_file:\n",
        "            json_file.write(model_json)\n",
        "        # serialize weights to HDF5\n",
        "        self.model.save_weights(base_path + f\"/{prefix}_{time_stamp}_weights.h5\")\n",
        "        print(f\"Saved model to disk num: {prefix} {time_stamp}\")\n",
        "    \n",
        "    def load(self, time_stamp, env_name, prefix=\"policy\"):\n",
        "        base_path =f\"./trained_internal_models/{env_name}\"\n",
        "        if not os.path.isdir(base_path):\n",
        "            print(\"no model was saved yet\")\n",
        "        else:\n",
        "            # load json and create model\n",
        "            json_file = open(base_path + f\"/{prefix}_{time_stamp}.json\", 'r')\n",
        "            loaded_model_json = json_file.read()\n",
        "            json_file.close()\n",
        "            self.model = model_from_json(loaded_model_json)\n",
        "            # load weights into new model\n",
        "            self.model.load_weights(base_path + f\"/{prefix}_{time_stamp}_weights.h5\")\n",
        "            \n",
        "            self.learning_rate_dynamic = self.learning_rate * self.decay_rate ** (self.step / self.decay_steps)\n",
        "            self.model.optimizer = Adam(learning_rate=self.get_lr)\n",
        "            lr_metric = self.get_lr_metric(self.optimizer)\n",
        "\n",
        "            self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=['acc', lr_metric])\n",
        "            print(\"Loaded model from disk\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz4ct4WUqyOk"
      },
      "source": [
        "### Actor-Critic Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Lg88wW2i8r_"
      },
      "outputs": [],
      "source": [
        "class ActorCriticAgent(object):\n",
        "  def __init__(self, state_size, action_size, policy_learning_rate, value_learning_rate, \n",
        "               policy_hidden_layers, value_hidden_layers, policy_decay_rate, value_decay_rate,\n",
        "               policy_decay_steps, value_decay_steps):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    np.random.seed(2)\n",
        "    # tf.reset_default_graph()\n",
        "    # Initialize the policy network\n",
        "    self.policy = NeuralNetwork(input_size=state_size, output_size=action_size,\n",
        "                                hidden_layers=policy_hidden_layers, init_learning_rate=policy_learning_rate,\n",
        "                                decay_steps=policy_decay_steps, decay_rate=policy_decay_rate,\n",
        "                                loss=\"categorical_crossentropy\",weights=None)\n",
        "    # Initialize the value network\n",
        "    self.value = NeuralNetwork(input_size=state_size, output_size=1,\n",
        "                                hidden_layers=list(value_hidden_layers), init_learning_rate=value_learning_rate,\n",
        "                                decay_steps=value_decay_steps, decay_rate=value_decay_rate,\n",
        "                                loss=\"mse\",weights=None)\n",
        "\n",
        "  def save_models(self, env_name):\n",
        "    time_stamp = datetime.now()\n",
        "    self.policy.save(time_stamp, env_name, prefix=\"policy\")\n",
        "    self.value.save(time_stamp, env_name, prefix=\"value\")\n",
        "\n",
        "\n",
        "  def train(self, env, env_actions_count, convergence_criterion, max_episodes, max_steps, discount_factor, scaler, scale=False, discretize=[], render=False, plot=True, init_noise=1,noise_decay=0.9999):\n",
        "    np.random.seed(1)\n",
        "    writer = SummaryWriter()\n",
        "    \n",
        "    # Start training the agent with REINFORCE algorithm\n",
        "    start_time = time.time()\n",
        "\n",
        "    # with tf.Session() as sess:\n",
        "    #   sess.run(tf.global_variables_initializer())\n",
        "    solved = False\n",
        "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    episode_rewards = np.zeros(max_episodes)\n",
        "    average_rewards = 0.0\n",
        "    for episode in range(max_episodes):\n",
        "        state = env.reset()\n",
        "        state = np.pad(state, (0,self.state_size-len(state)))\n",
        "        state = state.reshape([1, self.state_size]) #NOTE\n",
        "        if scale:\n",
        "          state = scaler.state_scale(state)\n",
        "\n",
        "        I = 1\n",
        "        episode_transitions = []\n",
        "        episode_val_losses = []\n",
        "        episode_policy_losses = []\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            self.policy.update_lr()\n",
        "            self.value.update_lr()\n",
        "\n",
        "            actions_distribution = self.policy.predict(state)\n",
        "\n",
        "            if len(discretize) > 0: #Continues\n",
        "                actions_distribution = actions_distribution[0][:env_actions_count].numpy()\n",
        "                #add noise to action selection\n",
        "                init_noise=init_noise*noise_decay\n",
        "                noise = np.random.normal(0, 1, env_actions_count) * init_noise\n",
        "                actions_distribution = actions_distribution + noise\n",
        "                #select action \n",
        "                action = random.choices(np.arange(env_actions_count), weights=actions_distribution)[0]\n",
        "                action_one_hot = one_hot(action, self.action_size)\n",
        "                # # noisy action\n",
        "                action = [discretize[action]]\n",
        "            else:    \n",
        "                action = random.choices(np.arange(env_actions_count), weights=actions_distribution[0][:env_actions_count])[0]\n",
        "                action_one_hot = one_hot(action, self.action_size)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            next_state = np.pad(next_state, (0,self.state_size-len(next_state)))\n",
        "            next_state = next_state.reshape([1, self.state_size]) #NOTE\n",
        "            if scale:\n",
        "              next_state = scaler.state_scale(next_state)\n",
        "            # if render:\n",
        "            #     env.render()\n",
        "            \n",
        "            episode_transitions.append(Transition(state=state, action=action_one_hot, reward=reward, next_state=next_state, done=done))\n",
        "            \n",
        "            episode_rewards[episode] += reward\n",
        "\n",
        "            # Compute value estimations\n",
        "            estimated_return_state = self.value.predict(state)\n",
        "            if not done:\n",
        "              estimated_return_next_state = self.value.predict(next_state)\n",
        "              discounted_estimated_return_next_state = discount_factor * estimated_return_next_state\n",
        "            else:\n",
        "              discounted_estimated_return_next_state = 0\n",
        "            # Compute TD-error \n",
        "            delta = (reward + discounted_estimated_return_next_state) - estimated_return_state\n",
        "            delta_I = I * delta\n",
        "            \n",
        "            # Value function update\n",
        "            with GradientTape() as tape:\n",
        "              # val_loss = self.value.model.compiled_loss((reward + discounted_estimated_return_next_state), estimated_return_state)\n",
        "              val_loss = math.reduce_mean(square((reward + discounted_estimated_return_next_state) - self.value.predict(state)))\n",
        "            # print(\"val_loss\", val_loss)\n",
        "            # print(\"val_loss\", val_loss.numpy())\n",
        "            variables = self.value.model.trainable_weights\n",
        "            gradients = tape.gradient(val_loss, variables)\n",
        "            # print(\"gradients\", gradients)\n",
        "            self.value.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "            # value_feed_dict= {self.value.state: state, self.value.R_t: reward + discounted_estimated_return_next_state} \n",
        "            # _, val_loss = sess.run([self.value.optimizer, self.value.loss], value_feed_dict)\n",
        "            episode_val_losses.append(val_loss)\n",
        "\n",
        "            # Policy function update\n",
        "            with GradientTape() as tape:\n",
        "              neg_log_prob = self.policy.model.compiled_loss(y_true=tf.expand_dims(action_one_hot, axis=0), y_pred=self.policy.predict(state))\n",
        "              # loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "              # print(\"neg_log_prob\", neg_log_prob)\n",
        "              # neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, labels=action_one_hot)\n",
        "              policy_loss = tf.reduce_mean(neg_log_prob * delta_I)\n",
        "              # print(\"policy_loss\", policy_loss)\n",
        "\n",
        "            variables = self.policy.model.trainable_weights\n",
        "            gradients = tape.gradient(policy_loss, variables)\n",
        "            self.policy.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "            episode_policy_losses.append(policy_loss)\n",
        "            \n",
        "            if step+1 == max_steps:\n",
        "              done=True\n",
        "            if done:\n",
        "                if episode > 98:\n",
        "                    # Check if solved\n",
        "                    average_rewards = np.mean(episode_rewards[(episode - 99):episode+1])\n",
        "                    # print(\"episode_rewards\", episode_rewards[(episode - 99):episode+1])\n",
        "                print(\"Episode {}\".format(episode))\n",
        "                print(f\"Reward: {episode_rewards[episode]},Average 100 episodes: {round(average_rewards, 2)}, Mean Value Loss: {np.mean(episode_val_losses)}, Mean Policy Loss: {np.mean(episode_policy_losses)}\")\n",
        "                solved = convergence_criterion(average_rewards)\n",
        "                print(\"value lr:\", self.value.optimizer._decayed_lr(tf.float32).numpy(), \"policy lr:\", self.policy.optimizer._decayed_lr(tf.float32).numpy(), \"noise:\", init_noise)\n",
        "                if solved:\n",
        "                    print(' Solved at episode: ' + str(episode))\n",
        "                break\n",
        "            state = next_state\n",
        "            I = discount_factor * I\n",
        "        writer.add_scalar('Total Reward', episode_rewards[episode], episode)\n",
        "        writer.add_scalar('Steps', step, episode)\n",
        "        # if render:\n",
        "        #   env.play()\n",
        "\n",
        "        if solved:\n",
        "            break\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"execution time (sec): {end_time - start_time}\")\n",
        "    if plot:\n",
        "      plt.plot(range(episode),episode_rewards[:episode])\n",
        "      plt.xlabel('Eposode')\n",
        "      plt.ylabel('Rewards')\n",
        "      plt.title('The total rewared')\n",
        "      plt.show()\n",
        "    \n",
        "    return solved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9KtNoKIJgJF"
      },
      "source": [
        "### Actor-Critic Agent - Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iI--f6Ylfr2"
      },
      "outputs": [],
      "source": [
        "def sample_batch(deque_to_sample_from, n):\n",
        "  \"\"\"\n",
        "  Randomly (Uniformly) samples n instances from a given deque\n",
        "\n",
        "  :param deque_to_sample_from: a deque to sample from\n",
        "  :param n: the number of instances to sample\n",
        "  :return: the sampled instances\n",
        "  \"\"\"\n",
        "  m = min(n, len(deque_to_sample_from))\n",
        "  batch = random.sample(list(deque_to_sample_from), m)\n",
        "  return batch\n",
        "\n",
        "class ActorCriticAgent_2(object):\n",
        "  def __init__(self, state_size, action_size, policy_learning_rate, value_learning_rate, \n",
        "               policy_hidden_layers, value_hidden_layers, policy_decay_rate, value_decay_rate,\n",
        "               policy_decay_steps, value_decay_steps):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    np.random.seed(1)\n",
        "    # tf.reset_default_graph()\n",
        "    # Initialize the policy network\n",
        "    self.policy = NeuralNetwork(input_size=state_size, output_size=action_size,\n",
        "                                hidden_layers=policy_hidden_layers, init_learning_rate=policy_learning_rate,\n",
        "                                decay_steps=policy_decay_steps, decay_rate=policy_decay_rate,\n",
        "                                loss=\"categorical_crossentropy\",weights=None)\n",
        "    # Initialize the value network\n",
        "    self.value = NeuralNetwork(input_size=state_size, output_size=1,\n",
        "                                hidden_layers=list(value_hidden_layers), init_learning_rate=value_learning_rate,\n",
        "                                decay_steps=value_decay_steps, decay_rate=value_decay_rate,\n",
        "                                loss=\"mse\",weights=None)\n",
        "\n",
        "  def save_models(self, env_name):\n",
        "    time_stamp = datetime.now()\n",
        "    self.policy.save(time_stamp, env_name, prefix=\"policy\")\n",
        "    self.value.save(time_stamp, env_name, prefix=\"value\")\n",
        "  \n",
        "  def train(self, env, env_actions_count, convergence_criterion, max_episodes, max_steps, discount_factor, scaler, scale=False, discretize=[], render=False, plot=True, batch_size=16, experience_size=2000, init_noise=1,noise_decay=0.9999):\n",
        "    np.random.seed(1)\n",
        "    writer = SummaryWriter()\n",
        "    # Start training the agent with REINFORCE algorithm\n",
        "    start_time = time.time()\n",
        "\n",
        "    # with tf.Session() as sess:\n",
        "    #   sess.run(tf.global_variables_initializer())\n",
        "    solved = False\n",
        "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    episode_rewards = np.zeros(max_episodes)\n",
        "    average_rewards = 0.0\n",
        "    experience_replay = deque(maxlen=experience_size)\n",
        "\n",
        "    for episode in range(max_episodes):\n",
        "        state = env.reset()\n",
        "        state = np.pad(state, (0,self.state_size-len(state)))\n",
        "        state = state.reshape([1, self.state_size]) #NOTE\n",
        "        if scale:\n",
        "          state = scaler.state_scale(state)\n",
        "\n",
        "        I = 1\n",
        "        episode_val_losses = []\n",
        "        episode_policy_losses = []\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            self.policy.update_lr()\n",
        "            self.value.update_lr()\n",
        "\n",
        "            actions_distribution = self.policy.predict(state)\n",
        "            if len(discretize) > 0: #Continues\n",
        "                actions_distribution = actions_distribution[0][:env_actions_count].numpy()\n",
        "                #add noise to action selection\n",
        "                init_noise=init_noise*noise_decay\n",
        "                noise = np.random.normal(0, 0.2, env_actions_count) * init_noise\n",
        "                actions_distribution = actions_distribution + noise\n",
        "                #select action \n",
        "                action = random.choices(np.arange(env_actions_count), weights=actions_distribution)[0]\n",
        "                action_one_hot = one_hot(action, self.action_size)\n",
        "                action = [discretize[action]]\n",
        "            else:    \n",
        "                action = random.choices(np.arange(env_actions_count), weights=actions_distribution[0][:env_actions_count])[0]\n",
        "                action_one_hot = one_hot(action, self.action_size)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            next_state = np.pad(next_state, (0,self.state_size-len(next_state)))\n",
        "            next_state = next_state.reshape([1, self.state_size]) #NOTE\n",
        "            if scale:\n",
        "              next_state = scaler.state_scale(next_state)\n",
        "            # if render:\n",
        "            #     env.render()\n",
        "            \n",
        "            experience_replay.append(Transition(state=state, action=action_one_hot, reward=reward, next_state=next_state, done=done))\n",
        "            \n",
        "            episode_rewards[episode] += reward\n",
        "            \n",
        "            # mini batch\n",
        "            transitions_batch = sample_batch(experience_replay, batch_size)\n",
        "\n",
        "            states_batch = []\n",
        "            actions_one_hot_batch = []\n",
        "            discounted_estimated_return_next_state_batch = []\n",
        "            delta_I_batch = []\n",
        "            for transition in transitions_batch:\n",
        "              states_batch.append(transition.state[0])\n",
        "              actions_one_hot_batch.append(transition.action)\n",
        "              if not transition.done:\n",
        "                estimated_return_next_state = self.value.predict(transition.next_state)\n",
        "                discounted_estimated_return_next_state =  transition.reward + discount_factor * estimated_return_next_state\n",
        "              else:\n",
        "                discounted_estimated_return_next_state = transition.reward + 0\n",
        "              discounted_estimated_return_next_state_batch.append(discounted_estimated_return_next_state)\n",
        "              \n",
        "              estimated_return_state = self.value.predict(transition.state)\n",
        "              delta = discounted_estimated_return_next_state - estimated_return_state\n",
        "              delta_I_batch.append(I * delta)\n",
        "            \n",
        "            states_batch = np.asarray(states_batch)\n",
        "            actions_one_hot_batch = tf.convert_to_tensor(actions_one_hot_batch)\n",
        "            discounted_estimated_return_next_state_batch = np.asarray(discounted_estimated_return_next_state_batch)\n",
        "            delta_I_batch = np.asarray(delta_I_batch)\n",
        "            # Value function update\n",
        "            with GradientTape() as tape:\n",
        "              # val_loss = self.value.model.compiled_loss((reward + discounted_estimated_return_next_state), estimated_return_state)\n",
        "              val_loss = math.reduce_mean(square(discounted_estimated_return_next_state_batch - self.value.predict(states_batch)))\n",
        "            # print(\"val_loss\", val_loss)\n",
        "            # print(\"val_loss\", val_loss.numpy())\n",
        "            variables = self.value.model.trainable_weights\n",
        "            gradients = tape.gradient(val_loss, variables)\n",
        "            # print(\"gradients\", gradients)\n",
        "            self.value.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "            # value_feed_dict= {self.value.state: state, self.value.R_t: reward + discounted_estimated_return_next_state} \n",
        "            # _, val_loss = sess.run([self.value.optimizer, self.value.loss], value_feed_dict)\n",
        "            episode_val_losses.append(val_loss)\n",
        "\n",
        "            # Policy function update\n",
        "            with GradientTape() as tape:\n",
        "              neg_log_prob = self.policy.model.compiled_loss(y_true=actions_one_hot_batch, y_pred=self.policy.predict(states_batch))\n",
        "              # loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "              # print(\"neg_log_prob\", neg_log_prob)\n",
        "              # neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, labels=action_one_hot)\n",
        "              policy_loss = tf.reduce_mean(neg_log_prob * delta_I_batch)\n",
        "              # print(\"policy_loss\", policy_loss)\n",
        "\n",
        "            variables = self.policy.model.trainable_weights\n",
        "            gradients = tape.gradient(policy_loss, variables)\n",
        "            self.policy.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "            episode_policy_losses.append(policy_loss)\n",
        "            \n",
        "            if step+1 == max_steps:\n",
        "              done=True\n",
        "            if done:\n",
        "                if episode > 98:\n",
        "                    # Check if solved\n",
        "                    average_rewards = np.mean(episode_rewards[(episode - 99):episode+1])\n",
        "                    # print(\"episode_rewards\", episode_rewards[(episode - 99):episode+1])\n",
        "                print(\"Episode {}\".format(episode))\n",
        "                print(f\"Reward: {episode_rewards[episode]},Average 100 episodes: {round(average_rewards, 2)}, Mean Value Loss: {np.mean(episode_val_losses)}, Mean Policy Loss: {np.mean(episode_policy_losses)}\")\n",
        "                solved = convergence_criterion(average_rewards)\n",
        "                print(\"value lr:\", self.value.optimizer._decayed_lr(tf.float32).numpy(), \"policy lr:\", self.policy.optimizer._decayed_lr(tf.float32).numpy())\n",
        "                if solved:\n",
        "                    print(' Solved at episode: ' + str(episode))\n",
        "                break\n",
        "            state = next_state\n",
        "            I = discount_factor * I\n",
        "        writer.add_scalar('Total Reward', episode_rewards[episode], episode)\n",
        "        writer.add_scalar('Steps', step, episode)\n",
        "        # if render:\n",
        "        #   env.play()\n",
        "\n",
        "        if solved:\n",
        "            break\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"execution time (sec): {end_time - start_time}\")\n",
        "    if plot:\n",
        "      plt.plot(range(episode),episode_rewards[:episode])\n",
        "      plt.xlabel('Eposode')\n",
        "      plt.ylabel('Rewards')\n",
        "      plt.title('The total rewared')\n",
        "      plt.show()\n",
        "    \n",
        "    return solved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1poCgRel5O3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "class StateScaler:\n",
        "    def __init__(self, env, padding):\n",
        "        state_space_samples = np.array(\n",
        "            [np.pad(env.observation_space.sample(), (0, padding), 'constant') for x in range(10000)])\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.scaler.fit(state_space_samples)\n",
        "    def state_scale(self, state):\n",
        "        return self.scaler.transform(state)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J74Y10wirIuA"
      },
      "source": [
        "## Environments Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbRqnRN0l3Gb"
      },
      "outputs": [],
      "source": [
        "envs_names = ['CartPole-v1',\n",
        "              'Acrobot-v1',\n",
        "              'MountainCarContinuous-v0'] \n",
        "mountain_car_discrit_actions = [-1, 1]\n",
        "\n",
        "def get_env_actions_count(env):\n",
        "  if len(env.action_space.shape) > 0:\n",
        "    actions_count = env.action_space.shape[0]\n",
        "  else:\n",
        "    actions_count = env.action_space.n\n",
        "  return actions_count\n",
        "\n",
        "max_actions_count = len(mountain_car_discrit_actions)\n",
        "max_states_count = 0\n",
        "envs = {}\n",
        "for env_name in envs_names:\n",
        "  env = Monitor(gym.make(env_name), './video', force=True)\n",
        "\n",
        "  actions_count = get_env_actions_count(env)\n",
        "\n",
        "  states_count = env.observation_space.shape[0]\n",
        "  \n",
        "  envs[env_name] = {\"env\": gym.make(env_name), \n",
        "                    \"actions_count\": actions_count, \n",
        "                    \"states_count\": states_count}\n",
        "  \n",
        "  if actions_count > max_actions_count:\n",
        "    max_actions_count = actions_count\n",
        "  if states_count > max_states_count:\n",
        "    max_states_count = states_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGNtovEYjrMa"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "max_episodes = 500\n",
        "max_steps = 1000\n",
        "discount = 0.999\n",
        "\n",
        "envs[\"CartPole-v1\"].update({\n",
        "    \"layers_policy\": [7, 6, 6], \n",
        "    \"layers_value\": [12], \n",
        "    \"lr_policy\": 0.003, \n",
        "    \"lr_value\":0.01, \n",
        "    \"decay_steps_policy\": 600, \n",
        "    \"decay_steps_value\": 850, \n",
        "    \"decay_rate_policy\": 0.88, \n",
        "    \"decay_rate_value\": 0.92,\n",
        "    \"discretize\":[],\n",
        "    \"scale_states\": False})\n",
        "\n",
        "envs[\"Acrobot-v1\"].update({\n",
        "    \"layers_policy\": [7,7,6], \n",
        "    \"layers_value\": [12], \n",
        "    \"lr_policy\": 0.001, \n",
        "    \"lr_value\":0.01, \n",
        "    \"decay_steps_policy\": 1000, \n",
        "    \"decay_steps_value\": 1000, \n",
        "    \"decay_rate_policy\": 0.95, \n",
        "    \"decay_rate_value\": 0.98,\n",
        "    \"discretize\":[],\n",
        "    \"scale_states\": False})\n",
        "\n",
        "envs['MountainCarContinuous-v0'].update({\n",
        "    \"layers_policy\": [7,7,6], \n",
        "    \"layers_value\": [12], \n",
        "    \"lr_policy\": 0.001, \n",
        "    \"lr_value\":0.01, \n",
        "    \"decay_steps_policy\": 1000, \n",
        "    \"decay_steps_value\": 1000, \n",
        "    \"decay_rate_policy\": 0.95, \n",
        "    \"decay_rate_value\": 0.95,\n",
        "    \"discretize\":[],\n",
        "    \"actions_count\": len(mountain_car_discrit_actions), \n",
        "    \"discretize\": mountain_car_discrit_actions,\n",
        "    \"scale_states\": True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qqXo60ZrPQs"
      },
      "source": [
        "## Convergence Criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcJTbEhaI7oH"
      },
      "outputs": [],
      "source": [
        "def cart_pole_convergence_criterion(avg_rewards):\n",
        "  if avg_rewards > 475:\n",
        "    solved = True\n",
        "  else:\n",
        "    solved = False\n",
        "  return solved\n",
        "\n",
        "def acrobat_convergence_criterion(avg_rewards):\n",
        "  if avg_rewards > -90 and avg_rewards !=0.0:\n",
        "    solved = True\n",
        "  else:\n",
        "    solved = False\n",
        "  return solved\n",
        "\n",
        "def mountain_car_convergence_criterion(avg_rewards):\n",
        "  if avg_rewards > 90:\n",
        "    solved = True\n",
        "  else:\n",
        "    solved = False\n",
        "  return solved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C1dCQYq9x_Q"
      },
      "outputs": [],
      "source": [
        "envs['CartPole-v1']['convergence_criterion'] = cart_pole_convergence_criterion\n",
        "envs['Acrobot-v1']['convergence_criterion'] = acrobat_convergence_criterion\n",
        "envs['MountainCarContinuous-v0']['convergence_criterion'] = mountain_car_convergence_criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6htZmU0rd2M"
      },
      "source": [
        "## Training Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jHhlyqW4z6Pt",
        "outputId": "b7d4a7e4-f288-4ac5-9098-d4dccc6d6171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'env': <TimeLimit<CartPoleEnv<CartPole-v1>>>, 'actions_count': 2, 'states_count': 4, 'layers_policy': [7, 6, 6], 'layers_value': [12], 'lr_policy': 0.003, 'lr_value': 0.01, 'decay_steps_policy': 600, 'decay_steps_value': 850, 'decay_rate_policy': 0.88, 'decay_rate_value': 0.92, 'discretize': [], 'scale_states': False, 'convergence_criterion': <function cart_pole_convergence_criterion at 0x7f4808524710>, 'agent': <__main__.ActorCriticAgent object at 0x7f4808516090>}\n",
            "{'env': <TimeLimit<AcrobotEnv<Acrobot-v1>>>, 'actions_count': 3, 'states_count': 6, 'layers_policy': [7, 7, 6], 'layers_value': [12], 'lr_policy': 0.001, 'lr_value': 0.01, 'decay_steps_policy': 1000, 'decay_steps_value': 1000, 'decay_rate_policy': 0.95, 'decay_rate_value': 0.98, 'discretize': [], 'scale_states': False, 'convergence_criterion': <function acrobat_convergence_criterion at 0x7f48085245f0>, 'agent': <__main__.ActorCriticAgent object at 0x7f4808e62c10>}\n",
            "{'env': <TimeLimit<Continuous_MountainCarEnv<MountainCarContinuous-v0>>>, 'actions_count': 2, 'states_count': 2, 'layers_policy': [7, 7, 6], 'layers_value': [12], 'lr_policy': 0.001, 'lr_value': 0.01, 'decay_steps_policy': 1000, 'decay_steps_value': 1000, 'decay_rate_policy': 0.95, 'decay_rate_value': 0.95, 'discretize': [-1, 1], 'scale_states': True, 'convergence_criterion': <function mountain_car_convergence_criterion at 0x7f4808524560>}\n",
            "Model: \"model_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 7)                 49        \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 7)                 56        \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 6)                 48        \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 3)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 216\n",
            "Trainable params: 216\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 12)                84        \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 1)                 13        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139\n",
            "Trainable params: 139\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episode 0\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 1.6178553104400635, Mean Policy Loss: -0.3678147792816162\n",
            "value lr: 0.009500488 policy lr: 0.00095004874 noise: 0.9049233858971459\n",
            "Episode 1\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 1.417874813079834, Mean Policy Loss: -38.03241729736328\n",
            "value lr: 0.009025926 policy lr: 0.0009025926 noise: 0.8188863343435545\n",
            "Episode 2\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.6814589500427246, Mean Policy Loss: -144.78330993652344\n",
            "value lr: 0.00857507 policy lr: 0.00085750694 noise: 0.7410293943390709\n",
            "Episode 3\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 1.1452116966247559, Mean Policy Loss: -3.3390579223632812\n",
            "value lr: 0.008146734 policy lr: 0.0008146734 noise: 0.6705748285746239\n",
            "Episode 4\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.91471529006958, Mean Policy Loss: 17.82367515563965\n",
            "value lr: 0.007739794 policy lr: 0.0007739794 noise: 0.6068188443711456\n",
            "Episode 5\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.620757818222046, Mean Policy Loss: 96.78426361083984\n",
            "value lr: 0.0073531815 policy lr: 0.0007353182 noise: 0.5491245632745304\n",
            "Episode 6\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 1.8281134366989136, Mean Policy Loss: 184.1737060546875\n",
            "value lr: 0.006985881 policy lr: 0.0006985881 noise: 0.4969156590776793\n",
            "Episode 7\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.9410629272460938, Mean Policy Loss: 158.79525756835938\n",
            "value lr: 0.0066369274 policy lr: 0.0006636927 noise: 0.4496706007178857\n",
            "Episode 8\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.2769041061401367, Mean Policy Loss: 105.91813659667969\n",
            "value lr: 0.0063054045 policy lr: 0.0006305404 noise: 0.40691744254003337\n",
            "Episode 9\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.991133689880371, Mean Policy Loss: 64.44818878173828\n",
            "value lr: 0.0059904414 policy lr: 0.00059904414 noise: 0.3682291098839334\n",
            "Episode 10\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.8210840225219727, Mean Policy Loss: 36.47325134277344\n",
            "value lr: 0.005691211 policy lr: 0.0005691211 noise: 0.3332191329020614\n",
            "Episode 11\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.2721657752990723, Mean Policy Loss: 11.930832862854004\n",
            "value lr: 0.005406928 policy lr: 0.00054069277 noise: 0.3015377859914441\n",
            "Episode 12\n",
            "Reward: 91.90000000000002,Average 100 episodes: 0.0, Mean Value Loss: 207.75033569335938, Mean Policy Loss: -4.19446325302124\n",
            "value lr: 0.00538451 policy lr: 0.000538451 noise: 0.2991050740720667\n",
            "Episode 13\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 1.355630874633789, Mean Policy Loss: -8.052257537841797\n",
            "value lr: 0.0051155468 policy lr: 0.0005115547 noise: 0.2706671763683105\n",
            "Episode 14\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 1.6572273969650269, Mean Policy Loss: -11.28242015838623\n",
            "value lr: 0.004860019 policy lr: 0.0004860019 noise: 0.244933057690431\n",
            "Episode 15\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.3850836753845215, Mean Policy Loss: -18.488643646240234\n",
            "value lr: 0.0046172547 policy lr: 0.0004617255 noise: 0.22164565188336607\n",
            "Episode 16\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.581648588180542, Mean Policy Loss: -19.75693702697754\n",
            "value lr: 0.004386617 policy lr: 0.0004386617 noise: 0.2005723337716758\n",
            "Episode 17\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 1.9812991619110107, Mean Policy Loss: -30.598365783691406\n",
            "value lr: 0.0041675 policy lr: 0.00041675 noise: 0.18150259539395738\n",
            "Episode 18\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.7356364727020264, Mean Policy Loss: -37.22540283203125\n",
            "value lr: 0.003959328 policy lr: 0.0003959328 noise: 0.16424594317301947\n",
            "Episode 19\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.5194461345672607, Mean Policy Loss: -42.876834869384766\n",
            "value lr: 0.0037615546 policy lr: 0.00037615545 noise: 0.14862999501599886\n",
            "Episode 20\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.9603309631347656, Mean Policy Loss: -49.622737884521484\n",
            "value lr: 0.00357366 policy lr: 0.00035736602 noise: 0.1344987583357537\n",
            "Episode 21\n",
            "Reward: 5.300000000001106,Average 100 episodes: 0.0, Mean Value Loss: 24.380428314208984, Mean Policy Loss: -60.651466369628906\n",
            "value lr: 0.0034042192 policy lr: 0.00034042192 noise: 0.12234564956711783\n",
            "Episode 22\n",
            "Reward: 2.200000000001282,Average 100 episodes: 0.0, Mean Value Loss: 21.281492233276367, Mean Policy Loss: -53.077430725097656\n",
            "value lr: 0.0032376596 policy lr: 0.00032376597 noise: 0.11094619362315601\n",
            "Episode 23\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 1.849299430847168, Mean Policy Loss: -14.04675006866455\n",
            "value lr: 0.0030759345 policy lr: 0.00030759344 noise: 0.10039780518586681\n",
            "Episode 24\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.6062023639678955, Mean Policy Loss: -66.3975601196289\n",
            "value lr: 0.0029222874 policy lr: 0.00029222877 noise: 0.09085232180543665\n",
            "Episode 25\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.3304593563079834, Mean Policy Loss: 1.5279029607772827\n",
            "value lr: 0.0027763157 policy lr: 0.00027763157 noise: 0.08221439066479266\n",
            "Episode 26\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.901182174682617, Mean Policy Loss: -46.64237594604492\n",
            "value lr: 0.002637635 policy lr: 0.0002637635 noise: 0.07439772476985483\n",
            "Episode 27\n",
            "Reward: 75.09999999999992,Average 100 episodes: 0.0, Mean Value Loss: 83.82086181640625, Mean Policy Loss: -90.73412322998047\n",
            "value lr: 0.0026041614 policy lr: 0.00026041613 noise: 0.07256800449380629\n",
            "Episode 28\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.2984330654144287, Mean Policy Loss: -34.25505065917969\n",
            "value lr: 0.0024740803 policy lr: 0.00024740802 noise: 0.06566848433433434\n",
            "Episode 29\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 1.7178503274917603, Mean Policy Loss: -48.502960205078125\n",
            "value lr: 0.0023504968 policy lr: 0.00023504968 noise: 0.059424947190559525\n",
            "Episode 30\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 1.9113825559616089, Mean Policy Loss: -42.51543045043945\n",
            "value lr: 0.0022330864 policy lr: 0.00022330864 noise: 0.05377502441844015\n",
            "Episode 31\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 1.9089562892913818, Mean Policy Loss: -32.78230285644531\n",
            "value lr: 0.002121541 policy lr: 0.00021215409 noise: 0.04866227717343652\n",
            "Episode 32\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.2348296642303467, Mean Policy Loss: -40.48588562011719\n",
            "value lr: 0.0020155674 policy lr: 0.00020155673 noise: 0.0440356326252516\n",
            "Episode 33\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.6878585815429688, Mean Policy Loss: -34.88629150390625\n",
            "value lr: 0.0019148871 policy lr: 0.00019148871 noise: 0.03984887377536542\n",
            "Episode 34\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.710376501083374, Mean Policy Loss: -17.813621520996094\n",
            "value lr: 0.0018192361 policy lr: 0.0001819236 noise: 0.03606017778099176\n",
            "Episode 35\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.6142098903656006, Mean Policy Loss: -29.8199405670166\n",
            "value lr: 0.001728363 policy lr: 0.0001728363 noise: 0.03263169817362803\n",
            "Episode 36\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.7779648303985596, Mean Policy Loss: -14.841595649719238\n",
            "value lr: 0.001642029 policy lr: 0.0001642029 noise: 0.02952918679885319\n",
            "Episode 37\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.9069647789001465, Mean Policy Loss: -21.65574836730957\n",
            "value lr: 0.0015600076 policy lr: 0.00015600075 noise: 0.026721651700807474\n",
            "Episode 38\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.0124239921569824, Mean Policy Loss: -23.559772491455078\n",
            "value lr: 0.0014820832 policy lr: 0.00014820832 noise: 0.024181047533858944\n",
            "Episode 39\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.5000226497650146, Mean Policy Loss: -30.368244171142578\n",
            "value lr: 0.0014080512 policy lr: 0.00014080513 noise: 0.021881995408879428\n",
            "Episode 40\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.1695451736450195, Mean Policy Loss: -15.175088882446289\n",
            "value lr: 0.0013377173 policy lr: 0.00013377173 noise: 0.019801529375589016\n",
            "Episode 41\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.6013402938842773, Mean Policy Loss: -27.631792068481445\n",
            "value lr: 0.0012708966 policy lr: 0.00012708966 noise: 0.017918867008499775\n",
            "Episode 42\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.450885057449341, Mean Policy Loss: -22.775463104248047\n",
            "value lr: 0.0012074137 policy lr: 0.00012074137 noise: 0.01621520180477221\n",
            "Episode 43\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 4.068389892578125, Mean Policy Loss: -23.15296173095703\n",
            "value lr: 0.0011471019 policy lr: 0.00011471019 noise: 0.014673515320179964\n",
            "Episode 44\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.009272336959839, Mean Policy Loss: -16.34741973876953\n",
            "value lr: 0.0010898027 policy lr: 0.00010898027 noise: 0.01327840716655091\n",
            "Episode 45\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.79533314704895, Mean Policy Loss: -9.46789836883545\n",
            "value lr: 0.0010353656 policy lr: 0.000103536564 noise: 0.012015941172476178\n",
            "Episode 46\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.854726552963257, Mean Policy Loss: -12.533833503723145\n",
            "value lr: 0.0009836479 policy lr: 9.8364784e-05 noise: 0.010873506170538073\n",
            "Episode 47\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.801302433013916, Mean Policy Loss: -16.729198455810547\n",
            "value lr: 0.0009345134 policy lr: 9.345134e-05 noise: 0.009839690020416821\n",
            "Episode 48\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.221778392791748, Mean Policy Loss: -14.805957794189453\n",
            "value lr: 0.00088783324 policy lr: 8.878332e-05 noise: 0.008904165609453955\n",
            "Episode 49\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.91774582862854, Mean Policy Loss: -16.854236602783203\n",
            "value lr: 0.0008434848 policy lr: 8.434849e-05 noise: 0.008057587691896016\n",
            "Episode 50\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.2515859603881836, Mean Policy Loss: -8.371336936950684\n",
            "value lr: 0.00080135174 policy lr: 8.013517e-05 noise: 0.007291499536313735\n",
            "Episode 51\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.689640760421753, Mean Policy Loss: -6.7750749588012695\n",
            "value lr: 0.0007613232 policy lr: 7.613232e-05 noise: 0.006598248448668482\n",
            "Episode 52\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.2334277629852295, Mean Policy Loss: -11.275521278381348\n",
            "value lr: 0.0007232941 policy lr: 7.2329414e-05 noise: 0.005970909327159675\n",
            "Episode 53\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.2623064517974854, Mean Policy Loss: -9.894381523132324\n",
            "value lr: 0.00068716466 policy lr: 6.8716465e-05 noise: 0.005403215485218181\n",
            "Episode 54\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.228253126144409, Mean Policy Loss: -13.045886039733887\n",
            "value lr: 0.0006528399 policy lr: 6.528399e-05 noise: 0.004889496051615529\n",
            "Episode 55\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.656108856201172, Mean Policy Loss: 0.0\n",
            "value lr: 0.00062022975 policy lr: 6.202297e-05 noise: 0.004424619322358651\n",
            "Episode 56\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.957947254180908, Mean Policy Loss: -2.6287403106689453\n",
            "value lr: 0.0005892485 policy lr: 5.8924845e-05 noise: 0.004003941498494713\n",
            "Episode 57\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.683906078338623, Mean Policy Loss: -5.482580184936523\n",
            "value lr: 0.00055981474 policy lr: 5.5981476e-05 noise: 0.003623260297751926\n",
            "Episode 58\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.774779796600342, Mean Policy Loss: -4.751747131347656\n",
            "value lr: 0.0005318513 policy lr: 5.318513e-05 noise: 0.00327877297662838\n",
            "Episode 59\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.9207911491394043, Mean Policy Loss: -7.468705177307129\n",
            "value lr: 0.00050528464 policy lr: 5.0528466e-05 noise: 0.002967038343598621\n",
            "Episode 60\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.225632667541504, Mean Policy Loss: 0.0\n",
            "value lr: 0.00048004504 policy lr: 4.8004506e-05 noise: 0.0026849423839759167\n",
            "Episode 61\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 2.969329833984375, Mean Policy Loss: -3.0337252616882324\n",
            "value lr: 0.0004560662 policy lr: 4.560662e-05 noise: 0.002429667153046237\n",
            "Episode 62\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.02024245262146, Mean Policy Loss: 0.0\n",
            "value lr: 0.00043328508 policy lr: 4.332851e-05 noise: 0.0021986626267376753\n",
            "Episode 63\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.7720274925231934, Mean Policy Loss: -8.956452369689941\n",
            "value lr: 0.00041164196 policy lr: 4.1164196e-05 noise: 0.001989621228632961\n",
            "Episode 64\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.392834424972534, Mean Policy Loss: 0.0\n",
            "value lr: 0.00039107993 policy lr: 3.9107992e-05 noise: 0.0018004547788673774\n",
            "Episode 65\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.920665979385376, Mean Policy Loss: 0.0\n",
            "value lr: 0.00037154497 policy lr: 3.71545e-05 noise: 0.001629273634647363\n",
            "Episode 66\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.233656644821167, Mean Policy Loss: 0.0\n",
            "value lr: 0.00035298584 policy lr: 3.5298584e-05 noise: 0.001474367814018043\n",
            "Episode 67\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.226411819458008, Mean Policy Loss: 0.0\n",
            "value lr: 0.00033535375 policy lr: 3.3535372e-05 noise: 0.0013341899143189822\n",
            "Episode 68\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.230290651321411, Mean Policy Loss: 0.0\n",
            "value lr: 0.0003186024 policy lr: 3.186024e-05 noise: 0.001207339654695358\n",
            "Episode 69\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.187394380569458, Mean Policy Loss: 0.0\n",
            "value lr: 0.0003026878 policy lr: 3.026878e-05 noise: 0.0010925498882548182\n",
            "Episode 70\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.2579004764556885, Mean Policy Loss: 0.0\n",
            "value lr: 0.00028756817 policy lr: 2.8756816e-05 noise: 0.0009886739441410965\n",
            "Episode 71\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.9825422763824463, Mean Policy Loss: 0.0\n",
            "value lr: 0.00027320377 policy lr: 2.7320377e-05 noise: 0.0008946741730804465\n",
            "Episode 72\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.769890785217285, Mean Policy Loss: 0.0\n",
            "value lr: 0.0002595569 policy lr: 2.595569e-05 noise: 0.000809611581978688\n",
            "Episode 73\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.556183338165283, Mean Policy Loss: -7.8630547523498535\n",
            "value lr: 0.0002465917 policy lr: 2.465917e-05 noise: 0.0007326364540257001\n",
            "Episode 74\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.2261581420898438, Mean Policy Loss: -3.9509942531585693\n",
            "value lr: 0.00023427412 policy lr: 2.3427412e-05 noise: 0.0006629798606086145\n",
            "Episode 75\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.213963508605957, Mean Policy Loss: 0.0\n",
            "value lr: 0.00022257183 policy lr: 2.2257183e-05 noise: 0.0005999459802435645\n",
            "Episode 76\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.82241153717041, Mean Policy Loss: 0.0\n",
            "value lr: 0.0002114541 policy lr: 2.114541e-05 noise: 0.0005429051477973892\n",
            "Episode 77\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.0213425159454346, Mean Policy Loss: 0.0\n",
            "value lr: 0.00020089169 policy lr: 2.008917e-05 noise: 0.0004912875645658053\n",
            "Episode 78\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.2702887058258057, Mean Policy Loss: -3.693514108657837\n",
            "value lr: 0.0001908569 policy lr: 1.908569e-05 noise: 0.000444577606376051\n",
            "Episode 79\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.9965932369232178, Mean Policy Loss: 0.0\n",
            "value lr: 0.00018132335 policy lr: 1.8132336e-05 noise: 0.00040230867285586405\n",
            "Episode 80\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.3478751182556152, Mean Policy Loss: -1.6189738512039185\n",
            "value lr: 0.00017226602 policy lr: 1.7226603e-05 noise: 0.00036405852641651515\n",
            "Episode 81\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.1210086345672607, Mean Policy Loss: 0.0\n",
            "value lr: 0.00016366111 policy lr: 1.6366112e-05 noise: 0.00032944507438955874\n",
            "Episode 82\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 4.289531707763672, Mean Policy Loss: 0.0\n",
            "value lr: 0.00015548604 policy lr: 1.5548603e-05 noise: 0.000298122552183736\n",
            "Episode 83\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.4511590003967285, Mean Policy Loss: 0.0\n",
            "value lr: 0.00014771931 policy lr: 1.4771931e-05 noise: 0.00026977806933440484\n",
            "Episode 84\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.22514271736145, Mean Policy Loss: 0.0\n",
            "value lr: 0.00014034055 policy lr: 1.4034054e-05 noise: 0.00024412848394288385\n",
            "Episode 85\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 4.012447834014893, Mean Policy Loss: 0.0\n",
            "value lr: 0.00013333035 policy lr: 1.3333035e-05 noise: 0.00022091757428353143\n",
            "Episode 86\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.749633312225342, Mean Policy Loss: 0.0\n",
            "value lr: 0.00012667033 policy lr: 1.2667033e-05 noise: 0.00019991347932483776\n",
            "Episode 87\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.9957265853881836, Mean Policy Loss: 0.0\n",
            "value lr: 0.000120342986 policy lr: 1.2034298e-05 noise: 0.00018090638259711135\n",
            "Episode 88\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.6124422550201416, Mean Policy Loss: 0.0\n",
            "value lr: 0.000114331706 policy lr: 1.1433171e-05 noise: 0.00016370641627018242\n",
            "Episode 89\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.93827223777771, Mean Policy Loss: 0.0\n",
            "value lr: 0.00010862069 policy lr: 1.0862069e-05 noise: 0.00014814176450430076\n",
            "Episode 90\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.4461095333099365, Mean Policy Loss: 0.0\n",
            "value lr: 0.00010319495 policy lr: 1.0319495e-05 noise: 0.00013405694712800934\n",
            "Episode 91\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.2446186542510986, Mean Policy Loss: 0.0\n",
            "value lr: 9.804023e-05 policy lr: 9.804023e-06 noise: 0.0001213112664981131\n",
            "Episode 92\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.5773203372955322, Mean Policy Loss: 0.0\n",
            "value lr: 9.3143e-05 policy lr: 9.3143e-06 noise: 0.00010977740202694379\n",
            "Episode 93\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 4.013252258300781, Mean Policy Loss: 0.0\n",
            "value lr: 8.8490386e-05 policy lr: 8.849039e-06 noise: 9.93401383372144e-05\n",
            "Episode 94\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.3952183723449707, Mean Policy Loss: 0.0\n",
            "value lr: 8.4070176e-05 policy lr: 8.407018e-06 noise: 8.989521433960277e-05\n",
            "Episode 95\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.550266981124878, Mean Policy Loss: 0.0\n",
            "value lr: 7.987076e-05 policy lr: 7.987076e-06 noise: 8.134828173614298e-05\n",
            "Episode 96\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.739827871322632, Mean Policy Loss: 0.0\n",
            "value lr: 7.588112e-05 policy lr: 7.588112e-06 noise: 7.361396254558524e-05\n",
            "Episode 97\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 3.997616767883301, Mean Policy Loss: 0.0\n",
            "value lr: 7.209076e-05 policy lr: 7.209076e-06 noise: 6.661499623605651e-05\n",
            "Episode 98\n",
            "Reward: -99.8999999999986,Average 100 episodes: 0.0, Mean Value Loss: 4.226924896240234, Mean Policy Loss: 0.0\n",
            "value lr: 6.848974e-05 policy lr: 6.8489735e-06 noise: 6.028146794545771e-05\n",
            "Episode 99\n",
            "Reward: -99.8999999999986,Average 100 episodes: -94.16, Mean Value Loss: 3.2661385536193848, Mean Policy Loss: -1.4868475198745728\n",
            "value lr: 6.506858e-05 policy lr: 6.5068584e-06 noise: 5.45501100800539e-05\n",
            "Episode 100\n",
            "Reward: -99.8999999999986,Average 100 episodes: -94.16, Mean Value Loss: 3.228025197982788, Mean Policy Loss: 0.0\n",
            "value lr: 6.1818326e-05 policy lr: 6.1818328e-06 noise: 4.93636703147044e-05\n",
            "Episode 101\n",
            "Reward: -99.8999999999986,Average 100 episodes: -94.16, Mean Value Loss: 3.2363901138305664, Mean Policy Loss: 0.0\n",
            "value lr: 5.8730424e-05 policy lr: 5.8730425e-06 noise: 4.467033968149275e-05\n",
            "Episode 102\n",
            "Reward: -99.8999999999986,Average 100 episodes: -94.16, Mean Value Loss: 3.913647174835205, Mean Policy Loss: 0.0\n",
            "value lr: 5.5796765e-05 policy lr: 5.5796763e-06 noise: 4.042323503375211e-05\n",
            "Episode 103\n",
            "Reward: -99.8999999999986,Average 100 episodes: -94.16, Mean Value Loss: 3.8510053157806396, Mean Policy Loss: 0.0\n",
            "value lr: 5.3009644e-05 policy lr: 5.3009644e-06 noise: 3.657993071565922e-05\n",
            "Episode 104\n",
            "Reward: -99.8999999999986,Average 100 episodes: -94.16, Mean Value Loss: 3.4161620140075684, Mean Policy Loss: 0.0\n",
            "value lr: 5.0361745e-05 policy lr: 5.0361746e-06 noise: 3.310203475909731e-05\n",
            "Episode 105\n",
            "Reward: -99.8999999999986,Average 100 episodes: -94.16, Mean Value Loss: 4.203075408935547, Mean Policy Loss: 0.0\n",
            "value lr: 4.784611e-05 policy lr: 4.7846115e-06 noise: 2.9954805374287373e-05\n",
            "Episode 106\n",
            "Reward: -99.8999999999986,Average 100 episodes: -94.16, Mean Value Loss: 3.463728189468384, Mean Policy Loss: 0.0\n",
            "value lr: 4.5456138e-05 policy lr: 4.545614e-06 noise: 2.710680390319012e-05\n",
            "Episode 107\n",
            "Reward: -99.8999999999986,Average 100 episodes: -94.16, Mean Value Loss: 3.68355393409729, Mean Policy Loss: 0.0\n",
            "value lr: 4.3185548e-05 policy lr: 4.3185546e-06 noise: 2.4529580768924805e-05\n",
            "Episode 108\n",
            "Reward: -99.8999999999986,Average 100 episodes: -94.16, Mean Value Loss: 3.80881929397583, Mean Policy Loss: 0.0\n",
            "value lr: 4.1028376e-05 policy lr: 4.1028375e-06 noise: 2.2197391284052984e-05\n",
            "Episode 109\n",
            "Reward: -99.8999999999986,Average 100 episodes: -94.16, Mean Value Loss: 3.2251341342926025, Mean Policy Loss: 0.0\n",
            "value lr: 3.8978953e-05 policy lr: 3.8978956e-06 noise: 2.0086938478849056e-05\n",
            "Episode 110\n",
            "Reward: -99.8999999999986,Average 100 episodes: -94.16, Mean Value Loss: 3.363563060760498, Mean Policy Loss: 0.0\n",
            "value lr: 3.7031907e-05 policy lr: 3.7031907e-06 noise: 1.8177140380587707e-05\n",
            "Episode 111\n",
            "Reward: -99.8999999999986,Average 100 episodes: -94.16, Mean Value Loss: 3.8983678817749023, Mean Policy Loss: 0.0\n",
            "value lr: 3.5182115e-05 policy lr: 3.5182115e-06 noise: 1.6448919419129185e-05\n",
            "Episode 112\n",
            "Reward: -99.8999999999986,Average 100 episodes: -96.08, Mean Value Loss: 3.237928867340088, Mean Policy Loss: 0.0\n",
            "value lr: 3.3424723e-05 policy lr: 3.3424724e-06 noise: 1.488501185510774e-05\n",
            "Episode 113\n",
            "Reward: -99.8999999999986,Average 100 episodes: -96.08, Mean Value Loss: 3.2416815757751465, Mean Policy Loss: 0.0\n",
            "value lr: 3.1755117e-05 policy lr: 3.1755117e-06 noise: 1.3469795327043225e-05\n",
            "Episode 114\n",
            "Reward: -99.8999999999986,Average 100 episodes: -96.08, Mean Value Loss: 4.162946701049805, Mean Policy Loss: 0.0\n",
            "value lr: 3.0168909e-05 policy lr: 3.0168908e-06 noise: 1.2189132794689515e-05\n",
            "Episode 115\n",
            "Reward: -99.8999999999986,Average 100 episodes: -96.08, Mean Value Loss: 3.948593854904175, Mean Policy Loss: 0.0\n",
            "value lr: 2.8661932e-05 policy lr: 2.8661934e-06 noise: 1.103023131972038e-05\n",
            "Episode 116\n",
            "Reward: -99.8999999999986,Average 100 episodes: -96.08, Mean Value Loss: 3.2813918590545654, Mean Policy Loss: 0.0\n",
            "value lr: 2.7230233e-05 policy lr: 2.7230233e-06 noise: 9.981514273070101e-06\n",
            "Episode 117\n",
            "Reward: -99.8999999999986,Average 100 episodes: -96.08, Mean Value Loss: 3.732182264328003, Mean Policy Loss: 0.0\n",
            "value lr: 2.587005e-05 policy lr: 2.587005e-06 noise: 9.032505692367293e-06\n",
            "Episode 118\n",
            "Reward: -99.8999999999986,Average 100 episodes: -96.08, Mean Value Loss: 3.998802423477173, Mean Policy Loss: 0.0\n",
            "value lr: 2.4577806e-05 policy lr: 2.4577807e-06 noise: 8.17372563427227e-06\n",
            "Episode 119\n",
            "Reward: -99.8999999999986,Average 100 episodes: -96.08, Mean Value Loss: 3.8306314945220947, Mean Policy Loss: 0.0\n",
            "value lr: 2.3350114e-05 policy lr: 2.3350115e-06 noise: 7.396595476359952e-06\n",
            "Episode 120\n",
            "Reward: -99.8999999999986,Average 100 episodes: -96.08, Mean Value Loss: 3.4027092456817627, Mean Policy Loss: 0.0\n",
            "value lr: 2.2183745e-05 policy lr: 2.2183747e-06 noise: 6.693352222579152e-06\n",
            "Episode 121\n",
            "Reward: -99.8999999999986,Average 100 episodes: -97.13, Mean Value Loss: 3.952867269515991, Mean Policy Loss: 0.0\n",
            "value lr: 2.107564e-05 policy lr: 2.107564e-06 noise: 6.0569709562585245e-06\n",
            "Episode 122\n",
            "Reward: -99.8999999999986,Average 100 episodes: -98.15, Mean Value Loss: 3.067553997039795, Mean Policy Loss: 0.0\n",
            "value lr: 2.0022884e-05 policy lr: 2.0022885e-06 noise: 5.481094666018139e-06\n",
            "Episode 123\n",
            "Reward: -99.8999999999986,Average 100 episodes: -98.15, Mean Value Loss: 4.191868305206299, Mean Policy Loss: 0.0\n",
            "value lr: 1.9022717e-05 policy lr: 1.9022716e-06 noise: 4.959970743595915e-06\n",
            "Episode 124\n",
            "Reward: -99.8999999999986,Average 100 episodes: -98.15, Mean Value Loss: 3.528111457824707, Mean Policy Loss: 0.0\n",
            "value lr: 1.8072507e-05 policy lr: 1.8072508e-06 noise: 4.488393519245596e-06\n",
            "Episode 125\n",
            "Reward: -99.8999999999986,Average 100 episodes: -98.15, Mean Value Loss: 3.8924760818481445, Mean Policy Loss: 0.0\n",
            "value lr: 1.7169763e-05 policy lr: 1.7169763e-06 noise: 4.061652260674526e-06\n",
            "Episode 126\n",
            "Reward: -99.8999999999986,Average 100 episodes: -98.15, Mean Value Loss: 3.2550418376922607, Mean Policy Loss: 0.0\n",
            "value lr: 1.6312111e-05 policy lr: 1.6312111e-06 noise: 3.6754841160663788e-06\n",
            "Episode 127\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.8964030742645264, Mean Policy Loss: 0.0\n",
            "value lr: 1.54973e-05 policy lr: 1.5497301e-06 noise: 3.3260315311219665e-06\n",
            "Episode 128\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.006410837173462, Mean Policy Loss: 0.0\n",
            "value lr: 1.472319e-05 policy lr: 1.4723191e-06 noise: 3.0098037147435637e-06\n",
            "Episode 129\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.3954968452453613, Mean Policy Loss: 0.0\n",
            "value lr: 1.3987748e-05 policy lr: 1.3987749e-06 noise: 2.723641768431548e-06\n",
            "Episode 130\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.692826509475708, Mean Policy Loss: 0.0\n",
            "value lr: 1.3289043e-05 policy lr: 1.3289043e-06 noise: 2.4646871310599707e-06\n",
            "Episode 131\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 4.137160301208496, Mean Policy Loss: 0.0\n",
            "value lr: 1.2625238e-05 policy lr: 1.2625238e-06 noise: 2.230353023815914e-06\n",
            "Episode 132\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.8858790397644043, Mean Policy Loss: 0.0\n",
            "value lr: 1.1994592e-05 policy lr: 1.1994591e-06 noise: 2.0182986100574323e-06\n",
            "Episode 133\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.272895336151123, Mean Policy Loss: 0.0\n",
            "value lr: 1.1395447e-05 policy lr: 1.1395447e-06 noise: 1.8264056119646766e-06\n",
            "Episode 134\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 4.015657901763916, Mean Policy Loss: 0.0\n",
            "value lr: 1.0826229e-05 policy lr: 1.0826229e-06 noise: 1.6527571504006236e-06\n",
            "Episode 135\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.408790111541748, Mean Policy Loss: 0.0\n",
            "value lr: 1.0285446e-05 policy lr: 1.0285446e-06 noise: 1.4956185966062507e-06\n",
            "Episode 136\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2449727058410645, Mean Policy Loss: 0.0\n",
            "value lr: 9.771675e-06 policy lr: 9.771675e-07 noise: 1.3534202444516689e-06\n",
            "Episode 137\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.243192672729492, Mean Policy Loss: 0.0\n",
            "value lr: 9.283567e-06 policy lr: 9.283567e-07 noise: 1.2247416301509467e-06\n",
            "Episode 138\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.270416259765625, Mean Policy Loss: 0.0\n",
            "value lr: 8.819841e-06 policy lr: 8.819841e-07 noise: 1.1082973428053832e-06\n",
            "Episode 139\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.236707925796509, Mean Policy Loss: 0.0\n",
            "value lr: 8.379278e-06 policy lr: 8.3792787e-07 noise: 1.0029241840322565e-06\n",
            "Episode 140\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.332386016845703, Mean Policy Loss: 0.0\n",
            "value lr: 7.960723e-06 policy lr: 7.960723e-07 noise: 9.075695484126026e-07\n",
            "Episode 141\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.7272253036499023, Mean Policy Loss: 0.0\n",
            "value lr: 7.563075e-06 policy lr: 7.563075e-07 noise: 8.212809086866758e-07\n",
            "Episode 142\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.275221586227417, Mean Policy Loss: 0.0\n",
            "value lr: 7.1852896e-06 policy lr: 7.1852895e-07 noise: 7.431963006614312e-07\n",
            "Episode 143\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2705116271972656, Mean Policy Loss: 0.0\n",
            "value lr: 6.8263753e-06 policy lr: 6.8263756e-07 noise: 6.725357127807769e-07\n",
            "Episode 144\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.963191032409668, Mean Policy Loss: 0.0\n",
            "value lr: 6.4853893e-06 policy lr: 6.485389e-07 noise: 6.085932943463302e-07\n",
            "Episode 145\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.593170642852783, Mean Policy Loss: 0.0\n",
            "value lr: 6.161436e-06 policy lr: 6.161436e-07 noise: 5.507303045541789e-07\n",
            "Episode 146\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 4.0043721199035645, Mean Policy Loss: 0.0\n",
            "value lr: 5.8536643e-06 policy lr: 5.853664e-07 noise: 4.983687319133341e-07\n",
            "Episode 147\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.322427988052368, Mean Policy Loss: 0.0\n",
            "value lr: 5.5612663e-06 policy lr: 5.561266e-07 noise: 4.509855203082805e-07\n",
            "Episode 148\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2351462841033936, Mean Policy Loss: 0.0\n",
            "value lr: 5.283474e-06 policy lr: 5.283474e-07 noise: 4.0810734402795497e-07\n",
            "Episode 149\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.3473758697509766, Mean Policy Loss: 0.0\n",
            "value lr: 5.0195576e-06 policy lr: 5.0195575e-07 noise: 3.6930587956726797e-07\n",
            "Episode 150\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2366409301757812, Mean Policy Loss: 0.0\n",
            "value lr: 4.7688245e-06 policy lr: 4.7688246e-07 noise: 3.3419352696973545e-07\n",
            "Episode 151\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.228123903274536, Mean Policy Loss: 0.0\n",
            "value lr: 4.5306156e-06 policy lr: 4.5306157e-07 noise: 3.024195379703628e-07\n",
            "Episode 152\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.941612958908081, Mean Policy Loss: 0.0\n",
            "value lr: 4.3043055e-06 policy lr: 4.3043056e-07 noise: 2.736665122615914e-07\n",
            "Episode 153\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.3141114711761475, Mean Policy Loss: 0.0\n",
            "value lr: 4.0893e-06 policy lr: 4.0893002e-07 noise: 2.476472268824219e-07\n",
            "Episode 154\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.268375873565674, Mean Policy Loss: 0.0\n",
            "value lr: 3.8850344e-06 policy lr: 3.8850342e-07 noise: 2.241017670584802e-07\n",
            "Episode 155\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.300025701522827, Mean Policy Loss: 0.0\n",
            "value lr: 3.6909719e-06 policy lr: 3.690972e-07 noise: 2.0279492983209353e-07\n",
            "Episode 156\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2453911304473877, Mean Policy Loss: 0.0\n",
            "value lr: 3.5066032e-06 policy lr: 3.5066032e-07 noise: 1.8351387454643213e-07\n",
            "Episode 157\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.996572971343994, Mean Policy Loss: 0.0\n",
            "value lr: 3.3314438e-06 policy lr: 3.3314438e-07 noise: 1.6606599671366124e-07\n",
            "Episode 158\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.290531635284424, Mean Policy Loss: 0.0\n",
            "value lr: 3.165034e-06 policy lr: 3.1650342e-07 noise: 1.5027700402851084e-07\n",
            "Episode 159\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.3372766971588135, Mean Policy Loss: 0.0\n",
            "value lr: 3.0069366e-06 policy lr: 3.0069367e-07 noise: 1.3598917530795922e-07\n",
            "Episode 160\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2388477325439453, Mean Policy Loss: 0.0\n",
            "value lr: 2.8567363e-06 policy lr: 2.8567362e-07 noise: 1.230597849650392e-07\n",
            "Episode 161\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 2.9781112670898438, Mean Policy Loss: 0.0\n",
            "value lr: 2.7140386e-06 policy lr: 2.7140388e-07 noise: 1.1135967727833793e-07\n",
            "Episode 162\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.1754045486450195, Mean Policy Loss: 0.0\n",
            "value lr: 2.578469e-06 policy lr: 2.578469e-07 noise: 1.007719762151271e-07\n",
            "Episode 163\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.309964656829834, Mean Policy Loss: 0.0\n",
            "value lr: 2.4496712e-06 policy lr: 2.449671e-07 noise: 9.119091792013934e-08\n",
            "Episode 164\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.247002363204956, Mean Policy Loss: 0.0\n",
            "value lr: 2.3273071e-06 policy lr: 2.327307e-07 noise: 8.25207942073612e-08\n",
            "Episode 165\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.542072057723999, Mean Policy Loss: 0.0\n",
            "value lr: 2.211055e-06 policy lr: 2.2110551e-07 noise: 7.467499650104679e-08\n",
            "Episode 166\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.6822586059570312, Mean Policy Loss: 0.0\n",
            "value lr: 2.1006101e-06 policy lr: 2.10061e-07 noise: 6.75751506755849e-08\n",
            "Episode 167\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2909908294677734, Mean Policy Loss: 0.0\n",
            "value lr: 1.995682e-06 policy lr: 1.9956819e-07 noise: 6.115033415186007e-08\n",
            "Episode 168\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.9766042232513428, Mean Policy Loss: 0.0\n",
            "value lr: 1.8959951e-06 policy lr: 1.895995e-07 noise: 5.533636742944317e-08\n",
            "Episode 169\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.259556531906128, Mean Policy Loss: 0.0\n",
            "value lr: 1.8012877e-06 policy lr: 1.8012878e-07 noise: 5.0075172977500286e-08\n",
            "Episode 170\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.3423943519592285, Mean Policy Loss: 0.0\n",
            "value lr: 1.7113111e-06 policy lr: 1.711311e-07 noise: 4.5314195080184755e-08\n",
            "Episode 171\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 4.014659404754639, Mean Policy Loss: 0.0\n",
            "value lr: 1.625829e-06 policy lr: 1.625829e-07 noise: 4.100587484116455e-08\n",
            "Episode 172\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.36617374420166, Mean Policy Loss: 0.0\n",
            "value lr: 1.5446167e-06 policy lr: 1.5446167e-07 noise: 3.710717510294114e-08\n",
            "Episode 173\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.8894574642181396, Mean Policy Loss: 0.0\n",
            "value lr: 1.4674612e-06 policy lr: 1.4674612e-07 noise: 3.357915053523173e-08\n",
            "Episode 174\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 4.273782253265381, Mean Policy Loss: 0.0\n",
            "value lr: 1.3941597e-06 policy lr: 1.3941596e-07 noise: 3.038655859789186e-08\n",
            "Episode 175\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.1992712020874023, Mean Policy Loss: 0.0\n",
            "value lr: 1.3245195e-06 policy lr: 1.3245196e-07 noise: 2.749750749216634e-08\n",
            "Episode 176\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 4.287369728088379, Mean Policy Loss: 0.0\n",
            "value lr: 1.2583581e-06 policy lr: 1.2583581e-07 noise: 2.4883137583543274e-08\n",
            "Episode 177\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 2.993044853210449, Mean Policy Loss: 0.0\n",
            "value lr: 1.1955016e-06 policy lr: 1.1955015e-07 noise: 2.2517333113844493e-08\n",
            "Episode 178\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.7146873474121094, Mean Policy Loss: 0.0\n",
            "value lr: 1.1357847e-06 policy lr: 1.1357847e-07 noise: 2.0376461322754074e-08\n",
            "Episode 179\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2701659202575684, Mean Policy Loss: 0.0\n",
            "value lr: 1.0790509e-06 policy lr: 1.0790508e-07 noise: 1.843913637278885e-08\n",
            "Episode 180\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.7010669708251953, Mean Policy Loss: 0.0\n",
            "value lr: 1.0251508e-06 policy lr: 1.0251509e-07 noise: 1.6686005719483285e-08\n",
            "Episode 181\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2304069995880127, Mean Policy Loss: 0.0\n",
            "value lr: 9.739433e-07 policy lr: 9.739433e-08 noise: 1.509955679277401e-08\n",
            "Episode 182\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 2.968447685241699, Mean Policy Loss: 0.0\n",
            "value lr: 9.252936e-07 policy lr: 9.252936e-08 noise: 1.3663942058463315e-08\n",
            "Episode 183\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 4.0002923011779785, Mean Policy Loss: 0.0\n",
            "value lr: 8.79074e-07 policy lr: 8.79074e-08 noise: 1.2364820712247047e-08\n",
            "Episode 184\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.7120213508605957, Mean Policy Loss: 0.0\n",
            "value lr: 8.351631e-07 policy lr: 8.351631e-08 noise: 1.1189215424937786e-08\n",
            "Episode 185\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.301832437515259, Mean Policy Loss: 0.0\n",
            "value lr: 7.934457e-07 policy lr: 7.934457e-08 noise: 1.0125382707867273e-08\n",
            "Episode 186\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.4499435424804688, Mean Policy Loss: 0.0\n",
            "value lr: 7.5381206e-07 policy lr: 7.538121e-08 noise: 9.162695603507641e-09\n",
            "Episode 187\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2660269737243652, Mean Policy Loss: 0.0\n",
            "value lr: 7.161582e-07 policy lr: 7.1615816e-08 noise: 8.291537529471015e-09\n",
            "Episode 188\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.254457473754883, Mean Policy Loss: 0.0\n",
            "value lr: 6.803852e-07 policy lr: 6.803852e-08 noise: 7.503206215462185e-09\n",
            "Episode 189\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2280757427215576, Mean Policy Loss: 0.0\n",
            "value lr: 6.463991e-07 policy lr: 6.4639906e-08 noise: 6.789826773580547e-09\n",
            "Episode 190\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 2.9659597873687744, Mean Policy Loss: 0.0\n",
            "value lr: 6.141106e-07 policy lr: 6.1411065e-08 noise: 6.1442730336036e-09\n",
            "Episode 191\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.7113215923309326, Mean Policy Loss: 0.0\n",
            "value lr: 5.83435e-07 policy lr: 5.8343502e-08 noise: 5.560096357445094e-09\n",
            "Episode 192\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2340762615203857, Mean Policy Loss: 0.0\n",
            "value lr: 5.542917e-07 policy lr: 5.542917e-08 noise: 5.031461221693596e-09\n",
            "Episode 193\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 2.9657492637634277, Mean Policy Loss: 0.0\n",
            "value lr: 5.2660414e-07 policy lr: 5.266041e-08 noise: 4.553086924745167e-09\n",
            "Episode 194\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.3292975425720215, Mean Policy Loss: 0.0\n",
            "value lr: 5.0029956e-07 policy lr: 5.0029957e-08 noise: 4.120194836224428e-09\n",
            "Episode 195\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2699990272521973, Mean Policy Loss: 0.0\n",
            "value lr: 4.7530898e-07 policy lr: 4.7530897e-08 noise: 3.728460661752138e-09\n",
            "Episode 196\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.8450469970703125, Mean Policy Loss: 0.0\n",
            "value lr: 4.5156668e-07 policy lr: 4.5156668e-08 noise: 3.3739712462170587e-09\n",
            "Episode 197\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 4.206794261932373, Mean Policy Loss: 0.0\n",
            "value lr: 4.2901036e-07 policy lr: 4.2901036e-08 noise: 3.0531854840463525e-09\n",
            "Episode 198\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.8390610218048096, Mean Policy Loss: 0.0\n",
            "value lr: 4.0758076e-07 policy lr: 4.0758074e-08 noise: 2.7628989459952333e-09\n",
            "Episode 199\n",
            "Reward: -99.8999999999986,Average 100 episodes: -99.9, Mean Value Loss: 3.2329273223876953, Mean Policy Loss: 0.0\n",
            "value lr: 3.8722158e-07 policy lr: 3.8722156e-08 noise: 2.500211869101663e-09\n",
            "execution time (sec): 4200.509934425354\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for env_name, env_dict in envs.items():\n",
        "  print(env_dict)\n",
        "  # if env_name == 'MountainCarContinuous-v0':\n",
        "  #   max_episodes = 200\n",
        "  agent = ActorCriticAgent(state_size=max_states_count, action_size=max_actions_count, \n",
        "                           policy_learning_rate=env_dict[\"lr_policy\"], value_learning_rate=env_dict[\"lr_value\"],\n",
        "                           policy_hidden_layers=env_dict[\"layers_policy\"], value_hidden_layers=env_dict[\"layers_value\"],\n",
        "                           policy_decay_rate=env_dict[\"decay_rate_policy\"], value_decay_rate=env_dict[\"decay_rate_value\"],\n",
        "                           policy_decay_steps=env_dict[\"decay_steps_policy\"], value_decay_steps=env_dict[\"decay_steps_value\"])\n",
        "  padding = max_states_count - env_dict[\"actions_count\"] \n",
        "  scaler = None\n",
        "  if env_dict[\"scale_states\"]:\n",
        "    scaler = StateScaler(env, padding)\n",
        "  solved = agent.train(env=env_dict[\"env\"], env_actions_count=env_dict[\"actions_count\"], convergence_criterion=env_dict[\"convergence_criterion\"], max_episodes=max_episodes, max_steps=max_steps, discount_factor=discount, discretize=env_dict[\"discretize\"], render=True, scaler=scaler, scale=env_dict[\"scale_states\"])\n",
        "  if solved:\n",
        "    agent.save_models(env_name)\n",
        "  env_dict[\"agent\"] = agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIGRrXlLyvZA"
      },
      "source": [
        "# Section 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbpnCRQt0cQs"
      },
      "outputs": [],
      "source": [
        "def prepare_progressive(neural_model, source_models):\n",
        "  print(\"original model - weights:\", len(neural_model.model.weights))\n",
        "  print(\"original model - trainable_weights:\", len(neural_model.model.trainable_weights))\n",
        "  print(\"original model - non_trainable_weights:\", len(neural_model.model.non_trainable_weights))\n",
        "  neural_model.convert_to_progressive(source_models)\n",
        "  print(\"\\ntransfer model - weights:\", len(neural_model.model.weights))\n",
        "  print(\"transfer model - trainable_weights:\", len(neural_model.model.trainable_weights))\n",
        "  print(\"transfer model - non_trainable_weights:\", len(neural_model.model.non_trainable_weights))\n",
        "  return neural_model\n",
        "\n",
        "\n",
        "def get_source_networks(envs, source_tasks):\n",
        "  source_policies = []\n",
        "  source_values = []\n",
        "  for source_task in source_tasks:\n",
        "    source_task_dict = envs[source_task]\n",
        "    source_task_agent = source_task_dict[\"agent\"]\n",
        "    source_policies.append(agent.policy.model)\n",
        "    source_values.append(agent.value.model)\n",
        "\n",
        "  return source_policies, source_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYVUO3_ky0fk"
      },
      "source": [
        "##{Acrobot, mountainCar}->CartPole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEyPC0nDy33A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba698cd-a50c-4c41-9195-b7b49391e2ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_52\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_80 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_81 (Dense)            (None, 7)                 49        \n",
            "                                                                 \n",
            " dense_82 (Dense)            (None, 6)                 48        \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 3)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 202\n",
            "Trainable params: 202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_53\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_86 (Dense)            (None, 12)                84        \n",
            "                                                                 \n",
            " dense_87 (Dense)            (None, 1)                 13        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139\n",
            "Trainable params: 139\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "original model - weights: 10\n",
            "original model - trainable_weights: 10\n",
            "original model - non_trainable_weights: 0\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_54\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 42\n",
            "Trainable params: 0\n",
            "Non-trainable params: 42\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_56\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 7)                 49        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 91\n",
            "Trainable params: 0\n",
            "Non-trainable params: 91\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_58\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 7)                 49        \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 7)                 56        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 147\n",
            "Trainable params: 0\n",
            "Non-trainable params: 147\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_60\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 7)                 49        \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 7)                 56        \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 6)                 48        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 195\n",
            "Trainable params: 0\n",
            "Non-trainable params: 195\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_62\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 7)                 49        \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 7)                 56        \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 6)                 48        \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 3)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 216\n",
            "Trainable params: 0\n",
            "Non-trainable params: 216\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_64\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 42\n",
            "Trainable params: 0\n",
            "Non-trainable params: 42\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_66\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 7)                 49        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 91\n",
            "Trainable params: 0\n",
            "Non-trainable params: 91\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_68\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 7)                 49        \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 7)                 56        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 147\n",
            "Trainable params: 0\n",
            "Non-trainable params: 147\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_70\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 7)                 49        \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 7)                 56        \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 6)                 48        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 195\n",
            "Trainable params: 0\n",
            "Non-trainable params: 195\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_72\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 7)                 49        \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 7)                 56        \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 6)                 48        \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 3)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 216\n",
            "Trainable params: 0\n",
            "Non-trainable params: 216\n",
            "_________________________________________________________________\n",
            "level_layers [<KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'dense_88')>, <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'model_54')>, <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'model_64')>]\n",
            "level_layers [<KerasTensor: shape=(None, 7) dtype=float32 (created by layer 'dense_89')>, <KerasTensor: shape=(None, 7) dtype=float32 (created by layer 'model_56')>, <KerasTensor: shape=(None, 7) dtype=float32 (created by layer 'model_66')>]\n",
            "level_layers [<KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'dense_90')>, <KerasTensor: shape=(None, 7) dtype=float32 (created by layer 'model_58')>, <KerasTensor: shape=(None, 7) dtype=float32 (created by layer 'model_68')>]\n",
            "source_model_layers_otputs {0: <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'model_54')>, 1: <KerasTensor: shape=(None, 7) dtype=float32 (created by layer 'model_56')>, 2: <KerasTensor: shape=(None, 7) dtype=float32 (created by layer 'model_58')>, 3: <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'model_60')>, 4: <KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'model_62')>}\n",
            "len(source_model_layers_otputs) 5\n",
            "source_model_layers_otputs {0: <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'model_64')>, 1: <KerasTensor: shape=(None, 7) dtype=float32 (created by layer 'model_66')>, 2: <KerasTensor: shape=(None, 7) dtype=float32 (created by layer 'model_68')>, 3: <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'model_70')>, 4: <KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'model_72')>}\n",
            "len(source_model_layers_otputs) 5\n",
            "Model: \"model_74\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, 6)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_88 (Dense)               (None, 6)            42          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_54 (Functional)          (None, 6)            42          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_64 (Functional)          (None, 6)            42          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 18)           0           ['dense_88[0][0]',               \n",
            "                                                                  'model_54[0][0]',               \n",
            "                                                                  'model_64[0][0]']               \n",
            "                                                                                                  \n",
            " dense_89 (Dense)               (None, 7)            133         ['concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            " model_56 (Functional)          (None, 7)            91          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_66 (Functional)          (None, 7)            91          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 21)           0           ['dense_89[0][0]',               \n",
            "                                                                  'model_56[0][0]',               \n",
            "                                                                  'model_66[0][0]']               \n",
            "                                                                                                  \n",
            " dense_90 (Dense)               (None, 6)            132         ['concatenate_7[0][0]']          \n",
            "                                                                                                  \n",
            " model_58 (Functional)          (None, 7)            147         ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_68 (Functional)          (None, 7)            147         ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 20)           0           ['dense_90[0][0]',               \n",
            "                                                                  'model_58[0][0]',               \n",
            "                                                                  'model_68[0][0]']               \n",
            "                                                                                                  \n",
            " dense_91 (Dense)               (None, 6)            126         ['concatenate_8[0][0]']          \n",
            "                                                                                                  \n",
            " model_60 (Functional)          (None, 6)            195         ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_70 (Functional)          (None, 6)            195         ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 18)           0           ['dense_91[0][0]',               \n",
            "                                                                  'model_60[0][0]',               \n",
            "                                                                  'model_70[0][0]']               \n",
            "                                                                                                  \n",
            " dense_92 (Dense)               (None, 3)            57          ['concatenate_9[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 685\n",
            "Trainable params: 490\n",
            "Non-trainable params: 195\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "transfer model - weights: 18\n",
            "transfer model - trainable_weights: 10\n",
            "transfer model - non_trainable_weights: 8\n",
            "original model - weights: 6\n",
            "original model - trainable_weights: 6\n",
            "original model - non_trainable_weights: 0\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_75\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 42\n",
            "Trainable params: 0\n",
            "Non-trainable params: 42\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_77\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 12)                84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 126\n",
            "Trainable params: 0\n",
            "Non-trainable params: 126\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_79\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 12)                84        \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 1)                 13        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139\n",
            "Trainable params: 0\n",
            "Non-trainable params: 139\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_81\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 42\n",
            "Trainable params: 0\n",
            "Non-trainable params: 42\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_83\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 12)                84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 126\n",
            "Trainable params: 0\n",
            "Non-trainable params: 126\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "intermediate_layer_model.summary()\n",
            "Model: \"model_85\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 12)                84        \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 1)                 13        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139\n",
            "Trainable params: 0\n",
            "Non-trainable params: 139\n",
            "_________________________________________________________________\n",
            "level_layers [<KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'dense_93')>, <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'model_75')>, <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'model_81')>]\n",
            "source_model_layers_otputs {0: <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'model_75')>, 1: <KerasTensor: shape=(None, 12) dtype=float32 (created by layer 'model_77')>, 2: <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'model_79')>}\n",
            "len(source_model_layers_otputs) 3\n",
            "source_model_layers_otputs {0: <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'model_81')>, 1: <KerasTensor: shape=(None, 12) dtype=float32 (created by layer 'model_83')>, 2: <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'model_85')>}\n",
            "len(source_model_layers_otputs) 3\n",
            "Model: \"model_87\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, 6)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_93 (Dense)               (None, 6)            42          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_75 (Functional)          (None, 6)            42          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_81 (Functional)          (None, 6)            42          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 18)           0           ['dense_93[0][0]',               \n",
            "                                                                  'model_75[0][0]',               \n",
            "                                                                  'model_81[0][0]']               \n",
            "                                                                                                  \n",
            " dense_94 (Dense)               (None, 12)           228         ['concatenate_10[0][0]']         \n",
            "                                                                                                  \n",
            " model_77 (Functional)          (None, 12)           126         ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_83 (Functional)          (None, 12)           126         ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 36)           0           ['dense_94[0][0]',               \n",
            "                                                                  'model_77[0][0]',               \n",
            "                                                                  'model_83[0][0]']               \n",
            "                                                                                                  \n",
            " dense_95 (Dense)               (None, 1)            37          ['concatenate_11[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 433\n",
            "Trainable params: 307\n",
            "Non-trainable params: 126\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "transfer model - weights: 10\n",
            "transfer model - trainable_weights: 6\n",
            "transfer model - non_trainable_weights: 4\n"
          ]
        }
      ],
      "source": [
        "source_tasks = [\"Acrobot-v1\",\"MountainCarContinuous-v0\"]\n",
        "target_task = \"CartPole-v1\"\n",
        "\n",
        "#target agent init\n",
        "target_task_dict = envs[target_task]\n",
        "target_agent = ActorCriticAgent(state_size=max_states_count, action_size=max_actions_count, \n",
        "                           policy_learning_rate=target_task_dict[\"lr_policy\"], value_learning_rate=target_task_dict[\"lr_value\"],\n",
        "                           policy_hidden_layers=target_task_dict[\"layers_policy\"], value_hidden_layers=target_task_dict[\"layers_value\"],\n",
        "                           policy_decay_rate=target_task_dict[\"decay_rate_policy\"], value_decay_rate=target_task_dict[\"decay_rate_value\"],\n",
        "                           policy_decay_steps=target_task_dict[\"decay_steps_policy\"], value_decay_steps=target_task_dict[\"decay_steps_value\"])\n",
        "\n",
        "#source agents\n",
        "source_policies, source_values = get_source_networks(envs, source_tasks)\n",
        "\n",
        "#converting the target networks to include the sources' networks \n",
        "target_agent.policy = prepare_progressive(target_agent.policy, source_policies)\n",
        "target_agent.value = prepare_progressive(target_agent.value, source_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfWXF9TSYVhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d320fe-ccd2-455e-b0e3-199ebc87d52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_74\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, 6)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_88 (Dense)               (None, 6)            42          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_54 (Functional)          (None, 6)            42          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_64 (Functional)          (None, 6)            42          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 18)           0           ['dense_88[0][0]',               \n",
            "                                                                  'model_54[0][0]',               \n",
            "                                                                  'model_64[0][0]']               \n",
            "                                                                                                  \n",
            " dense_89 (Dense)               (None, 7)            133         ['concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            " model_56 (Functional)          (None, 7)            91          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_66 (Functional)          (None, 7)            91          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 21)           0           ['dense_89[0][0]',               \n",
            "                                                                  'model_56[0][0]',               \n",
            "                                                                  'model_66[0][0]']               \n",
            "                                                                                                  \n",
            " dense_90 (Dense)               (None, 6)            132         ['concatenate_7[0][0]']          \n",
            "                                                                                                  \n",
            " model_58 (Functional)          (None, 7)            147         ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_68 (Functional)          (None, 7)            147         ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 20)           0           ['dense_90[0][0]',               \n",
            "                                                                  'model_58[0][0]',               \n",
            "                                                                  'model_68[0][0]']               \n",
            "                                                                                                  \n",
            " dense_91 (Dense)               (None, 6)            126         ['concatenate_8[0][0]']          \n",
            "                                                                                                  \n",
            " model_60 (Functional)          (None, 6)            195         ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_70 (Functional)          (None, 6)            195         ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 18)           0           ['dense_91[0][0]',               \n",
            "                                                                  'model_60[0][0]',               \n",
            "                                                                  'model_70[0][0]']               \n",
            "                                                                                                  \n",
            " dense_92 (Dense)               (None, 3)            57          ['concatenate_9[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 685\n",
            "Trainable params: 490\n",
            "Non-trainable params: 195\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_87\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, 6)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_93 (Dense)               (None, 6)            42          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_75 (Functional)          (None, 6)            42          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_81 (Functional)          (None, 6)            42          ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 18)           0           ['dense_93[0][0]',               \n",
            "                                                                  'model_75[0][0]',               \n",
            "                                                                  'model_81[0][0]']               \n",
            "                                                                                                  \n",
            " dense_94 (Dense)               (None, 12)           228         ['concatenate_10[0][0]']         \n",
            "                                                                                                  \n",
            " model_77 (Functional)          (None, 12)           126         ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " model_83 (Functional)          (None, 12)           126         ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 36)           0           ['dense_94[0][0]',               \n",
            "                                                                  'model_77[0][0]',               \n",
            "                                                                  'model_83[0][0]']               \n",
            "                                                                                                  \n",
            " dense_95 (Dense)               (None, 1)            37          ['concatenate_11[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 433\n",
            "Trainable params: 307\n",
            "Non-trainable params: 126\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "target_agent.policy.model.summary()\n",
        "target_agent.value.model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGCeooN1IuP9"
      },
      "outputs": [],
      "source": [
        "# re-train \n",
        "target_agent.train(env=target_task_dict[\"env\"], env_actions_count=target_task_dict[\"actions_count\"], convergence_criterion=target_task_dict[\"convergence_criterion\"], max_episodes=max_episodes, max_steps=max_steps, discount_factor=discount, discretize=target_task_dict[\"discretize\"], render=True, scaler=None, scale=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7MkPvPZy4Zy"
      },
      "source": [
        "##{CartPole, Acrobat}->mountainCar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tRPbsSAyzuP"
      },
      "outputs": [],
      "source": [
        "source_tasks = [\"CartPole-v1\", \"Acrobot-v1\"]\n",
        "target_task = \"MountainCarContinuous-v0\"\n",
        "\n",
        "#target agent init\n",
        "target_task_dict = envs[target_task]\n",
        "target_agent = ActorCriticAgent(state_size=max_states_count, action_size=max_actions_count, \n",
        "                           policy_learning_rate=target_task_dict[\"lr_policy\"], value_learning_rate=target_task_dict[\"lr_value\"],\n",
        "                           policy_hidden_layers=target_task_dict[\"layers_policy\"], value_hidden_layers=target_task_dict[\"layers_value\"],\n",
        "                           policy_decay_rate=target_task_dict[\"decay_rate_policy\"], value_decay_rate=target_task_dict[\"decay_rate_value\"],\n",
        "                           policy_decay_steps=target_task_dict[\"decay_steps_policy\"], value_decay_steps=target_task_dict[\"decay_steps_value\"])\n",
        "\n",
        "#source agents\n",
        "source_policies, source_values = get_source_networks(envs, source_tasks)\n",
        "\n",
        "#converting the target networks to include the sources' networks \n",
        "target_agent.policy = prepare_progressive(target_agent.policy, source_policies)\n",
        "target_agent.value = prepare_progressive(target_agent.value, source_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F6tyOXCYekY"
      },
      "outputs": [],
      "source": [
        "target_agent.policy.model.summary()\n",
        "target_agent.value.model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQPMzaIzyx5W"
      },
      "outputs": [],
      "source": [
        "# re-train \n",
        "target_agent.train(env=target_task_dict[\"env\"], env_actions_count=target_task_dict[\"actions_count\"], convergence_criterion=target_task_dict[\"convergence_criterion\"], max_episodes=max_episodes, max_steps=max_steps, discount_factor=discount, discretize=target_task_dict[\"discretize\"], render=True, scaler=None, scale=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsvVAUThTCCZ"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard \n",
        "%tensorboard --logdir \".logs/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePLDEFVObgud"
      },
      "outputs": [],
      "source": [
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DRL_Ass3-keras_sec_3.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}