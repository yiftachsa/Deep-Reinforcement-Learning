{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOOLUVuEOnII",
        "outputId": "e36611ba-3f6f-4386-e628-f9d6e9006095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 32.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 92 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 122 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 11.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4.1\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "!pip install tensorboardX\n",
        "# !pip install tensorflow-gpu \n",
        "\n",
        "# !pip install tf_slim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZv3ylAvRc_t"
      },
      "outputs": [],
      "source": [
        "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "# !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# !pip install -U colabgymrender\n",
        "\n",
        "# from colabgymrender.recorder import Recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WArmaXplNQUH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "# !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# !apt-get update > /dev/null 2>&1\n",
        "# !apt-get install cmake > /dev/null 2>&1\n",
        "# !pip install --upgrade setuptools 2>&1\n",
        "# !pip install ez_setup > /dev/null 2>&1\n",
        "# !pip install gym[atari] > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN1kCRH_PBv-"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import datetime\n",
        "# import tf_slim as slim\n",
        "from tensorboardX import SummaryWriter\n",
        "import random\n",
        "import time\n",
        "from gym.wrappers import Monitor\n",
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBO3_k5hNjsL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import glob\n",
        "# import io\n",
        "# import base64\n",
        "# from IPython.display import HTML\n",
        "# from pyvirtualdisplay import Display\n",
        "# from IPython import display as ipythondisplay\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()\n",
        "\n",
        "# def show_video():\n",
        "#   mp4list = glob.glob('video/*.mp4')\n",
        "#   if len(mp4list) > 0:\n",
        "#     mp4 = mp4list[0]\n",
        "#     video = io.open(mp4, 'r+b').read()\n",
        "#     encoded = base64.b64encode(video)\n",
        "#     ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "#                 loop controls style=\"height: 400px;\">\n",
        "#                 <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "#              </video>'''.format(encoded.decode('ascii'))))\n",
        "#   else: \n",
        "#     print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "# def wrap_env(env):\n",
        "#   env = Monitor(env, './video', force=True)\n",
        "#   return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3rAMsYYqdp2"
      },
      "source": [
        "#Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh3vKQZGqlIM"
      },
      "source": [
        "##Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAIEjzgmYOKH"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "import gym\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Activation  # , Reshape\n",
        "from tensorflow.keras.models import Model, model_from_json\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow import GradientTape, math, one_hot, square, summary, config\n",
        "import keras.backend as K\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49JqRgsBbPvn"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghMjwStXX4nx"
      },
      "outputs": [],
      "source": [
        "# class LearningRateLoggingCallback(tf.keras.callbacks.Callback):\n",
        "#     def on_epoch_end(self, epoch):\n",
        "#         lr = self.model.optimizer.lr\n",
        "#         tf.summary.scalar('learning rate', data=lr, step=epoch)\n",
        "\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "    \"\"\"\n",
        "    Neural network class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_layers, init_learning_rate,\n",
        "                 decay_steps, decay_rate, loss=\"mse\",weights=None):\n",
        "        \"\"\"\n",
        "        Initialize the Feed Forward Neural Network and the optimizer\n",
        "\n",
        "        :param input_size: the length of the input\n",
        "        :param output_size: the length of the output\n",
        "        :param hidden_layers: list of the size of the hidden layers\n",
        "        :param init_learning_rate: the initial learning rate\n",
        "        :param decay_steps: Decay steps of the learning rate\n",
        "        :param decay_rate: Decay rate of the learning rate\n",
        "        :param weights: if given, weights of the model\n",
        "        :param loss: either \"mse\" or \"categorical_crossentropy\". detrmines the loss function and the output's layer activation (linear or softmax)\n",
        "        \"\"\"\n",
        "        self.state_size = input_size\n",
        "        self.input_size = input_size\n",
        "        self.action_size = output_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = init_learning_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.decay_rate = decay_rate\n",
        "        self.hidden_layers_sizes = hidden_layers\n",
        "        self.loss = loss\n",
        "        self.step = 0\n",
        "        # self.layers = []\n",
        "\n",
        "        # lr_schedule = ExponentialDecay(\n",
        "        #     initial_learning_rate=init_learning_rate,\n",
        "        #     decay_steps=decay_steps,\n",
        "        #     decay_rate=decay_rate,\n",
        "        #     staircase=True)\n",
        "        self.learning_rate_dynamic = self.learning_rate * self.decay_rate ** (self.step / self.decay_steps)\n",
        "\n",
        "        self.optimizer = Adam(learning_rate=self.get_lr)\n",
        "        lr_metric = self.get_lr_metric(self.optimizer)\n",
        "        self.model = self.create_model(input_size, output_size, layers=list(hidden_layers), lr_metric=lr_metric, loss=loss)\n",
        "        if weights is not None:\n",
        "            self.copy_weights(weights)\n",
        "\n",
        "    # source: https://stackoverflow.com/questions/47490834/how-can-i-print-the-learning-rate-at-each-epoch-with-adam-optimizer-in-keras\n",
        "    def get_lr_metric(self, optimizer):\n",
        "        def lr(y_true, y_pred):\n",
        "            return optimizer._decayed_lr(\"float32\")\n",
        "\n",
        "        return lr\n",
        "\n",
        "    def create_model(self, input_size, output_size, lr_metric, loss, layers=[]):\n",
        "        \"\"\"\n",
        "        Creates and returns Dense NN based on the given params\n",
        "\n",
        "        :param input_size: the length of the input\n",
        "        :param output_size: the length of the output\n",
        "        :param layers: list of the size of the hidden layers\n",
        "        :param lr_metric: Learning rate metric of the optimizer\n",
        "        :param loss: either \"mse\" or \"categorical_crossentropy\". detrmines the loss function and the output's layer activation (linear or softmax)\n",
        "        :return: dense NN\n",
        "        \"\"\"\n",
        "        state_input = Input(shape=(input_size,), dtype='float64', name=\"states input\")\n",
        "\n",
        "        x = Dense(input_size, activation='relu')(state_input)  # , kernel_initializer='RandomNormal'\n",
        "        for i in range(0, len(layers)):\n",
        "            x = Dense(layers[i], activation='relu')(x)\n",
        "            # x = BatchNormalization()(x)\n",
        "            # x = Dropout(0.2)(x)\n",
        "\n",
        "        \n",
        "        if loss == \"mse\":\n",
        "          x = Dense(output_size, activation='linear')(x)\n",
        "        elif loss == \"categorical_crossentropy\":\n",
        "          x = Dense(output_size, activation='softmax')(x)\n",
        "\n",
        "        model = Model(state_input, x)\n",
        "\n",
        "        model.compile(loss=loss, optimizer=self.optimizer, metrics=['acc', lr_metric])\n",
        "\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "    def predict(self, x, np_array=False):\n",
        "        \"\"\"\n",
        "         use self.model to predict on the given x\n",
        "\n",
        "        :param x: input for the model\n",
        "        :return: prediction of the model on x\n",
        "        \"\"\"\n",
        "        if np_array:\n",
        "          return self.model(np.atleast_2d(x.astype('float64')))\n",
        "        else:\n",
        "          return self.model(x)\n",
        "\n",
        "    def convert_to_transfer_model(self):\n",
        "      #trasfer weights\n",
        "      layer_input = self.model.input\n",
        "      layer_output = self.model.layers[-2].output\n",
        "      intermediate_layer_model = Model(inputs=layer_input, outputs=layer_output)\n",
        "\n",
        "      input = Input(shape=(self.input_size,), dtype='float64', name=\"states input\")\n",
        "      x = intermediate_layer_model(input, training=False)\n",
        "      \n",
        "      #add a new output layer\n",
        "      if self.loss == \"mse\":\n",
        "          x = Dense(self.output_size, activation='linear')(x)\n",
        "      elif self.loss == \"categorical_crossentropy\":\n",
        "          x = Dense(self.output_size, activation='softmax')(x)\n",
        "      \n",
        "      # lr_schedule = ExponentialDecay(\n",
        "      #       initial_learning_rate=self.learning_rate,\n",
        "      #       decay_steps=self.decay_steps,\n",
        "      #       decay_rate=self.decay_rate,\n",
        "      #       staircase=True)\n",
        "      self.learning_rate_dynamic = self.learning_rate * self.decay_rate ** (self.step / self.decay_steps)\n",
        "\n",
        "\n",
        "      self.model.optimizer = Adam(learning_rate=self.get_lr)\n",
        "      lr_metric = self.get_lr_metric(self.optimizer)\n",
        "\n",
        "      self.transfer_model = Model(input, x)\n",
        "      self.transfer_model.compile(loss=self.loss, optimizer=self.optimizer, metrics=['acc', lr_metric])\n",
        "      for layer in self.transfer_model.layers[:-1]:\n",
        "        layer.trainable = False\n",
        "      self.transfer_model.summary()\n",
        "      self.model = self.transfer_model\n",
        "    \n",
        "    def get_lr(self):\n",
        "      return self.learning_rate_dynamic\n",
        "\n",
        "    def update_lr(self):\n",
        "      self.step = self.step + 1\n",
        "      self.learning_rate_dynamic = self.learning_rate * self.decay_rate ** (self.step / self.decay_steps)\n",
        "      \n",
        "\n",
        "\n",
        "    def predict_logits(self, x, np_array=False):\n",
        "      layer_input = self.model.input\n",
        "      layer_output = self.model.layers[-2].output\n",
        "      intermediate_layer_model = Model(inputs=layer_input, outputs=layer_output)\n",
        "      if np_array:\n",
        "        return intermediate_layer_model(np.atleast_2d(x.astype('float64')))\n",
        "      else:\n",
        "        return intermediate_layer_model(x)\n",
        "\n",
        "    def copy_weights(self, other_weights):\n",
        "        \"\"\"\n",
        "        copies the given weights to self.model\n",
        "\n",
        "        :param other_weights: weights of a different model with the same structure\n",
        "        \"\"\"\n",
        "        # owm_weights = self.model.trainable_variables\n",
        "        # for v1, v2 in zip(owm_weights, other_weights):\n",
        "        #     v1.assign(v2.numpy())\n",
        "        self.model.set_weights(other_weights)\n",
        "\n",
        "    def save(self, time_stamp, env_name, prefix=\"policy\"):\n",
        "        base_path =f\"./trained_internal_models/{env_name}\"\n",
        "        if not os.path.isdir(base_path):\n",
        "            os.makedirs(base_path)\n",
        "\n",
        "        # serialize model to JSON\n",
        "        model_json = self.model.to_json()\n",
        "        with open(base_path + f\"/{prefix}_{time_stamp}.json\", \"w\") as json_file:\n",
        "            json_file.write(model_json)\n",
        "        # serialize weights to HDF5\n",
        "        self.model.save_weights(base_path + f\"/{prefix}_{time_stamp}_weights.h5\")\n",
        "        print(f\"Saved model to disk num: {prefix} {time_stamp}\")\n",
        "    \n",
        "    def load(self, time_stamp, env_name, prefix=\"policy\"):\n",
        "        base_path =f\"./trained_internal_models/{env_name}\"\n",
        "        if not os.path.isdir(base_path):\n",
        "            print(\"no model was saved yet\")\n",
        "        else:\n",
        "            # load json and create model\n",
        "            json_file = open(base_path + f\"/{prefix}_{time_stamp}.json\", 'r')\n",
        "            loaded_model_json = json_file.read()\n",
        "            json_file.close()\n",
        "            self.model = model_from_json(loaded_model_json)\n",
        "            # load weights into new model\n",
        "            self.model.load_weights(base_path + f\"/{prefix}_{time_stamp}_weights.h5\")\n",
        "            \n",
        "            self.learning_rate_dynamic = self.learning_rate * self.decay_rate ** (self.step / self.decay_steps)\n",
        "            self.model.optimizer = Adam(learning_rate=self.get_lr)\n",
        "            lr_metric = self.get_lr_metric(self.optimizer)\n",
        "\n",
        "            self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=['acc', lr_metric])\n",
        "            print(\"Loaded model from disk\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz4ct4WUqyOk"
      },
      "source": [
        "### Actor-Critic Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Lg88wW2i8r_"
      },
      "outputs": [],
      "source": [
        "class ActorCriticAgent(object):\n",
        "  def __init__(self, state_size, action_size, policy_learning_rate, value_learning_rate, \n",
        "               policy_hidden_layers, value_hidden_layers, policy_decay_rate, value_decay_rate,\n",
        "               policy_decay_steps, value_decay_steps):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    np.random.seed(2)\n",
        "    # tf.reset_default_graph()\n",
        "    # Initialize the policy network\n",
        "    self.policy = NeuralNetwork(input_size=state_size, output_size=action_size,\n",
        "                                hidden_layers=policy_hidden_layers, init_learning_rate=policy_learning_rate,\n",
        "                                decay_steps=policy_decay_steps, decay_rate=policy_decay_rate,\n",
        "                                loss=\"categorical_crossentropy\",weights=None)\n",
        "    # Initialize the value network\n",
        "    self.value = NeuralNetwork(input_size=state_size, output_size=1,\n",
        "                                hidden_layers=list(value_hidden_layers), init_learning_rate=value_learning_rate,\n",
        "                                decay_steps=value_decay_steps, decay_rate=value_decay_rate,\n",
        "                                loss=\"mse\",weights=None)\n",
        "\n",
        "  def save_models(self, env_name):\n",
        "    time_stamp = datetime.now()\n",
        "    self.policy.save(time_stamp, env_name, prefix=\"policy\")\n",
        "    self.value.save(time_stamp, env_name, prefix=\"value\")\n",
        "\n",
        "\n",
        "  def train(self, env, env_actions_count, convergence_criterion, max_episodes, max_steps, discount_factor, scaler, scale=False, discretize=[], render=False, plot=True, init_noise=1,noise_decay=0.9999):\n",
        "    np.random.seed(0)\n",
        "    writer = SummaryWriter()\n",
        "    \n",
        "    # Start training the agent with REINFORCE algorithm\n",
        "    start_time = time.time()\n",
        "\n",
        "    # with tf.Session() as sess:\n",
        "    #   sess.run(tf.global_variables_initializer())\n",
        "    solved = False\n",
        "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    episode_rewards = np.zeros(max_episodes)\n",
        "    average_rewards = 0.0\n",
        "    for episode in range(max_episodes):\n",
        "        state = env.reset()\n",
        "        state = np.pad(state, (0,self.state_size-len(state)))\n",
        "        state = state.reshape([1, self.state_size]) #NOTE\n",
        "        if scale:\n",
        "          state = scaler.state_scale(state)\n",
        "\n",
        "        I = 1\n",
        "        episode_transitions = []\n",
        "        episode_val_losses = []\n",
        "        episode_policy_losses = []\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            self.policy.update_lr()\n",
        "            self.value.update_lr()\n",
        "\n",
        "            actions_distribution = self.policy.predict(state)\n",
        "\n",
        "            if len(discretize) > 0: #Continues\n",
        "                actions_distribution = actions_distribution[0][:env_actions_count].numpy()\n",
        "                #add noise to action selection\n",
        "                init_noise=init_noise*noise_decay\n",
        "                noise = np.random.normal(0, 1, env_actions_count) * init_noise\n",
        "                actions_distribution = actions_distribution + noise\n",
        "                #select action \n",
        "                action = random.choices(np.arange(env_actions_count), weights=actions_distribution)[0]\n",
        "                action_one_hot = one_hot(action, self.action_size)\n",
        "                # # noisy action\n",
        "                action = [discretize[action]]\n",
        "            else:    \n",
        "                action = random.choices(np.arange(env_actions_count), weights=actions_distribution[0][:env_actions_count])[0]\n",
        "                action_one_hot = one_hot(action, self.action_size)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            next_state = np.pad(next_state, (0,self.state_size-len(next_state)))\n",
        "            next_state = next_state.reshape([1, self.state_size]) #NOTE\n",
        "            if scale:\n",
        "              next_state = scaler.state_scale(next_state)\n",
        "            # if render:\n",
        "            #     env.render()\n",
        "            \n",
        "            episode_transitions.append(Transition(state=state, action=action_one_hot, reward=reward, next_state=next_state, done=done))\n",
        "            \n",
        "            episode_rewards[episode] += reward\n",
        "\n",
        "            # Compute value estimations\n",
        "            estimated_return_state = self.value.predict(state)\n",
        "            if not done:\n",
        "              estimated_return_next_state = self.value.predict(next_state)\n",
        "              discounted_estimated_return_next_state = discount_factor * estimated_return_next_state\n",
        "            else:\n",
        "              discounted_estimated_return_next_state = 0\n",
        "            # Compute TD-error \n",
        "            delta = (reward + discounted_estimated_return_next_state) - estimated_return_state\n",
        "            delta_I = I * delta\n",
        "            \n",
        "            # Value function update\n",
        "            with GradientTape() as tape:\n",
        "              # val_loss = self.value.model.compiled_loss((reward + discounted_estimated_return_next_state), estimated_return_state)\n",
        "              val_loss = math.reduce_mean(square((reward + discounted_estimated_return_next_state) - self.value.predict(state)))\n",
        "            # print(\"val_loss\", val_loss)\n",
        "            # print(\"val_loss\", val_loss.numpy())\n",
        "            variables = self.value.model.trainable_weights\n",
        "            gradients = tape.gradient(val_loss, variables)\n",
        "            # print(\"gradients\", gradients)\n",
        "            self.value.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "            # value_feed_dict= {self.value.state: state, self.value.R_t: reward + discounted_estimated_return_next_state} \n",
        "            # _, val_loss = sess.run([self.value.optimizer, self.value.loss], value_feed_dict)\n",
        "            episode_val_losses.append(val_loss)\n",
        "\n",
        "            # Policy function update\n",
        "            with GradientTape() as tape:\n",
        "              neg_log_prob = self.policy.model.compiled_loss(y_true=tf.expand_dims(action_one_hot, axis=0), y_pred=self.policy.predict(state))\n",
        "              # loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "              # print(\"neg_log_prob\", neg_log_prob)\n",
        "              # neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, labels=action_one_hot)\n",
        "              policy_loss = tf.reduce_mean(neg_log_prob * delta_I)\n",
        "              # print(\"policy_loss\", policy_loss)\n",
        "\n",
        "            variables = self.policy.model.trainable_weights\n",
        "            gradients = tape.gradient(policy_loss, variables)\n",
        "            self.policy.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "            episode_policy_losses.append(policy_loss)\n",
        "            \n",
        "            if step+1 == max_steps:\n",
        "              done=True\n",
        "            if done:\n",
        "                if episode > 98:\n",
        "                    # Check if solved\n",
        "                    average_rewards = np.mean(episode_rewards[(episode - 99):episode+1])\n",
        "                    # print(\"episode_rewards\", episode_rewards[(episode - 99):episode+1])\n",
        "                print(\"Episode {}\".format(episode))\n",
        "                print(f\"Reward: {episode_rewards[episode]},Average 100 episodes: {round(average_rewards, 2)}, Mean Value Loss: {np.mean(episode_val_losses)}, Mean Policy Loss: {np.mean(episode_policy_losses)}\")\n",
        "                solved = convergence_criterion(average_rewards)\n",
        "                print(\"value lr:\", self.value.optimizer._decayed_lr(tf.float32).numpy(), \"policy lr:\", self.policy.optimizer._decayed_lr(tf.float32).numpy(), \"noise:\", init_noise)\n",
        "                if solved:\n",
        "                    print(' Solved at episode: ' + str(episode))\n",
        "                break\n",
        "            state = next_state\n",
        "            I = discount_factor * I\n",
        "        writer.add_scalar('Total Reward', episode_rewards[episode], episode)\n",
        "        writer.add_scalar('Steps', step, episode)\n",
        "        # if render:\n",
        "        #   env.play()\n",
        "\n",
        "        if solved:\n",
        "            break\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"execution time (sec): {end_time - start_time}\")\n",
        "    if plot:\n",
        "      plt.plot(range(episode),episode_rewards[:episode])\n",
        "      plt.xlabel('Eposode')\n",
        "      plt.ylabel('Rewards')\n",
        "      plt.title('The total rewared')\n",
        "      plt.show()\n",
        "    \n",
        "    return solved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9KtNoKIJgJF"
      },
      "source": [
        "### Actor-Critic Agent - Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iI--f6Ylfr2"
      },
      "outputs": [],
      "source": [
        "def sample_batch(deque_to_sample_from, n):\n",
        "  \"\"\"\n",
        "  Randomly (Uniformly) samples n instances from a given deque\n",
        "\n",
        "  :param deque_to_sample_from: a deque to sample from\n",
        "  :param n: the number of instances to sample\n",
        "  :return: the sampled instances\n",
        "  \"\"\"\n",
        "  m = min(n, len(deque_to_sample_from))\n",
        "  batch = random.sample(list(deque_to_sample_from), m)\n",
        "  return batch\n",
        "\n",
        "class ActorCriticAgent_2(object):\n",
        "  def __init__(self, state_size, action_size, policy_learning_rate, value_learning_rate, \n",
        "               policy_hidden_layers, value_hidden_layers, policy_decay_rate, value_decay_rate,\n",
        "               policy_decay_steps, value_decay_steps):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    np.random.seed(1)\n",
        "    # tf.reset_default_graph()\n",
        "    # Initialize the policy network\n",
        "    self.policy = NeuralNetwork(input_size=state_size, output_size=action_size,\n",
        "                                hidden_layers=policy_hidden_layers, init_learning_rate=policy_learning_rate,\n",
        "                                decay_steps=policy_decay_steps, decay_rate=policy_decay_rate,\n",
        "                                loss=\"categorical_crossentropy\",weights=None)\n",
        "    # Initialize the value network\n",
        "    self.value = NeuralNetwork(input_size=state_size, output_size=1,\n",
        "                                hidden_layers=list(value_hidden_layers), init_learning_rate=value_learning_rate,\n",
        "                                decay_steps=value_decay_steps, decay_rate=value_decay_rate,\n",
        "                                loss=\"mse\",weights=None)\n",
        "\n",
        "  def save_models(self, env_name):\n",
        "    time_stamp = datetime.now()\n",
        "    self.policy.save(time_stamp, env_name, prefix=\"policy\")\n",
        "    self.value.save(time_stamp, env_name, prefix=\"value\")\n",
        "  \n",
        "  def train(self, env, env_actions_count, convergence_criterion, max_episodes, max_steps, discount_factor, scaler, scale=False, discretize=[], render=False, plot=True, batch_size=16, experience_size=2000, init_noise=1,noise_decay=0.9999):\n",
        "    np.random.seed(1)\n",
        "    writer = SummaryWriter()\n",
        "    # Start training the agent with REINFORCE algorithm\n",
        "    start_time = time.time()\n",
        "\n",
        "    # with tf.Session() as sess:\n",
        "    #   sess.run(tf.global_variables_initializer())\n",
        "    solved = False\n",
        "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    episode_rewards = np.zeros(max_episodes)\n",
        "    average_rewards = 0.0\n",
        "    experience_replay = deque(maxlen=experience_size)\n",
        "\n",
        "    for episode in range(max_episodes):\n",
        "        state = env.reset()\n",
        "        state = np.pad(state, (0,self.state_size-len(state)))\n",
        "        state = state.reshape([1, self.state_size]) #NOTE\n",
        "        if scale:\n",
        "          state = scaler.state_scale(state)\n",
        "\n",
        "        I = 1\n",
        "        episode_val_losses = []\n",
        "        episode_policy_losses = []\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            self.policy.update_lr()\n",
        "            self.value.update_lr()\n",
        "\n",
        "            actions_distribution = self.policy.predict(state)\n",
        "            if len(discretize) > 0: #Continues\n",
        "                actions_distribution = actions_distribution[0][:env_actions_count].numpy()\n",
        "                #add noise to action selection\n",
        "                init_noise=init_noise*noise_decay\n",
        "                noise = np.random.normal(0, 0.2, env_actions_count) * init_noise\n",
        "                actions_distribution = actions_distribution + noise\n",
        "                #select action \n",
        "                action = random.choices(np.arange(env_actions_count), weights=actions_distribution)[0]\n",
        "                action_one_hot = one_hot(action, self.action_size)\n",
        "                action = [discretize[action]]\n",
        "            else:    \n",
        "                action = random.choices(np.arange(env_actions_count), weights=actions_distribution[0][:env_actions_count])[0]\n",
        "                action_one_hot = one_hot(action, self.action_size)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            next_state = np.pad(next_state, (0,self.state_size-len(next_state)))\n",
        "            next_state = next_state.reshape([1, self.state_size]) #NOTE\n",
        "            if scale:\n",
        "              next_state = scaler.state_scale(next_state)\n",
        "            # if render:\n",
        "            #     env.render()\n",
        "            \n",
        "            experience_replay.append(Transition(state=state, action=action_one_hot, reward=reward, next_state=next_state, done=done))\n",
        "            \n",
        "            episode_rewards[episode] += reward\n",
        "            \n",
        "            # mini batch\n",
        "            transitions_batch = sample_batch(experience_replay, batch_size)\n",
        "\n",
        "            states_batch = []\n",
        "            actions_one_hot_batch = []\n",
        "            discounted_estimated_return_next_state_batch = []\n",
        "            delta_I_batch = []\n",
        "            for transition in transitions_batch:\n",
        "              states_batch.append(transition.state[0])\n",
        "              actions_one_hot_batch.append(transition.action)\n",
        "              if not transition.done:\n",
        "                estimated_return_next_state = self.value.predict(transition.next_state)\n",
        "                discounted_estimated_return_next_state =  transition.reward + discount_factor * estimated_return_next_state\n",
        "              else:\n",
        "                discounted_estimated_return_next_state = transition.reward + 0\n",
        "              discounted_estimated_return_next_state_batch.append(discounted_estimated_return_next_state)\n",
        "              \n",
        "              estimated_return_state = self.value.predict(transition.state)\n",
        "              delta = discounted_estimated_return_next_state - estimated_return_state\n",
        "              delta_I_batch.append(I * delta)\n",
        "            \n",
        "            states_batch = np.asarray(states_batch)\n",
        "            actions_one_hot_batch = tf.convert_to_tensor(actions_one_hot_batch)\n",
        "            discounted_estimated_return_next_state_batch = np.asarray(discounted_estimated_return_next_state_batch)\n",
        "            delta_I_batch = np.asarray(delta_I_batch)\n",
        "            # Value function update\n",
        "            with GradientTape() as tape:\n",
        "              # val_loss = self.value.model.compiled_loss((reward + discounted_estimated_return_next_state), estimated_return_state)\n",
        "              val_loss = math.reduce_mean(square(discounted_estimated_return_next_state_batch - self.value.predict(states_batch)))\n",
        "            # print(\"val_loss\", val_loss)\n",
        "            # print(\"val_loss\", val_loss.numpy())\n",
        "            variables = self.value.model.trainable_weights\n",
        "            gradients = tape.gradient(val_loss, variables)\n",
        "            # print(\"gradients\", gradients)\n",
        "            self.value.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "            # value_feed_dict= {self.value.state: state, self.value.R_t: reward + discounted_estimated_return_next_state} \n",
        "            # _, val_loss = sess.run([self.value.optimizer, self.value.loss], value_feed_dict)\n",
        "            episode_val_losses.append(val_loss)\n",
        "\n",
        "            # Policy function update\n",
        "            with GradientTape() as tape:\n",
        "              neg_log_prob = self.policy.model.compiled_loss(y_true=actions_one_hot_batch, y_pred=self.policy.predict(states_batch))\n",
        "              # loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "              # print(\"neg_log_prob\", neg_log_prob)\n",
        "              # neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, labels=action_one_hot)\n",
        "              policy_loss = tf.reduce_mean(neg_log_prob * delta_I_batch)\n",
        "              # print(\"policy_loss\", policy_loss)\n",
        "\n",
        "            variables = self.policy.model.trainable_weights\n",
        "            gradients = tape.gradient(policy_loss, variables)\n",
        "            self.policy.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "            episode_policy_losses.append(policy_loss)\n",
        "            \n",
        "            if step+1 == max_steps:\n",
        "              done=True\n",
        "            if done:\n",
        "                if episode > 98:\n",
        "                    # Check if solved\n",
        "                    average_rewards = np.mean(episode_rewards[(episode - 99):episode+1])\n",
        "                    # print(\"episode_rewards\", episode_rewards[(episode - 99):episode+1])\n",
        "                print(\"Episode {}\".format(episode))\n",
        "                print(f\"Reward: {episode_rewards[episode]},Average 100 episodes: {round(average_rewards, 2)}, Mean Value Loss: {np.mean(episode_val_losses)}, Mean Policy Loss: {np.mean(episode_policy_losses)}\")\n",
        "                solved = convergence_criterion(average_rewards)\n",
        "                print(\"value lr:\", self.value.optimizer._decayed_lr(tf.float32).numpy(), \"policy lr:\", self.policy.optimizer._decayed_lr(tf.float32).numpy())\n",
        "                if solved:\n",
        "                    print(' Solved at episode: ' + str(episode))\n",
        "                break\n",
        "            state = next_state\n",
        "            I = discount_factor * I\n",
        "        writer.add_scalar('Total Reward', episode_rewards[episode], episode)\n",
        "        writer.add_scalar('Steps', step, episode)\n",
        "        # if render:\n",
        "        #   env.play()\n",
        "\n",
        "        if solved:\n",
        "            break\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"execution time (sec): {end_time - start_time}\")\n",
        "    if plot:\n",
        "      plt.plot(range(episode),episode_rewards[:episode])\n",
        "      plt.xlabel('Eposode')\n",
        "      plt.ylabel('Rewards')\n",
        "      plt.title('The total rewared')\n",
        "      plt.show()\n",
        "    \n",
        "    return solved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1poCgRel5O3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "class StateScaler:\n",
        "    def __init__(self, env, padding):\n",
        "        state_space_samples = np.array(\n",
        "            [np.pad(env.observation_space.sample(), (0, padding), 'constant') for x in range(10000)])\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.scaler.fit(state_space_samples)\n",
        "    def state_scale(self, state):\n",
        "        return self.scaler.transform(state)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J74Y10wirIuA"
      },
      "source": [
        "## Environments Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbRqnRN0l3Gb"
      },
      "outputs": [],
      "source": [
        "envs_names = ['CartPole-v1',\n",
        "              'Acrobot-v1',\n",
        "              'MountainCarContinuous-v0'] \n",
        "mountain_car_discrit_actions = [-1, 1]\n",
        "\n",
        "def get_env_actions_count(env):\n",
        "  if len(env.action_space.shape) > 0:\n",
        "    actions_count = env.action_space.shape[0]\n",
        "  else:\n",
        "    actions_count = env.action_space.n\n",
        "  return actions_count\n",
        "\n",
        "max_actions_count = len(mountain_car_discrit_actions)\n",
        "max_states_count = 0\n",
        "envs = {}\n",
        "for env_name in envs_names:\n",
        "  env = Monitor(gym.make(env_name), './video', force=True)\n",
        "\n",
        "  actions_count = get_env_actions_count(env)\n",
        "\n",
        "  states_count = env.observation_space.shape[0]\n",
        "  \n",
        "  envs[env_name] = {\"env\": gym.make(env_name), \n",
        "                    \"actions_count\": actions_count, \n",
        "                    \"states_count\": states_count}\n",
        "  \n",
        "  if actions_count > max_actions_count:\n",
        "    max_actions_count = actions_count\n",
        "  if states_count > max_states_count:\n",
        "    max_states_count = states_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGNtovEYjrMa"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "max_episodes = 500\n",
        "max_steps = 1000\n",
        "discount = 0.999\n",
        "\n",
        "envs[\"CartPole-v1\"].update({\n",
        "    \"layers_policy\": [7, 6, 6], \n",
        "    \"layers_value\": [12], \n",
        "    \"lr_policy\": 0.003, \n",
        "    \"lr_value\":0.01, \n",
        "    \"decay_steps_policy\": 600, \n",
        "    \"decay_steps_value\": 850, \n",
        "    \"decay_rate_policy\": 0.88, \n",
        "    \"decay_rate_value\": 0.92,\n",
        "    \"discretize\":[],\n",
        "    \"scale_states\": False})\n",
        "\n",
        "envs[\"Acrobot-v1\"].update({\n",
        "    \"layers_policy\": [7,7,6], \n",
        "    \"layers_value\": [12], \n",
        "    \"lr_policy\": 0.001, \n",
        "    \"lr_value\":0.01, \n",
        "    \"decay_steps_policy\": 1000, \n",
        "    \"decay_steps_value\": 1000, \n",
        "    \"decay_rate_policy\": 0.95, \n",
        "    \"decay_rate_value\": 0.98,\n",
        "    \"discretize\":[],\n",
        "    \"scale_states\": False})\n",
        "\n",
        "envs['MountainCarContinuous-v0'].update({\n",
        "    \"layers_policy\": [7,7,6], \n",
        "    \"layers_value\": [12], \n",
        "    \"lr_policy\": 0.001, \n",
        "    \"lr_value\":0.01, \n",
        "    \"decay_steps_policy\": 1000, \n",
        "    \"decay_steps_value\": 1000, \n",
        "    \"decay_rate_policy\": 0.95, \n",
        "    \"decay_rate_value\": 0.95,\n",
        "    \"discretize\":[],\n",
        "    \"actions_count\": len(mountain_car_discrit_actions), \n",
        "    \"discretize\": mountain_car_discrit_actions,\n",
        "    \"scale_states\": True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qqXo60ZrPQs"
      },
      "source": [
        "## Convergence Criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcJTbEhaI7oH"
      },
      "outputs": [],
      "source": [
        "def cart_pole_convergence_criterion(avg_rewards):\n",
        "  if avg_rewards > 475:\n",
        "    solved = True\n",
        "  else:\n",
        "    solved = False\n",
        "  return solved\n",
        "\n",
        "def acrobat_convergence_criterion(avg_rewards):\n",
        "  if avg_rewards > -90 and avg_rewards !=0.0:\n",
        "    solved = True\n",
        "  else:\n",
        "    solved = False\n",
        "  return solved\n",
        "\n",
        "def mountain_car_convergence_criterion(avg_rewards):\n",
        "  if avg_rewards > 90:\n",
        "    solved = True\n",
        "  else:\n",
        "    solved = False\n",
        "  return solved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C1dCQYq9x_Q"
      },
      "outputs": [],
      "source": [
        "envs['CartPole-v1']['convergence_criterion'] = cart_pole_convergence_criterion\n",
        "envs['Acrobot-v1']['convergence_criterion'] = acrobat_convergence_criterion\n",
        "envs['MountainCarContinuous-v0']['convergence_criterion'] = mountain_car_convergence_criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6htZmU0rd2M"
      },
      "source": [
        "## Training Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHhlyqW4z6Pt"
      },
      "outputs": [],
      "source": [
        "for env_name, env_dict in envs.items():\n",
        "  print(env_dict)\n",
        "  agent = ActorCriticAgent(state_size=max_states_count, action_size=max_actions_count, \n",
        "                           policy_learning_rate=env_dict[\"lr_policy\"], value_learning_rate=env_dict[\"lr_value\"],\n",
        "                           policy_hidden_layers=env_dict[\"layers_policy\"], value_hidden_layers=env_dict[\"layers_value\"],\n",
        "                           policy_decay_rate=env_dict[\"decay_rate_policy\"], value_decay_rate=env_dict[\"decay_rate_value\"],\n",
        "                           policy_decay_steps=env_dict[\"decay_steps_policy\"], value_decay_steps=env_dict[\"decay_steps_value\"])\n",
        "  padding = max_states_count - env_dict[\"actions_count\"] \n",
        "  scaler = None\n",
        "  if env_dict[\"scale_states\"]:\n",
        "    scaler = StateScaler(env, padding)\n",
        "  solved = agent.train(env=env_dict[\"env\"], env_actions_count=env_dict[\"actions_count\"], convergence_criterion=env_dict[\"convergence_criterion\"], max_episodes=max_episodes, max_steps=max_steps, discount_factor=discount, discretize=env_dict[\"discretize\"], render=True, scaler=scaler, scale=env_dict[\"scale_states\"])\n",
        "  if solved:\n",
        "    agent.save_models(env_name)\n",
        "  env_dict[\"agent\"] = agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIGRrXlLyvZA"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbpnCRQt0cQs"
      },
      "outputs": [],
      "source": [
        "def transfer(neural_model):\n",
        "  print(\"original model - weights:\", len(neural_model.model.weights))\n",
        "  print(\"original model - trainable_weights:\", len(neural_model.model.trainable_weights))\n",
        "  print(\"original model - non_trainable_weights:\", len(neural_model.model.non_trainable_weights))\n",
        "  neural_model.convert_to_transfer_model()\n",
        "  print(\"\\ntransfer model - weights:\", len(neural_model.model.weights))\n",
        "  print(\"transfer model - trainable_weights:\", len(neural_model.model.trainable_weights))\n",
        "  print(\"transfer model - non_trainable_weights:\", len(neural_model.model.non_trainable_weights))\n",
        "  return neural_model\n",
        "\n",
        "\n",
        "def prepare_for_transfer(agent):\n",
        "  agent.policy = transfer(agent.policy)\n",
        "  agent.value = transfer(agent.value)\n",
        "  # Freeze all layers except the output layer\n",
        "  # for layer in model.layers[:-2]:\n",
        "  #   layer.trainable = False\n",
        "  # Reinitialize output layer weights\n",
        "\n",
        "  return agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYVUO3_ky0fk"
      },
      "source": [
        "##Acrobat->CartPole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEyPC0nDy33A"
      },
      "outputs": [],
      "source": [
        "source_task = \"Acrobot-v1\"\n",
        "target_task = \"CartPole-v1\"\n",
        "\n",
        "source_task_dict = envs[source_task]\n",
        "target_task_dict = envs[target_task]\n",
        "\n",
        "source_agent = source_task_dict[\"agent\"]\n",
        "\n",
        "target_agent = prepare_for_transfer(source_agent)\n",
        "# re-train \n",
        "\n",
        "target_agent.train(env=target_task_dict[\"env\"], env_actions_count=target_task_dict[\"actions_count\"], convergence_criterion=target_task_dict[\"convergence_criterion\"], max_episodes=max_episodes, max_steps=max_steps, discount_factor=discount, discretize=target_task_dict[\"discretize\"], render=True, scaler=None, scale=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7MkPvPZy4Zy"
      },
      "source": [
        "##CartPole->mountainCar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tRPbsSAyzuP"
      },
      "outputs": [],
      "source": [
        "source_task = \"CartPole-v1\"\n",
        "target_task = \"MountainCarContinuous-v0\"\n",
        "\n",
        "source_task_dict = envs[source_task]\n",
        "target_task_dict = envs[target_task]\n",
        "\n",
        "source_agent = source_task_dict[\"agent\"]\n",
        "\n",
        "target_agent = prepare_for_transfer(source_agent)\n",
        "# re-train \n",
        "\n",
        "target_agent.train(env=target_task_dict[\"env\"], env_actions_count=target_task_dict[\"actions_count\"], convergence_criterion=target_task_dict[\"convergence_criterion\"], max_episodes=max_episodes, max_steps=max_steps, discount_factor=discount, discretize=target_task_dict[\"discretize\"], render=True, scaler=None, scale=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQPMzaIzyx5W"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsvVAUThTCCZ"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard \n",
        "%tensorboard --logdir \".logs/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePLDEFVObgud"
      },
      "outputs": [],
      "source": [
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w9KtNoKIJgJF"
      ],
      "name": "DRL_Ass3-keras_sec_1_sec_2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}