{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te8X8J1syM_5"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qk0yNUdyTbk",
        "outputId": "151db029-bb8e-4edd-e094-f976fa6912f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 4)]               0         \n",
            "                                                                 \n",
            " dense_80 (Dense)            (None, 7)                 35        \n",
            "                                                                 \n",
            " dense_81 (Dense)            (None, 7)                 56        \n",
            "                                                                 \n",
            " dense_82 (Dense)            (None, 4)                 32        \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 133\n",
            "Trainable params: 133\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " states input (InputLayer)   [(None, 4)]               0         \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 7)                 35        \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 7)                 56        \n",
            "                                                                 \n",
            " dense_86 (Dense)            (None, 4)                 32        \n",
            "                                                                 \n",
            " dense_87 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 133\n",
            "Trainable params: 133\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Step 100, Episode 7 Training loss (for one batch): -1.0000, Episode total rewards 6.00, Episode epsilon 0.54288, Learning Rate 0.01000\n",
            "Step 200, Episode 14 Training loss (for one batch): -1.0000, Episode total rewards 11.00, Episode epsilon 0.49119, Learning Rate 0.01000\n",
            "Step 300, Episode 21 Training loss (for one batch): -1.0000, Episode total rewards 3.00, Episode epsilon 0.44442, Learning Rate 0.01000\n",
            "Episode 23, episode reward: 10.0,running avg reward (100 episodes): 14.347826086956522\n",
            "Episode 24, episode reward: 11.0,running avg reward (100 episodes): 14.208333333333334\n",
            "Episode 25, episode reward: 15.0,running avg reward (100 episodes): 14.24\n",
            "Episode 26, episode reward: 12.0,running avg reward (100 episodes): 14.153846153846153\n",
            "Episode 27, episode reward: 11.0,running avg reward (100 episodes): 14.037037037037036\n",
            "Episode 28, episode reward: 11.0,running avg reward (100 episodes): 13.928571428571429\n",
            "Episode 29, episode reward: 9.0,running avg reward (100 episodes): 13.758620689655173\n",
            "Step 400, Episode 30 Training loss (for one batch): 0.3114, Episode total rewards 1.00, Episode epsilon 0.40211, Learning Rate 0.01000\n",
            "Episode 30, episode reward: 37.0,running avg reward (100 episodes): 14.533333333333333\n",
            "Episode 31, episode reward: 14.0,running avg reward (100 episodes): 14.516129032258064\n",
            "Episode 32, episode reward: 11.0,running avg reward (100 episodes): 14.40625\n",
            "Episode 33, episode reward: 16.0,running avg reward (100 episodes): 14.454545454545455\n",
            "Step 500, Episode 34 Training loss (for one batch): 0.7373, Episode total rewards 23.00, Episode epsilon 0.36383, Learning Rate 0.00950\n",
            "Episode 34, episode reward: 26.0,running avg reward (100 episodes): 14.794117647058824\n",
            "Episode 35, episode reward: 12.0,running avg reward (100 episodes): 14.714285714285714\n",
            "Episode 36, episode reward: 13.0,running avg reward (100 episodes): 14.666666666666666\n",
            "Episode 37, episode reward: 11.0,running avg reward (100 episodes): 14.567567567567568\n",
            "Episode 38, episode reward: 17.0,running avg reward (100 episodes): 14.631578947368421\n",
            "Episode 39, episode reward: 20.0,running avg reward (100 episodes): 14.76923076923077\n",
            "Episode 40, episode reward: 17.0,running avg reward (100 episodes): 14.825\n",
            "Step 600, Episode 41 Training loss (for one batch): 1.3365, Episode total rewards 7.00, Episode epsilon 0.32919, Learning Rate 0.00950\n",
            "Episode 41, episode reward: 10.0,running avg reward (100 episodes): 14.707317073170731\n",
            "Episode 42, episode reward: 11.0,running avg reward (100 episodes): 14.619047619047619\n",
            "Episode 43, episode reward: 47.0,running avg reward (100 episodes): 15.372093023255815\n",
            "Episode 44, episode reward: 14.0,running avg reward (100 episodes): 15.340909090909092\n",
            "Episode 45, episode reward: 10.0,running avg reward (100 episodes): 15.222222222222221\n",
            "Episode 46, episode reward: 9.0,running avg reward (100 episodes): 15.08695652173913\n",
            "Step 700, Episode 47 Training loss (for one batch): 1.4383, Episode total rewards 6.00, Episode epsilon 0.29785, Learning Rate 0.00902\n",
            "Episode 47, episode reward: 12.0,running avg reward (100 episodes): 15.02127659574468\n",
            "Episode 48, episode reward: 77.0,running avg reward (100 episodes): 16.3125\n",
            "Step 800, Episode 49 Training loss (for one batch): 1.0256, Episode total rewards 17.00, Episode epsilon 0.26949, Learning Rate 0.00857\n",
            "Episode 49, episode reward: 29.0,running avg reward (100 episodes): 16.571428571428573\n",
            "Episode 50, episode reward: 42.0,running avg reward (100 episodes): 17.08\n",
            "Step 900, Episode 51 Training loss (for one batch): 1.9714, Episode total rewards 46.00, Episode epsilon 0.24383, Learning Rate 0.00857\n",
            "Episode 51, episode reward: 65.0,running avg reward (100 episodes): 18.019607843137255\n",
            "Episode 52, episode reward: 35.0,running avg reward (100 episodes): 18.346153846153847\n",
            "Episode 53, episode reward: 27.0,running avg reward (100 episodes): 18.50943396226415\n",
            "Step 1000, Episode 54 Training loss (for one batch): 1.0296, Episode total rewards 19.00, Episode epsilon 0.22062, Learning Rate 0.00815\n",
            "Episode 54, episode reward: 61.0,running avg reward (100 episodes): 19.296296296296298\n",
            "Episode 55, episode reward: 19.0,running avg reward (100 episodes): 19.29090909090909\n",
            "Step 1100, Episode 56 Training loss (for one batch): 15.4201, Episode total rewards 39.00, Episode epsilon 0.19961, Learning Rate 0.00774\n",
            "Episode 56, episode reward: 69.0,running avg reward (100 episodes): 20.178571428571427\n",
            "Step 1200, Episode 57 Training loss (for one batch): 1.7466, Episode total rewards 70.00, Episode epsilon 0.18061, Learning Rate 0.00774\n",
            "Step 1300, Episode 57 Training loss (for one batch): 1.3879, Episode total rewards 170.00, Episode epsilon 0.16341, Learning Rate 0.00735\n",
            "Episode 57, episode reward: 227.0,running avg reward (100 episodes): 23.80701754385965\n",
            "Step 1400, Episode 58 Training loss (for one batch): 32.1004, Episode total rewards 43.00, Episode epsilon 0.14785, Learning Rate 0.00698\n",
            "Episode 58, episode reward: 109.0,running avg reward (100 episodes): 25.275862068965516\n",
            "Step 1500, Episode 59 Training loss (for one batch): 8.3296, Episode total rewards 34.00, Episode epsilon 0.13378, Learning Rate 0.00698\n",
            "Episode 59, episode reward: 129.0,running avg reward (100 episodes): 27.033898305084747\n",
            "Step 1600, Episode 60 Training loss (for one batch): 2.5834, Episode total rewards 5.00, Episode epsilon 0.12104, Learning Rate 0.00663\n",
            "Step 1700, Episode 60 Training loss (for one batch): 2.6447, Episode total rewards 105.00, Episode epsilon 0.10952, Learning Rate 0.00630\n",
            "Episode 60, episode reward: 163.0,running avg reward (100 episodes): 29.3\n",
            "Step 1800, Episode 61 Training loss (for one batch): 32.3939, Episode total rewards 42.00, Episode epsilon 0.09909, Learning Rate 0.00630\n",
            "Step 1900, Episode 61 Training loss (for one batch): 1.4477, Episode total rewards 142.00, Episode epsilon 0.08966, Learning Rate 0.00599\n",
            "Episode 61, episode reward: 237.0,running avg reward (100 episodes): 32.704918032786885\n",
            "Step 2000, Episode 62 Training loss (for one batch): 0.4815, Episode total rewards 5.00, Episode epsilon 0.08112, Learning Rate 0.00569\n",
            "Step 2100, Episode 62 Training loss (for one batch): 0.9454, Episode total rewards 105.00, Episode epsilon 0.07340, Learning Rate 0.00569\n",
            "Step 2200, Episode 62 Training loss (for one batch): 0.6253, Episode total rewards 205.00, Episode epsilon 0.06641, Learning Rate 0.00540\n",
            "Step 2300, Episode 62 Training loss (for one batch): 0.1446, Episode total rewards 305.00, Episode epsilon 0.06009, Learning Rate 0.00513\n",
            "Episode 62, episode reward: 335.0,running avg reward (100 episodes): 37.58064516129032\n",
            "Step 2400, Episode 63 Training loss (for one batch): 0.4187, Episode total rewards 70.00, Episode epsilon 0.05437, Learning Rate 0.00513\n",
            "Step 2500, Episode 63 Training loss (for one batch): 0.4020, Episode total rewards 170.00, Episode epsilon 0.04919, Learning Rate 0.00488\n",
            "Episode 63, episode reward: 246.0,running avg reward (100 episodes): 40.888888888888886\n",
            "Step 2600, Episode 64 Training loss (for one batch): 15.8022, Episode total rewards 24.00, Episode epsilon 0.04451, Learning Rate 0.00463\n",
            "Step 2700, Episode 64 Training loss (for one batch): 0.1486, Episode total rewards 124.00, Episode epsilon 0.04027, Learning Rate 0.00463\n",
            "Step 2800, Episode 64 Training loss (for one batch): 0.1864, Episode total rewards 224.00, Episode epsilon 0.03643, Learning Rate 0.00440\n",
            "Episode 64, episode reward: 237.0,running avg reward (100 episodes): 43.953125\n",
            "Step 2900, Episode 65 Training loss (for one batch): 0.1890, Episode total rewards 87.00, Episode epsilon 0.03297, Learning Rate 0.00418\n",
            "Step 3000, Episode 65 Training loss (for one batch): 0.0885, Episode total rewards 187.00, Episode epsilon 0.02983, Learning Rate 0.00418\n",
            "Step 3100, Episode 65 Training loss (for one batch): 0.1237, Episode total rewards 287.00, Episode epsilon 0.02699, Learning Rate 0.00397\n",
            "Episode 65, episode reward: 326.0,running avg reward (100 episodes): 48.292307692307695\n",
            "Step 3200, Episode 66 Training loss (for one batch): 7.7790, Episode total rewards 61.00, Episode epsilon 0.02442, Learning Rate 0.00377\n",
            "Step 3300, Episode 66 Training loss (for one batch): 0.2823, Episode total rewards 161.00, Episode epsilon 0.02209, Learning Rate 0.00377\n",
            "Episode 66, episode reward: 199.0,running avg reward (100 episodes): 50.57575757575758\n",
            "Step 3400, Episode 67 Training loss (for one batch): 0.0471, Episode total rewards 62.00, Episode epsilon 0.01999, Learning Rate 0.00358\n",
            "Step 3500, Episode 67 Training loss (for one batch): 0.2209, Episode total rewards 162.00, Episode epsilon 0.01809, Learning Rate 0.00341\n",
            "Step 3600, Episode 67 Training loss (for one batch): 0.0851, Episode total rewards 262.00, Episode epsilon 0.01636, Learning Rate 0.00341\n",
            "Episode 67, episode reward: 332.0,running avg reward (100 episodes): 54.776119402985074\n",
            "Step 3700, Episode 68 Training loss (for one batch): 0.2641, Episode total rewards 30.00, Episode epsilon 0.01481, Learning Rate 0.00324\n",
            "Step 3800, Episode 68 Training loss (for one batch): 0.0692, Episode total rewards 130.00, Episode epsilon 0.01340, Learning Rate 0.00307\n",
            "Step 3900, Episode 68 Training loss (for one batch): 7.7015, Episode total rewards 230.00, Episode epsilon 0.01212, Learning Rate 0.00307\n",
            "Step 4000, Episode 68 Training loss (for one batch): 0.4028, Episode total rewards 330.00, Episode epsilon 0.01097, Learning Rate 0.00292\n",
            "Step 4100, Episode 68 Training loss (for one batch): 0.0446, Episode total rewards 430.00, Episode epsilon 0.00992, Learning Rate 0.00277\n",
            "Episode 68, episode reward: 500.0,running avg reward (100 episodes): 61.3235294117647\n",
            "Step 4200, Episode 69 Training loss (for one batch): 91.6104, Episode total rewards 30.00, Episode epsilon 0.00898, Learning Rate 0.00277\n",
            "Step 4300, Episode 69 Training loss (for one batch): 0.0951, Episode total rewards 130.00, Episode epsilon 0.00812, Learning Rate 0.00264\n",
            "Step 4400, Episode 69 Training loss (for one batch): 0.0840, Episode total rewards 230.00, Episode epsilon 0.00735, Learning Rate 0.00250\n",
            "Episode 69, episode reward: 269.0,running avg reward (100 episodes): 64.33333333333333\n",
            "Step 4500, Episode 70 Training loss (for one batch): 0.4355, Episode total rewards 61.00, Episode epsilon 0.00665, Learning Rate 0.00250\n",
            "Step 4600, Episode 70 Training loss (for one batch): 0.6350, Episode total rewards 161.00, Episode epsilon 0.00602, Learning Rate 0.00238\n",
            "Step 4700, Episode 70 Training loss (for one batch): 0.1129, Episode total rewards 261.00, Episode epsilon 0.00544, Learning Rate 0.00226\n",
            "Step 4800, Episode 70 Training loss (for one batch): 0.1358, Episode total rewards 361.00, Episode epsilon 0.00493, Learning Rate 0.00226\n",
            "Step 4900, Episode 70 Training loss (for one batch): 0.0675, Episode total rewards 461.00, Episode epsilon 0.00446, Learning Rate 0.00215\n",
            "Episode 70, episode reward: 500.0,running avg reward (100 episodes): 70.55714285714286\n",
            "Step 5000, Episode 71 Training loss (for one batch): 6.4344, Episode total rewards 61.00, Episode epsilon 0.00403, Learning Rate 0.00204\n",
            "Step 5100, Episode 71 Training loss (for one batch): 1.2057, Episode total rewards 161.00, Episode epsilon 0.00365, Learning Rate 0.00204\n",
            "Episode 71, episode reward: 241.0,running avg reward (100 episodes): 72.95774647887323\n",
            "Step 5200, Episode 72 Training loss (for one batch): 4.4286, Episode total rewards 20.00, Episode epsilon 0.00330, Learning Rate 0.00194\n",
            "Step 5300, Episode 72 Training loss (for one batch): 0.4069, Episode total rewards 120.00, Episode epsilon 0.00299, Learning Rate 0.00184\n",
            "Episode 72, episode reward: 218.0,running avg reward (100 episodes): 74.97222222222223\n",
            "Step 5400, Episode 73 Training loss (for one batch): 11.1810, Episode total rewards 2.00, Episode epsilon 0.00270, Learning Rate 0.00184\n",
            "Step 5500, Episode 73 Training loss (for one batch): 12.5475, Episode total rewards 102.00, Episode epsilon 0.00245, Learning Rate 0.00175\n",
            "Step 5600, Episode 73 Training loss (for one batch): 1.1410, Episode total rewards 202.00, Episode epsilon 0.00221, Learning Rate 0.00166\n",
            "Episode 73, episode reward: 210.0,running avg reward (100 episodes): 76.82191780821918\n",
            "Step 5700, Episode 74 Training loss (for one batch): 1.4276, Episode total rewards 92.00, Episode epsilon 0.00200, Learning Rate 0.00166\n",
            "Step 5800, Episode 74 Training loss (for one batch): 1.1585, Episode total rewards 192.00, Episode epsilon 0.00181, Learning Rate 0.00158\n",
            "Episode 74, episode reward: 224.0,running avg reward (100 episodes): 78.8108108108108\n",
            "Step 5900, Episode 75 Training loss (for one batch): 3.5144, Episode total rewards 68.00, Episode epsilon 0.00164, Learning Rate 0.00150\n",
            "Step 6000, Episode 75 Training loss (for one batch): 0.6533, Episode total rewards 168.00, Episode epsilon 0.00148, Learning Rate 0.00150\n",
            "Episode 75, episode reward: 199.0,running avg reward (100 episodes): 80.41333333333333\n",
            "Step 6100, Episode 76 Training loss (for one batch): 0.5104, Episode total rewards 69.00, Episode epsilon 0.00134, Learning Rate 0.00142\n",
            "Step 6200, Episode 76 Training loss (for one batch): 0.7775, Episode total rewards 169.00, Episode epsilon 0.00121, Learning Rate 0.00135\n",
            "Episode 76, episode reward: 223.0,running avg reward (100 episodes): 82.28947368421052\n",
            "Step 6300, Episode 77 Training loss (for one batch): 0.4019, Episode total rewards 46.00, Episode epsilon 0.00110, Learning Rate 0.00135\n",
            "Step 6400, Episode 77 Training loss (for one batch): 0.3944, Episode total rewards 146.00, Episode epsilon 0.00099, Learning Rate 0.00129\n",
            "Episode 77, episode reward: 236.0,running avg reward (100 episodes): 84.28571428571429\n",
            "Step 6500, Episode 78 Training loss (for one batch): 1.0938, Episode total rewards 10.00, Episode epsilon 0.00090, Learning Rate 0.00122\n",
            "Step 6600, Episode 78 Training loss (for one batch): 1.1337, Episode total rewards 110.00, Episode epsilon 0.00081, Learning Rate 0.00122\n",
            "Episode 78, episode reward: 194.0,running avg reward (100 episodes): 85.6923076923077\n",
            "Step 6700, Episode 79 Training loss (for one batch): 0.3878, Episode total rewards 16.00, Episode epsilon 0.00074, Learning Rate 0.00116\n",
            "Step 6800, Episode 79 Training loss (for one batch): 0.2699, Episode total rewards 116.00, Episode epsilon 0.00067, Learning Rate 0.00110\n",
            "Step 6900, Episode 79 Training loss (for one batch): 0.5874, Episode total rewards 216.00, Episode epsilon 0.00060, Learning Rate 0.00110\n",
            "Episode 79, episode reward: 219.0,running avg reward (100 episodes): 87.37974683544304\n",
            "Step 7000, Episode 80 Training loss (for one batch): 0.9296, Episode total rewards 97.00, Episode epsilon 0.00055, Learning Rate 0.00105\n",
            "Episode 80, episode reward: 171.0,running avg reward (100 episodes): 88.425\n",
            "Step 7100, Episode 81 Training loss (for one batch): 0.3969, Episode total rewards 26.00, Episode epsilon 0.00049, Learning Rate 0.00099\n",
            "Step 7200, Episode 81 Training loss (for one batch): 0.2240, Episode total rewards 126.00, Episode epsilon 0.00045, Learning Rate 0.00099\n",
            "Episode 81, episode reward: 200.0,running avg reward (100 episodes): 89.80246913580247\n",
            "Step 7300, Episode 82 Training loss (for one batch): 0.4858, Episode total rewards 26.00, Episode epsilon 0.00040, Learning Rate 0.00094\n",
            "Step 7400, Episode 82 Training loss (for one batch): 0.1825, Episode total rewards 126.00, Episode epsilon 0.00037, Learning Rate 0.00090\n",
            "Episode 82, episode reward: 200.0,running avg reward (100 episodes): 91.14634146341463\n",
            "Step 7500, Episode 83 Training loss (for one batch): 0.2046, Episode total rewards 26.00, Episode epsilon 0.00033, Learning Rate 0.00090\n",
            "Step 7600, Episode 83 Training loss (for one batch): 0.2648, Episode total rewards 126.00, Episode epsilon 0.00030, Learning Rate 0.00085\n",
            "Episode 83, episode reward: 214.0,running avg reward (100 episodes): 92.62650602409639\n",
            "Step 7700, Episode 84 Training loss (for one batch): 0.1089, Episode total rewards 12.00, Episode epsilon 0.00027, Learning Rate 0.00081\n",
            "Step 7800, Episode 84 Training loss (for one batch): 0.0904, Episode total rewards 112.00, Episode epsilon 0.00024, Learning Rate 0.00081\n",
            "Episode 84, episode reward: 203.0,running avg reward (100 episodes): 93.94047619047619\n",
            "Step 7900, Episode 85 Training loss (for one batch): 0.0915, Episode total rewards 9.00, Episode epsilon 0.00022, Learning Rate 0.00077\n",
            "Step 8000, Episode 85 Training loss (for one batch): 0.1018, Episode total rewards 109.00, Episode epsilon 0.00020, Learning Rate 0.00073\n",
            "Step 8100, Episode 85 Training loss (for one batch): 0.0763, Episode total rewards 209.00, Episode epsilon 0.00018, Learning Rate 0.00073\n",
            "Episode 85, episode reward: 214.0,running avg reward (100 episodes): 95.3529411764706\n",
            "Step 8200, Episode 86 Training loss (for one batch): 0.0630, Episode total rewards 95.00, Episode epsilon 0.00016, Learning Rate 0.00069\n",
            "Episode 86, episode reward: 193.0,running avg reward (100 episodes): 96.48837209302326\n",
            "Step 8300, Episode 87 Training loss (for one batch): 0.1127, Episode total rewards 2.00, Episode epsilon 0.00015, Learning Rate 0.00066\n",
            "Step 8400, Episode 87 Training loss (for one batch): 0.0677, Episode total rewards 102.00, Episode epsilon 0.00013, Learning Rate 0.00066\n",
            "Step 8500, Episode 87 Training loss (for one batch): 0.1449, Episode total rewards 202.00, Episode epsilon 0.00012, Learning Rate 0.00063\n",
            "Episode 87, episode reward: 250.0,running avg reward (100 episodes): 98.25287356321839\n",
            "Episode 88, episode reward: 11.0,running avg reward (100 episodes): 97.26136363636364\n",
            "Episode 89, episode reward: 8.0,running avg reward (100 episodes): 96.25842696629213\n",
            "Episode 90, episode reward: 9.0,running avg reward (100 episodes): 95.28888888888889\n",
            "Episode 91, episode reward: 9.0,running avg reward (100 episodes): 94.34065934065934\n",
            "Episode 92, episode reward: 11.0,running avg reward (100 episodes): 93.43478260869566\n",
            "Step 8600, Episode 93 Training loss (for one batch): 7.8042, Episode total rewards 4.00, Episode epsilon 0.00011, Learning Rate 0.00060\n",
            "Episode 93, episode reward: 10.0,running avg reward (100 episodes): 92.53763440860214\n",
            "Episode 94, episode reward: 9.0,running avg reward (100 episodes): 91.64893617021276\n",
            "Step 8700, Episode 95 Training loss (for one batch): 4.7758, Episode total rewards 85.00, Episode epsilon 0.00010, Learning Rate 0.00060\n",
            "Episode 95, episode reward: 144.0,running avg reward (100 episodes): 92.2\n",
            "Episode 96, episode reward: 9.0,running avg reward (100 episodes): 91.33333333333333\n",
            "Episode 97, episode reward: 11.0,running avg reward (100 episodes): 90.50515463917526\n",
            "Episode 98, episode reward: 9.0,running avg reward (100 episodes): 89.6734693877551\n",
            "Episode 99, episode reward: 10.0,running avg reward (100 episodes): 88.86868686868686\n",
            "Step 8800, Episode 100 Training loss (for one batch): 79.0187, Episode total rewards 2.00, Episode epsilon 0.00010, Learning Rate 0.00057\n",
            "Episode 100, episode reward: 8.0,running avg reward (100 episodes): 88.06\n",
            "Episode 101, episode reward: 10.0,running avg reward (100 episodes): 88.04\n",
            "Episode 102, episode reward: 10.0,running avg reward (100 episodes): 87.97\n",
            "Episode 103, episode reward: 9.0,running avg reward (100 episodes): 87.94\n",
            "Episode 104, episode reward: 10.0,running avg reward (100 episodes): 87.87\n",
            "Episode 105, episode reward: 9.0,running avg reward (100 episodes): 87.81\n",
            "Episode 106, episode reward: 10.0,running avg reward (100 episodes): 87.7\n",
            "Episode 107, episode reward: 11.0,running avg reward (100 episodes): 87.67\n",
            "Step 8900, Episode 108 Training loss (for one batch): 297.6643, Episode total rewards 25.00, Episode epsilon 0.00010, Learning Rate 0.00054\n",
            "Step 9000, Episode 108 Training loss (for one batch): 6.9077, Episode total rewards 125.00, Episode epsilon 0.00010, Learning Rate 0.00054\n",
            "Episode 108, episode reward: 136.0,running avg reward (100 episodes): 88.87\n",
            "Step 9100, Episode 109 Training loss (for one batch): 5.7528, Episode total rewards 89.00, Episode epsilon 0.00010, Learning Rate 0.00051\n",
            "Episode 109, episode reward: 178.0,running avg reward (100 episodes): 90.47\n",
            "Step 9200, Episode 110 Training loss (for one batch): 5.3355, Episode total rewards 11.00, Episode epsilon 0.00010, Learning Rate 0.00048\n",
            "Step 9300, Episode 110 Training loss (for one batch): 7.3852, Episode total rewards 111.00, Episode epsilon 0.00010, Learning Rate 0.00048\n",
            "Step 9400, Episode 110 Training loss (for one batch): 77.8482, Episode total rewards 211.00, Episode epsilon 0.00010, Learning Rate 0.00046\n",
            "Step 9500, Episode 110 Training loss (for one batch): 8.2143, Episode total rewards 311.00, Episode epsilon 0.00010, Learning Rate 0.00044\n",
            "Step 9600, Episode 110 Training loss (for one batch): 2.1937, Episode total rewards 411.00, Episode epsilon 0.00010, Learning Rate 0.00044\n",
            "Episode 110, episode reward: 500.0,running avg reward (100 episodes): 95.35\n",
            "Step 9700, Episode 111 Training loss (for one batch): 2.8237, Episode total rewards 11.00, Episode epsilon 0.00010, Learning Rate 0.00042\n",
            "Step 9800, Episode 111 Training loss (for one batch): 1.4091, Episode total rewards 111.00, Episode epsilon 0.00010, Learning Rate 0.00039\n",
            "Step 9900, Episode 111 Training loss (for one batch): 1.8999, Episode total rewards 211.00, Episode epsilon 0.00010, Learning Rate 0.00039\n",
            "Step 10000, Episode 111 Training loss (for one batch): 1.0805, Episode total rewards 311.00, Episode epsilon 0.00010, Learning Rate 0.00038\n",
            "Step 10100, Episode 111 Training loss (for one batch): 2.1910, Episode total rewards 411.00, Episode epsilon 0.00010, Learning Rate 0.00036\n",
            "Episode 111, episode reward: 500.0,running avg reward (100 episodes): 100.25\n",
            "Step 10200, Episode 112 Training loss (for one batch): 3.1196, Episode total rewards 11.00, Episode epsilon 0.00010, Learning Rate 0.00036\n",
            "Step 10300, Episode 112 Training loss (for one batch): 1.4302, Episode total rewards 111.00, Episode epsilon 0.00010, Learning Rate 0.00034\n",
            "Step 10400, Episode 112 Training loss (for one batch): 1.2523, Episode total rewards 211.00, Episode epsilon 0.00010, Learning Rate 0.00032\n",
            "Step 10500, Episode 112 Training loss (for one batch): 1.1503, Episode total rewards 311.00, Episode epsilon 0.00010, Learning Rate 0.00032\n",
            "Step 10600, Episode 112 Training loss (for one batch): 2.0418, Episode total rewards 411.00, Episode epsilon 0.00010, Learning Rate 0.00031\n",
            "Episode 112, episode reward: 500.0,running avg reward (100 episodes): 105.12\n",
            "Step 10700, Episode 113 Training loss (for one batch): 2.1094, Episode total rewards 11.00, Episode epsilon 0.00010, Learning Rate 0.00029\n",
            "Step 10800, Episode 113 Training loss (for one batch): 1.8263, Episode total rewards 111.00, Episode epsilon 0.00010, Learning Rate 0.00029\n",
            "Step 10900, Episode 113 Training loss (for one batch): 1.6285, Episode total rewards 211.00, Episode epsilon 0.00010, Learning Rate 0.00028\n",
            "Step 11000, Episode 113 Training loss (for one batch): 2.0295, Episode total rewards 311.00, Episode epsilon 0.00010, Learning Rate 0.00026\n",
            "Step 11100, Episode 113 Training loss (for one batch): 4.4825, Episode total rewards 411.00, Episode epsilon 0.00010, Learning Rate 0.00026\n",
            "Episode 113, episode reward: 500.0,running avg reward (100 episodes): 110.0\n",
            "Step 11200, Episode 114 Training loss (for one batch): 2.8601, Episode total rewards 11.00, Episode epsilon 0.00010, Learning Rate 0.00025\n",
            "Step 11300, Episode 114 Training loss (for one batch): 4.0681, Episode total rewards 111.00, Episode epsilon 0.00010, Learning Rate 0.00024\n",
            "Step 11400, Episode 114 Training loss (for one batch): 2.0741, Episode total rewards 211.00, Episode epsilon 0.00010, Learning Rate 0.00024\n",
            "Step 11500, Episode 114 Training loss (for one batch): 2.7118, Episode total rewards 311.00, Episode epsilon 0.00010, Learning Rate 0.00022\n",
            "Step 11600, Episode 114 Training loss (for one batch): 2.2662, Episode total rewards 411.00, Episode epsilon 0.00010, Learning Rate 0.00021\n",
            "Episode 114, episode reward: 500.0,running avg reward (100 episodes): 114.89\n",
            "Step 11700, Episode 115 Training loss (for one batch): 95.2203, Episode total rewards 11.00, Episode epsilon 0.00010, Learning Rate 0.00021\n",
            "Step 11800, Episode 115 Training loss (for one batch): 0.9743, Episode total rewards 111.00, Episode epsilon 0.00010, Learning Rate 0.00020\n",
            "Step 11900, Episode 115 Training loss (for one batch): 1.3934, Episode total rewards 211.00, Episode epsilon 0.00010, Learning Rate 0.00019\n",
            "Step 12000, Episode 115 Training loss (for one batch): 1.9694, Episode total rewards 311.00, Episode epsilon 0.00010, Learning Rate 0.00019\n",
            "Step 12100, Episode 115 Training loss (for one batch): 0.6013, Episode total rewards 411.00, Episode epsilon 0.00010, Learning Rate 0.00018\n",
            "Episode 115, episode reward: 500.0,running avg reward (100 episodes): 119.71\n",
            "Step 12200, Episode 116 Training loss (for one batch): 1.5122, Episode total rewards 11.00, Episode epsilon 0.00010, Learning Rate 0.00017\n",
            "Step 12300, Episode 116 Training loss (for one batch): 0.7269, Episode total rewards 111.00, Episode epsilon 0.00010, Learning Rate 0.00017\n",
            "Step 12400, Episode 116 Training loss (for one batch): 1.1933, Episode total rewards 211.00, Episode epsilon 0.00010, Learning Rate 0.00017\n",
            "Step 12500, Episode 116 Training loss (for one batch): 1.3581, Episode total rewards 311.00, Episode epsilon 0.00010, Learning Rate 0.00016\n",
            "Episode 116, episode reward: 354.0,running avg reward (100 episodes): 123.02\n",
            "Step 12600, Episode 117 Training loss (for one batch): 1.3232, Episode total rewards 57.00, Episode epsilon 0.00010, Learning Rate 0.00016\n",
            "Step 12700, Episode 117 Training loss (for one batch): 1.3775, Episode total rewards 157.00, Episode epsilon 0.00010, Learning Rate 0.00015\n",
            "Step 12800, Episode 117 Training loss (for one batch): 1.3384, Episode total rewards 257.00, Episode epsilon 0.00010, Learning Rate 0.00014\n",
            "Episode 117, episode reward: 276.0,running avg reward (100 episodes): 125.68\n",
            "Step 12900, Episode 118 Training loss (for one batch): 0.8800, Episode total rewards 81.00, Episode epsilon 0.00010, Learning Rate 0.00014\n",
            "Step 13000, Episode 118 Training loss (for one batch): 1.0070, Episode total rewards 181.00, Episode epsilon 0.00010, Learning Rate 0.00013\n",
            "Step 13100, Episode 118 Training loss (for one batch): 0.3586, Episode total rewards 281.00, Episode epsilon 0.00010, Learning Rate 0.00013\n",
            "Step 13200, Episode 118 Training loss (for one batch): 0.5657, Episode total rewards 381.00, Episode epsilon 0.00010, Learning Rate 0.00013\n",
            "Step 13300, Episode 118 Training loss (for one batch): 1.2564, Episode total rewards 481.00, Episode epsilon 0.00010, Learning Rate 0.00012\n",
            "Episode 118, episode reward: 500.0,running avg reward (100 episodes): 130.46\n",
            "Step 13400, Episode 119 Training loss (for one batch): 0.5236, Episode total rewards 81.00, Episode epsilon 0.00010, Learning Rate 0.00012\n",
            "Step 13500, Episode 119 Training loss (for one batch): 0.4763, Episode total rewards 181.00, Episode epsilon 0.00010, Learning Rate 0.00012\n",
            "Episode 119, episode reward: 210.0,running avg reward (100 episodes): 132.42\n",
            "Step 13600, Episode 120 Training loss (for one batch): 189.9837, Episode total rewards 71.00, Episode epsilon 0.00010, Learning Rate 0.00011\n",
            "Step 13700, Episode 120 Training loss (for one batch): 1.4369, Episode total rewards 171.00, Episode epsilon 0.00010, Learning Rate 0.00010\n",
            "Step 13800, Episode 120 Training loss (for one batch): 1.4904, Episode total rewards 271.00, Episode epsilon 0.00010, Learning Rate 0.00010\n",
            "Step 13900, Episode 120 Training loss (for one batch): 1.8031, Episode total rewards 371.00, Episode epsilon 0.00010, Learning Rate 0.00010\n",
            "Step 14000, Episode 120 Training loss (for one batch): 0.7172, Episode total rewards 471.00, Episode epsilon 0.00010, Learning Rate 0.00009\n",
            "Episode 120, episode reward: 500.0,running avg reward (100 episodes): 137.32\n",
            "Step 14100, Episode 121 Training loss (for one batch): 1.1620, Episode total rewards 71.00, Episode epsilon 0.00010, Learning Rate 0.00009\n",
            "Step 14200, Episode 121 Training loss (for one batch): 1.4914, Episode total rewards 171.00, Episode epsilon 0.00010, Learning Rate 0.00009\n",
            "Step 14300, Episode 121 Training loss (for one batch): 1.3299, Episode total rewards 271.00, Episode epsilon 0.00010, Learning Rate 0.00008\n",
            "Step 14400, Episode 121 Training loss (for one batch): 1.3856, Episode total rewards 371.00, Episode epsilon 0.00010, Learning Rate 0.00008\n",
            "Step 14500, Episode 121 Training loss (for one batch): 69.8676, Episode total rewards 471.00, Episode epsilon 0.00010, Learning Rate 0.00008\n",
            "Episode 121, episode reward: 500.0,running avg reward (100 episodes): 142.21\n",
            "Step 14600, Episode 122 Training loss (for one batch): 0.7406, Episode total rewards 71.00, Episode epsilon 0.00010, Learning Rate 0.00008\n",
            "Step 14700, Episode 122 Training loss (for one batch): 71.6299, Episode total rewards 171.00, Episode epsilon 0.00010, Learning Rate 0.00008\n",
            "Step 14800, Episode 122 Training loss (for one batch): 76.1239, Episode total rewards 271.00, Episode epsilon 0.00010, Learning Rate 0.00007\n",
            "Step 14900, Episode 122 Training loss (for one batch): 0.8781, Episode total rewards 371.00, Episode epsilon 0.00010, Learning Rate 0.00007\n",
            "Step 15000, Episode 122 Training loss (for one batch): 0.4296, Episode total rewards 471.00, Episode epsilon 0.00010, Learning Rate 0.00007\n",
            "Episode 122, episode reward: 500.0,running avg reward (100 episodes): 147.09\n",
            "Step 15100, Episode 123 Training loss (for one batch): 0.6622, Episode total rewards 71.00, Episode epsilon 0.00010, Learning Rate 0.00007\n",
            "Step 15200, Episode 123 Training loss (for one batch): 0.8323, Episode total rewards 171.00, Episode epsilon 0.00010, Learning Rate 0.00006\n",
            "Step 15300, Episode 123 Training loss (for one batch): 0.4011, Episode total rewards 271.00, Episode epsilon 0.00010, Learning Rate 0.00006\n",
            "Step 15400, Episode 123 Training loss (for one batch): 0.6229, Episode total rewards 371.00, Episode epsilon 0.00010, Learning Rate 0.00006\n",
            "Step 15500, Episode 123 Training loss (for one batch): 0.7220, Episode total rewards 471.00, Episode epsilon 0.00010, Learning Rate 0.00006\n",
            "Episode 123, episode reward: 500.0,running avg reward (100 episodes): 151.99\n",
            "Step 15600, Episode 124 Training loss (for one batch): 0.2470, Episode total rewards 71.00, Episode epsilon 0.00010, Learning Rate 0.00006\n",
            "Step 15700, Episode 124 Training loss (for one batch): 0.6753, Episode total rewards 171.00, Episode epsilon 0.00010, Learning Rate 0.00005\n",
            "Step 15800, Episode 124 Training loss (for one batch): 0.5636, Episode total rewards 271.00, Episode epsilon 0.00010, Learning Rate 0.00005\n",
            "Step 15900, Episode 124 Training loss (for one batch): 0.4193, Episode total rewards 371.00, Episode epsilon 0.00010, Learning Rate 0.00005\n",
            "Step 16000, Episode 124 Training loss (for one batch): 0.6385, Episode total rewards 471.00, Episode epsilon 0.00010, Learning Rate 0.00005\n",
            "Episode 124, episode reward: 490.0,running avg reward (100 episodes): 156.78\n",
            "Step 16100, Episode 125 Training loss (for one batch): 0.4577, Episode total rewards 81.00, Episode epsilon 0.00010, Learning Rate 0.00005\n",
            "Step 16200, Episode 125 Training loss (for one batch): 0.5566, Episode total rewards 181.00, Episode epsilon 0.00010, Learning Rate 0.00005\n",
            "Step 16300, Episode 125 Training loss (for one batch): 0.4289, Episode total rewards 281.00, Episode epsilon 0.00010, Learning Rate 0.00004\n",
            "Step 16400, Episode 125 Training loss (for one batch): 0.7997, Episode total rewards 381.00, Episode epsilon 0.00010, Learning Rate 0.00004\n",
            "Step 16500, Episode 125 Training loss (for one batch): 0.3293, Episode total rewards 481.00, Episode epsilon 0.00010, Learning Rate 0.00004\n",
            "Episode 125, episode reward: 486.0,running avg reward (100 episodes): 161.49\n",
            "Step 16600, Episode 126 Training loss (for one batch): 0.5504, Episode total rewards 95.00, Episode epsilon 0.00010, Learning Rate 0.00004\n",
            "Step 16700, Episode 126 Training loss (for one batch): 0.4554, Episode total rewards 195.00, Episode epsilon 0.00010, Learning Rate 0.00004\n",
            "Step 16800, Episode 126 Training loss (for one batch): 0.6113, Episode total rewards 295.00, Episode epsilon 0.00010, Learning Rate 0.00004\n",
            "Step 16900, Episode 126 Training loss (for one batch): 0.5782, Episode total rewards 395.00, Episode epsilon 0.00010, Learning Rate 0.00004\n",
            "Episode 126, episode reward: 461.0,running avg reward (100 episodes): 165.98\n",
            "Step 17000, Episode 127 Training loss (for one batch): 0.4391, Episode total rewards 34.00, Episode epsilon 0.00010, Learning Rate 0.00003\n",
            "Step 17100, Episode 127 Training loss (for one batch): 0.8923, Episode total rewards 134.00, Episode epsilon 0.00010, Learning Rate 0.00003\n",
            "Step 17200, Episode 127 Training loss (for one batch): 0.6664, Episode total rewards 234.00, Episode epsilon 0.00010, Learning Rate 0.00003\n",
            "Step 17300, Episode 127 Training loss (for one batch): 0.4913, Episode total rewards 334.00, Episode epsilon 0.00010, Learning Rate 0.00003\n",
            "Step 17400, Episode 127 Training loss (for one batch): 1.1856, Episode total rewards 434.00, Episode epsilon 0.00010, Learning Rate 0.00003\n",
            "Episode 127, episode reward: 500.0,running avg reward (100 episodes): 170.87\n",
            "Step 17500, Episode 128 Training loss (for one batch): 0.5693, Episode total rewards 34.00, Episode epsilon 0.00010, Learning Rate 0.00003\n",
            "Step 17600, Episode 128 Training loss (for one batch): 0.9489, Episode total rewards 134.00, Episode epsilon 0.00010, Learning Rate 0.00003\n",
            "Step 17700, Episode 128 Training loss (for one batch): 1.2369, Episode total rewards 234.00, Episode epsilon 0.00010, Learning Rate 0.00003\n",
            "Step 17800, Episode 128 Training loss (for one batch): 0.7451, Episode total rewards 334.00, Episode epsilon 0.00010, Learning Rate 0.00003\n",
            "Episode 128, episode reward: 428.0,running avg reward (100 episodes): 175.04\n",
            "Step 17900, Episode 129 Training loss (for one batch): 0.9485, Episode total rewards 6.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Step 18000, Episode 129 Training loss (for one batch): 0.6112, Episode total rewards 106.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Step 18100, Episode 129 Training loss (for one batch): 1.0389, Episode total rewards 206.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Step 18200, Episode 129 Training loss (for one batch): 0.7398, Episode total rewards 306.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Step 18300, Episode 129 Training loss (for one batch): 0.4179, Episode total rewards 406.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Episode 129, episode reward: 421.0,running avg reward (100 episodes): 179.16\n",
            "Step 18400, Episode 130 Training loss (for one batch): 0.4862, Episode total rewards 85.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Step 18500, Episode 130 Training loss (for one batch): 0.7996, Episode total rewards 185.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Step 18600, Episode 130 Training loss (for one batch): 0.7725, Episode total rewards 285.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Episode 130, episode reward: 340.0,running avg reward (100 episodes): 182.19\n",
            "Step 18700, Episode 131 Training loss (for one batch): 57.7970, Episode total rewards 45.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Step 18800, Episode 131 Training loss (for one batch): 0.6732, Episode total rewards 145.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Step 18900, Episode 131 Training loss (for one batch): 57.5822, Episode total rewards 245.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Step 19000, Episode 131 Training loss (for one batch): 0.9174, Episode total rewards 345.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Episode 131, episode reward: 382.0,running avg reward (100 episodes): 185.87\n",
            "Step 19100, Episode 132 Training loss (for one batch): 0.5498, Episode total rewards 63.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Step 19200, Episode 132 Training loss (for one batch): 0.3518, Episode total rewards 163.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Step 19300, Episode 132 Training loss (for one batch): 0.7033, Episode total rewards 263.00, Episode epsilon 0.00010, Learning Rate 0.00002\n",
            "Episode 132, episode reward: 361.0,running avg reward (100 episodes): 189.37\n",
            "Step 19400, Episode 133 Training loss (for one batch): 0.5578, Episode total rewards 2.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 19500, Episode 133 Training loss (for one batch): 0.5721, Episode total rewards 102.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 19600, Episode 133 Training loss (for one batch): 0.5671, Episode total rewards 202.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 19700, Episode 133 Training loss (for one batch): 0.7025, Episode total rewards 302.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Episode 133, episode reward: 335.0,running avg reward (100 episodes): 192.56\n",
            "Step 19800, Episode 134 Training loss (for one batch): 0.4876, Episode total rewards 67.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 19900, Episode 134 Training loss (for one batch): 1.2876, Episode total rewards 167.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 20000, Episode 134 Training loss (for one batch): 0.7562, Episode total rewards 267.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Episode 134, episode reward: 344.0,running avg reward (100 episodes): 195.74\n",
            "Step 20100, Episode 135 Training loss (for one batch): 0.8180, Episode total rewards 23.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 20200, Episode 135 Training loss (for one batch): 0.5733, Episode total rewards 123.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 20300, Episode 135 Training loss (for one batch): 0.7113, Episode total rewards 223.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 20400, Episode 135 Training loss (for one batch): 0.9937, Episode total rewards 323.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Episode 135, episode reward: 378.0,running avg reward (100 episodes): 199.4\n",
            "Step 20500, Episode 136 Training loss (for one batch): 0.5313, Episode total rewards 45.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 20600, Episode 136 Training loss (for one batch): 1.0147, Episode total rewards 145.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 20700, Episode 136 Training loss (for one batch): 0.7090, Episode total rewards 245.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Episode 136, episode reward: 277.0,running avg reward (100 episodes): 202.04\n",
            "Step 20800, Episode 137 Training loss (for one batch): 0.4745, Episode total rewards 68.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 20900, Episode 137 Training loss (for one batch): 0.7088, Episode total rewards 168.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 21000, Episode 137 Training loss (for one batch): 0.9923, Episode total rewards 268.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Episode 137, episode reward: 312.0,running avg reward (100 episodes): 205.05\n",
            "Step 21100, Episode 138 Training loss (for one batch): 0.8745, Episode total rewards 56.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 21200, Episode 138 Training loss (for one batch): 1.2235, Episode total rewards 156.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 21300, Episode 138 Training loss (for one batch): 0.7094, Episode total rewards 256.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Episode 138, episode reward: 272.0,running avg reward (100 episodes): 207.6\n",
            "Step 21400, Episode 139 Training loss (for one batch): 0.9910, Episode total rewards 84.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 21500, Episode 139 Training loss (for one batch): 1.4169, Episode total rewards 184.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 21600, Episode 139 Training loss (for one batch): 0.9189, Episode total rewards 284.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Episode 139, episode reward: 345.0,running avg reward (100 episodes): 210.85\n",
            "Step 21700, Episode 140 Training loss (for one batch): 0.9633, Episode total rewards 39.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 21800, Episode 140 Training loss (for one batch): 1.0133, Episode total rewards 139.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 21900, Episode 140 Training loss (for one batch): 0.9617, Episode total rewards 239.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Episode 140, episode reward: 258.0,running avg reward (100 episodes): 213.26\n",
            "Step 22000, Episode 141 Training loss (for one batch): 0.9206, Episode total rewards 81.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 22100, Episode 141 Training loss (for one batch): 57.1656, Episode total rewards 181.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 22200, Episode 141 Training loss (for one batch): 56.8271, Episode total rewards 281.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Episode 141, episode reward: 292.0,running avg reward (100 episodes): 216.08\n",
            "Step 22300, Episode 142 Training loss (for one batch): 1.3795, Episode total rewards 89.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 22400, Episode 142 Training loss (for one batch): 0.7679, Episode total rewards 189.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Episode 142, episode reward: 249.0,running avg reward (100 episodes): 218.46\n",
            "Step 22500, Episode 143 Training loss (for one batch): 0.9853, Episode total rewards 40.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 22600, Episode 143 Training loss (for one batch): 1.2998, Episode total rewards 140.00, Episode epsilon 0.00010, Learning Rate 0.00001\n",
            "Step 22700, Episode 143 Training loss (for one batch): 0.9533, Episode total rewards 240.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 143, episode reward: 260.0,running avg reward (100 episodes): 220.59\n",
            "Step 22800, Episode 144 Training loss (for one batch): 59.7357, Episode total rewards 80.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 22900, Episode 144 Training loss (for one batch): 1.1421, Episode total rewards 180.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 23000, Episode 144 Training loss (for one batch): 1.0065, Episode total rewards 280.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 144, episode reward: 298.0,running avg reward (100 episodes): 223.43\n",
            "Step 23100, Episode 145 Training loss (for one batch): 1.0134, Episode total rewards 82.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 23200, Episode 145 Training loss (for one batch): 0.9468, Episode total rewards 182.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 145, episode reward: 276.0,running avg reward (100 episodes): 226.09\n",
            "Step 23300, Episode 146 Training loss (for one batch): 0.9783, Episode total rewards 6.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 23400, Episode 146 Training loss (for one batch): 0.9146, Episode total rewards 106.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 23500, Episode 146 Training loss (for one batch): 1.3140, Episode total rewards 206.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 146, episode reward: 268.0,running avg reward (100 episodes): 228.68\n",
            "Step 23600, Episode 147 Training loss (for one batch): 0.8130, Episode total rewards 38.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 23700, Episode 147 Training loss (for one batch): 1.1309, Episode total rewards 138.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 23800, Episode 147 Training loss (for one batch): 1.1194, Episode total rewards 238.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 147, episode reward: 251.0,running avg reward (100 episodes): 231.07\n",
            "Step 23900, Episode 148 Training loss (for one batch): 0.9560, Episode total rewards 87.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 24000, Episode 148 Training loss (for one batch): 1.2832, Episode total rewards 187.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 148, episode reward: 266.0,running avg reward (100 episodes): 232.96\n",
            "Step 24100, Episode 149 Training loss (for one batch): 1.1833, Episode total rewards 21.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 24200, Episode 149 Training loss (for one batch): 1.1238, Episode total rewards 121.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 24300, Episode 149 Training loss (for one batch): 1.0484, Episode total rewards 221.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 149, episode reward: 249.0,running avg reward (100 episodes): 235.16\n",
            "Step 24400, Episode 150 Training loss (for one batch): 59.1556, Episode total rewards 72.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 24500, Episode 150 Training loss (for one batch): 0.9362, Episode total rewards 172.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 150, episode reward: 268.0,running avg reward (100 episodes): 237.42\n",
            "Step 24600, Episode 151 Training loss (for one batch): 1.4553, Episode total rewards 4.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 24700, Episode 151 Training loss (for one batch): 1.3938, Episode total rewards 104.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 24800, Episode 151 Training loss (for one batch): 1.2932, Episode total rewards 204.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 151, episode reward: 246.0,running avg reward (100 episodes): 239.23\n",
            "Step 24900, Episode 152 Training loss (for one batch): 1.1593, Episode total rewards 58.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 25000, Episode 152 Training loss (for one batch): 1.0077, Episode total rewards 158.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 25100, Episode 152 Training loss (for one batch): 1.3517, Episode total rewards 258.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 152, episode reward: 263.0,running avg reward (100 episodes): 241.51\n",
            "Step 25200, Episode 153 Training loss (for one batch): 1.3810, Episode total rewards 95.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 25300, Episode 153 Training loss (for one batch): 0.9482, Episode total rewards 195.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 153, episode reward: 256.0,running avg reward (100 episodes): 243.8\n",
            "Step 25400, Episode 154 Training loss (for one batch): 0.9839, Episode total rewards 39.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 25500, Episode 154 Training loss (for one batch): 0.6018, Episode total rewards 139.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 25600, Episode 154 Training loss (for one batch): 0.6998, Episode total rewards 239.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 154, episode reward: 263.0,running avg reward (100 episodes): 245.82\n",
            "Step 25700, Episode 155 Training loss (for one batch): 1.3638, Episode total rewards 76.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 25800, Episode 155 Training loss (for one batch): 1.3178, Episode total rewards 176.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 25900, Episode 155 Training loss (for one batch): 0.9622, Episode total rewards 276.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 155, episode reward: 289.0,running avg reward (100 episodes): 248.52\n",
            "Step 26000, Episode 156 Training loss (for one batch): 1.6059, Episode total rewards 87.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 26100, Episode 156 Training loss (for one batch): 1.1101, Episode total rewards 187.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 156, episode reward: 228.0,running avg reward (100 episodes): 250.11\n",
            "Step 26200, Episode 157 Training loss (for one batch): 0.9799, Episode total rewards 59.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 26300, Episode 157 Training loss (for one batch): 1.5633, Episode total rewards 159.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 26400, Episode 157 Training loss (for one batch): 0.8575, Episode total rewards 259.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 157, episode reward: 273.0,running avg reward (100 episodes): 250.57\n",
            "Step 26500, Episode 158 Training loss (for one batch): 1.4359, Episode total rewards 86.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 26600, Episode 158 Training loss (for one batch): 0.9712, Episode total rewards 186.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 158, episode reward: 254.0,running avg reward (100 episodes): 252.02\n",
            "Step 26700, Episode 159 Training loss (for one batch): 1.4105, Episode total rewards 32.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 26800, Episode 159 Training loss (for one batch): 1.2852, Episode total rewards 132.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 26900, Episode 159 Training loss (for one batch): 0.9124, Episode total rewards 232.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 159, episode reward: 243.0,running avg reward (100 episodes): 253.16\n",
            "Step 27000, Episode 160 Training loss (for one batch): 61.5910, Episode total rewards 89.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 27100, Episode 160 Training loss (for one batch): 0.7281, Episode total rewards 189.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 160, episode reward: 239.0,running avg reward (100 episodes): 253.92\n",
            "Step 27200, Episode 161 Training loss (for one batch): 58.1450, Episode total rewards 50.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 27300, Episode 161 Training loss (for one batch): 1.3520, Episode total rewards 150.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 27400, Episode 161 Training loss (for one batch): 1.1143, Episode total rewards 250.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 161, episode reward: 283.0,running avg reward (100 episodes): 254.38\n",
            "Step 27500, Episode 162 Training loss (for one batch): 114.9044, Episode total rewards 67.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 27600, Episode 162 Training loss (for one batch): 1.1918, Episode total rewards 167.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 162, episode reward: 246.0,running avg reward (100 episodes): 253.49\n",
            "Step 27700, Episode 163 Training loss (for one batch): 1.3567, Episode total rewards 21.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 27800, Episode 163 Training loss (for one batch): 0.9418, Episode total rewards 121.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 27900, Episode 163 Training loss (for one batch): 0.7676, Episode total rewards 221.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 163, episode reward: 260.0,running avg reward (100 episodes): 253.63\n",
            "Step 28000, Episode 164 Training loss (for one batch): 0.7160, Episode total rewards 61.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 28100, Episode 164 Training loss (for one batch): 0.7895, Episode total rewards 161.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 28200, Episode 164 Training loss (for one batch): 0.9382, Episode total rewards 261.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 164, episode reward: 276.0,running avg reward (100 episodes): 254.02\n",
            "Step 28300, Episode 165 Training loss (for one batch): 1.1001, Episode total rewards 85.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 28400, Episode 165 Training loss (for one batch): 1.0879, Episode total rewards 185.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 165, episode reward: 259.0,running avg reward (100 episodes): 253.35\n",
            "Step 28500, Episode 166 Training loss (for one batch): 1.3437, Episode total rewards 26.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 28600, Episode 166 Training loss (for one batch): 1.1454, Episode total rewards 126.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 28700, Episode 166 Training loss (for one batch): 0.8083, Episode total rewards 226.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 166, episode reward: 246.0,running avg reward (100 episodes): 253.82\n",
            "Step 28800, Episode 167 Training loss (for one batch): 1.3708, Episode total rewards 80.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 28900, Episode 167 Training loss (for one batch): 0.9845, Episode total rewards 180.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 167, episode reward: 269.0,running avg reward (100 episodes): 253.19\n",
            "Step 29000, Episode 168 Training loss (for one batch): 1.3223, Episode total rewards 11.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 29100, Episode 168 Training loss (for one batch): 1.3699, Episode total rewards 111.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 29200, Episode 168 Training loss (for one batch): 57.3423, Episode total rewards 211.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Episode 168, episode reward: 224.0,running avg reward (100 episodes): 250.43\n",
            "Step 29300, Episode 169 Training loss (for one batch): 57.1124, Episode total rewards 87.00, Episode epsilon 0.00010, Learning Rate 0.00000\n",
            "Step 29400, Episode 169 Training loss (for one batch): 1.3591, Episode total rewards 187.00, Episode epsilon 0.00010, Learning Rate 0.00000\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "import gym\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout  # , Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow import GradientTape, math, one_hot, square, summary, config\n",
        "from numpy.random import seed\n",
        "from tensorflow import random as tf_rand\n",
        "\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "\n",
        "    def __init__(self, input_size, output_size, layers, init_learning_rate, decay_steps, decay_rate, weights=None):\n",
        "        lr_schedule = ExponentialDecay(\n",
        "            initial_learning_rate=init_learning_rate,\n",
        "            decay_steps=decay_steps,\n",
        "            decay_rate=decay_rate,\n",
        "            staircase=True)\n",
        "        self.optimizer = Adam(learning_rate=lr_schedule)\n",
        "        lr_metric = self.get_lr_metric(self.optimizer)\n",
        "        self.model = self.create_model(input_size, output_size, layers, lr_metric)\n",
        "        if weights is not None:\n",
        "            self.copy_weights(weights)\n",
        "\n",
        "    # source: https://stackoverflow.com/questions/47490834/how-can-i-print-the-learning-rate-at-each-epoch-with-adam-optimizer-in-keras\n",
        "    def get_lr_metric(self, optimizer):\n",
        "        def lr(y_true, y_pred):\n",
        "            return optimizer._decayed_lr(\"float32\")\n",
        "\n",
        "        return lr\n",
        "\n",
        "    def create_model(self, input_size, output_size, layers, lr_metric):\n",
        "        state_input = Input(shape=(input_size,), dtype='float64', name=\"states input\")\n",
        "\n",
        "        x = Dense(layers[0], activation='relu')(state_input)  # , kernel_initializer='RandomNormal'\n",
        "        for i in range(1, len(layers)):\n",
        "            x = Dense(layers[i], activation='relu')(x)\n",
        "            # x = BatchNormalization()(x)\n",
        "            # x = Dropout(0.2)(x)\n",
        "\n",
        "        x = Dense(output_size, activation='linear')(x)\n",
        "\n",
        "        model = Model(state_input, x)\n",
        "        model.compile(loss='mse', optimizer=self.optimizer, metrics=['acc', lr_metric])\n",
        "\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.model(np.atleast_2d(x.astype('float64')))\n",
        "\n",
        "    def copy_weights(self, other_weights):\n",
        "        # owm_weights = self.model.trainable_variables\n",
        "        # for v1, v2 in zip(owm_weights, other_weights):\n",
        "        #     v1.assign(v2.numpy())\n",
        "        self.model.set_weights(other_weights)\n",
        "\n",
        "\n",
        "def sample_batch(deque_to_sample_from, n):\n",
        "    batch = random.sample(list(deque_to_sample_from), n)\n",
        "    return batch\n",
        "\n",
        "\n",
        "def sample_batch_stochastic_prioritized(deque_to_sample_from, n):\n",
        "    sorted_deque = sorted(deque_to_sample_from, key=lambda r: r[\"td_error\"])\n",
        "    alpha = 0.95\n",
        "    # e = 1e-6\n",
        "    w = [(1/(i+1)) ** alpha for i in range(len(sorted_deque))]\n",
        "    # w = [(i[\"td_error\"] + e) ** alpha for i in deque_to_sample_from]\n",
        "    w_sum = sum(w)\n",
        "    w = [i / w_sum for i in range(len(w))]\n",
        "    batch = random.choices(deque_to_sample_from, weights=w, k=n)\n",
        "    return batch\n",
        "\n",
        "\n",
        "def epsilon_decay(old_epsilon, final_epsilon, decay_rate):\n",
        "    new_epsilon = max(final_epsilon, decay_rate * old_epsilon)  # decrease epsilon\n",
        "    return new_epsilon\n",
        "\n",
        "\n",
        "def sample_action(epsilon, actions_q_values):\n",
        "    rand = random.random()\n",
        "    if rand < epsilon:\n",
        "        action = random.randint(0, actions_q_values.shape[1] - 1)\n",
        "    else:\n",
        "        action = np.argmax(actions_q_values)\n",
        "    return action\n",
        "\n",
        "\n",
        "def train_agent(q_hyper_params, nn_hyper_params, log_dir, avg_return_steps=100):\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    summary_writer = summary.create_file_writer(log_dir)\n",
        "\n",
        "    experience_replay = deque(maxlen=q_hyper_params[\"N\"])  # D\n",
        "    q_value_network = NeuralNetwork(input_size=env.observation_space.shape[0], output_size=env.action_space.n,\n",
        "                                    layers=nn_hyper_params[\"layers\"], init_learning_rate=nn_hyper_params[\"initial_lr\"],\n",
        "                                    decay_steps=nn_hyper_params[\"decay_steps\"],\n",
        "                                    decay_rate=nn_hyper_params[\"decay_rate\"])\n",
        "    target_network = NeuralNetwork(input_size=env.observation_space.shape[0], output_size=env.action_space.n,\n",
        "                                   layers=nn_hyper_params[\"layers\"],\n",
        "                                   # weights=q_value_network.model.trainable_variables,\n",
        "                                   init_learning_rate=nn_hyper_params[\"initial_lr\"],\n",
        "                                   decay_steps=nn_hyper_params[\"decay_steps\"],\n",
        "                                   decay_rate=nn_hyper_params[\"decay_rate\"])\n",
        "    episodes_total_rewards = []\n",
        "    rewards = []\n",
        "    losses = []\n",
        "\n",
        "    s_t = env.reset()\n",
        "\n",
        "    passed_avg_return_of_475 = False\n",
        "    training_started = False\n",
        "    total_steps_counter = 0\n",
        "    epsilon = q_hyper_params[\"epsilon_init\"]\n",
        "    loss = -1\n",
        "    for episode_index in range(q_hyper_params[\"M\"]):\n",
        "        episode_total_rewards = 0\n",
        "        episode_total_losses = []\n",
        "        done = False\n",
        "        # play an episode - single trajectory\n",
        "        while not done and not passed_avg_return_of_475:\n",
        "            epsilon = epsilon_decay(old_epsilon=epsilon, final_epsilon=q_hyper_params[\"epsilon_end\"],\n",
        "                                    decay_rate=q_hyper_params[\"epsilon_decay_rate\"])\n",
        "\n",
        "            # selecting action\n",
        "            state_action_q_values = q_value_network.predict(s_t)\n",
        "            action = sample_action(epsilon,\n",
        "                                   state_action_q_values)\n",
        "\n",
        "            # taking a single step\n",
        "            s_t_1, reward, done, info = env.step(action)\n",
        "            total_steps_counter += 1\n",
        "            rewards.append(reward)\n",
        "            episode_total_rewards += reward\n",
        "\n",
        "            # Stochastic Prioritized Experience\n",
        "            if done:\n",
        "                r_t_1 = reward\n",
        "            else:\n",
        "                r = reward\n",
        "                target_q_values = target_network.predict(s_t_1)[0]\n",
        "                max_q_value = max(target_q_values)\n",
        "                r_t_1 = r + q_hyper_params[\"discount_factor\"] * max_q_value\n",
        "            td_error = r_t_1 - math.reduce_sum(\n",
        "                state_action_q_values * one_hot(action, env.action_space.n),\n",
        "                axis=1)\n",
        "\n",
        "            # Storing transition\n",
        "            current_transition = {\"s_t\": s_t, \"action\": action, \"reward\": reward, \"s_t_1\": s_t_1, \"done\": done,\n",
        "                                  \"td_error\": td_error}\n",
        "            experience_replay.append(current_transition)\n",
        "\n",
        "            s_t = s_t_1\n",
        "\n",
        "            if len(experience_replay) > nn_hyper_params[\"batch_size\"] * 10:  # enough initial observations\n",
        "                training_started = True\n",
        "                transitions_batch = sample_batch_stochastic_prioritized(experience_replay,\n",
        "                                                                        nn_hyper_params[\"batch_size\"])\n",
        "\n",
        "                y = []\n",
        "                states_batch = []\n",
        "                actions_batch = []\n",
        "                for transition in transitions_batch:\n",
        "                    states_batch.append(transition[\"s_t\"])\n",
        "                    actions_batch.append(transition[\"action\"])\n",
        "                    if transition[\"done\"]:\n",
        "                        y.append(transition[\"reward\"])\n",
        "                    else:\n",
        "                        r = transition[\"reward\"]\n",
        "                        target_q_values = target_network.predict(transition[\"s_t_1\"])[0]\n",
        "                        max_q_value = max(target_q_values)\n",
        "                        y.append(r + q_hyper_params[\"discount_factor\"] * max_q_value)\n",
        "\n",
        "                # training\n",
        "                y = np.asarray(y)\n",
        "                states_batch = np.asarray(states_batch)\n",
        "                actions_batch = np.asarray(actions_batch)\n",
        "                # gradient step\n",
        "                with GradientTape() as tape:\n",
        "                    predicted_action_value_q = math.reduce_sum(\n",
        "                        q_value_network.model(states_batch, training=True) * one_hot(actions_batch, env.action_space.n),\n",
        "                        axis=1)\n",
        "                    # Compute the loss value for this minibatch.\n",
        "                    loss = math.reduce_mean(square(y - predicted_action_value_q))\n",
        "\n",
        "                    # loss = nn_hyper_params[\"loss_fn\"](y, predicted_action_value_q)\n",
        "                    # loss = q_value_network.model.compiled_loss(y, predicted_action_value_q)\n",
        "\n",
        "                variables = q_value_network.model.trainable_weights\n",
        "                gradients = tape.gradient(loss, variables)\n",
        "                q_value_network.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "                losses.append(loss)\n",
        "                episode_total_losses.append(loss)\n",
        "                with summary_writer.as_default():\n",
        "                    summary.scalar(\"steps' losses\", loss, step=total_steps_counter)\n",
        "            if done:\n",
        "                s_t = env.reset()\n",
        "                episodes_total_rewards.append(episode_total_rewards)\n",
        "            if total_steps_counter % q_hyper_params[\"C\"] == 0:\n",
        "                target_network.copy_weights(q_value_network.model.get_weights())\n",
        "            if len(rewards) % 100 == 0:\n",
        "                print(\n",
        "                    \"Step %d, Episode %d Training loss (for one batch): %.4f, Episode total rewards %.2f, Episode epsilon %.5f, Learning Rate %.5f\" % (\n",
        "                        total_steps_counter, episode_index + 1, float(loss), episode_total_rewards, epsilon,\n",
        "                        q_value_network.optimizer._decayed_lr(\"float32\")))\n",
        "\n",
        "        last_episodes_avg_returns = (\n",
        "                sum(episodes_total_rewards[-min(len(episodes_total_rewards), avg_return_steps):]) / len(\n",
        "            episodes_total_rewards[-min(len(episodes_total_rewards), avg_return_steps):]))\n",
        "        if episode_index >= avg_return_steps and not passed_avg_return_of_475:\n",
        "            if last_episodes_avg_returns > 475:\n",
        "                passed_avg_return_of_475 = True\n",
        "                print(\n",
        "                    f\"Average reward per episode for the last {avg_return_steps} episodes passed 475 after episode: {episode_index + 1}\")\n",
        "                break\n",
        "        if training_started:\n",
        "            last_steps_avg_losses = (sum(losses[-min(len(losses), 1000):]) / len(\n",
        "                losses[-min(len(losses), 1000):]))\n",
        "            print(\n",
        "                f\"Episode {episode_index + 1}, episode reward: {episode_total_rewards},running avg reward (100 episodes): {last_episodes_avg_returns}\")\n",
        "            with summary_writer.as_default():\n",
        "                summary.scalar('episode reward', episode_total_rewards, step=episode_index)\n",
        "                summary.scalar('running avg reward (100 episodes)', last_episodes_avg_returns, step=episode_index)\n",
        "                summary.scalar('running avg reward (1000 steps)', last_steps_avg_losses, step=episode_index)\n",
        "                summary.scalar(\"episode's steps average loss\", sum(episode_total_losses) / len(episode_total_losses),\n",
        "                               step=episode_index)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    end_time = time.time()\n",
        "    train_time = end_time - start_time\n",
        "    print(f\"Training Time (in seconds): {train_time}\")\n",
        "    return q_value_network, losses, episodes_total_rewards, train_time\n",
        "\n",
        "\n",
        "def test_agent(q_net, path):\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env = gym.wrappers.Monitor(env, path, force=True)\n",
        "\n",
        "    s_t = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        env.render()  # enable in test\n",
        "        state_action_q_values = q_net.predict(s_t)\n",
        "        action = np.argmax(state_action_q_values)\n",
        "        s_t_1, reward, done, info = env.step(action)\n",
        "        s_t = s_t_1\n",
        "    env.close()\n",
        "\n",
        "\n",
        "def plot_line(x, y, x_label, y_label, data_label, title, color):\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    plt.plot(x, y, color=color, label=data_label)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.legend(loc='best', shadow=True)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def main():\n",
        "    dqn_params = {\n",
        "        \"N\": 1000,  # experience_replay deque size\n",
        "        \"M\": 1000,  # number of episodes #50000\n",
        "        \"C\": 25,  # steps update interval for the target network's weights\n",
        "        \"epsilon_init\": 0.6,  # 0.6\n",
        "        \"epsilon_end\": 0.0001,\n",
        "        \"epsilon_decay_rate\": 0.999,\n",
        "        \"discount_factor\": 0.99}\n",
        "    nn_hyper_parameters = {\n",
        "        \"batch_size\": 32,\n",
        "        \"initial_lr\": 1e-2,\n",
        "        \"decay_steps\": 150,\n",
        "        \"decay_rate\": 0.95,\n",
        "        \"loss_fn\": MeanSquaredError(),\n",
        "        \"layers\": [6, 6, 6, 5, 4]}\n",
        "\n",
        "    start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    log_path = 'logs/dqn/' + start_time\n",
        "\n",
        "    q_network, losses, episodes_total_rewards, training_time = train_agent(q_hyper_params=dqn_params,\n",
        "                                                                           nn_hyper_params=nn_hyper_parameters,\n",
        "                                                                           avg_return_steps=100, log_dir=log_path)\n",
        "\n",
        "    plot_line(range(len(losses)), losses, x_label=\"steps\", y_label=\"MSE\", data_label=\"losses\",\n",
        "              title=\"MSE for each episode\",\n",
        "              color=\"b\")\n",
        "    plot_line(range(len(episodes_total_rewards)), episodes_total_rewards, x_label=\"episodes\", y_label=\"rewards sum\",\n",
        "              data_label=\"rewards per episode\",\n",
        "              title=\"total reward for each episode\", color=\"c\")\n",
        "    for i in range(10):\n",
        "        test_agent(q_network, path=os.path.join(os.getcwd(), f\"videos/{start_time}/{i}\"))\n",
        "\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qLJ4X0RrZm9x",
        "outputId": "9edc527b-be9b-4c1d-92cf-8fa2906a1cf0"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/logs/dqn/20211201-135918"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ztr19cHHZtDp",
        "outputId": "bd36f294-b7ca-4498-c14f-b13143d5ec61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecting TensorBoard with logdir /content/logs/dqn/20211201-135918 (started 0:00:31 ago; port 6006, pid 175).\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '1000');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tensorboard import notebook\n",
        "\n",
        "notebook.display(port=6006, height=1000) "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DRL-DQN-hw1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}